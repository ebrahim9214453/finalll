{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_sleep.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYcAbQ49GNDm",
        "colab_type": "text"
      },
      "source": [
        "# Start with smile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz5nyX_4Pcrd",
        "colab_type": "text"
      },
      "source": [
        "https://createmomo.github.io/2017/11/11/CRF-Layer-on-the-Top-of-BiLSTM-5/\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/Hironsan/keras-crf-layer\n",
        "\n",
        "\n",
        "https://github.com/keunwoochoi/kapre#one-shot-example\n",
        "\n",
        "\n",
        "https://github.com/keunwoochoi/keras_STFT_layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6_RDQ97-vM-",
        "colab_type": "code",
        "outputId": "d37ca152-9843-43e4-d1e0-1640e0a60c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykrgh7Q2-1I-",
        "colab_type": "code",
        "outputId": "72971db1-dd38-4dd9-c87f-b6951e3fadb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#@title \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "import os\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "import keras\n",
        "from keras import optimizers, losses, activations, models\n",
        "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D,concatenate, SpatialDropout1D, TimeDistributed, Bidirectional, LSTM\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import IPython\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.layers import Convolution2D, MaxPool2D, GlobalMaxPool2D, GlobalAveragePooling2D, SpatialDropout2D\n",
        "\n",
        "#from models import get_model_cnn\n",
        "import numpy as np\n",
        "#from utils import gen, chunker, WINDOW_SIZE, rescale_array\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from glob import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-4i6l6aa_\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-4i6l6aa_\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.2.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.16.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.9)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yhn079tq/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qemPFticPwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.engine import Layer, InputSpec\n",
        "\n",
        "try:\n",
        "    from tensorflow.contrib.crf import crf_decode\n",
        "except ImportError:\n",
        "    from tensorflow.python.framework import dtypes\n",
        "    from tensorflow.python.ops import array_ops, gen_array_ops, math_ops, rnn, rnn_cell\n",
        "\n",
        "\n",
        "    class CrfDecodeForwardRnnCell(rnn_cell.RNNCell):\n",
        "        \"\"\"Computes the forward decoding in a linear-chain CRF.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, transition_params):\n",
        "            \"\"\"Initialize the CrfDecodeForwardRnnCell.\n",
        "            Args:\n",
        "              transition_params: A [num_tags, num_tags] matrix of binary\n",
        "                potentials. This matrix is expanded into a\n",
        "                [1, num_tags, num_tags] in preparation for the broadcast\n",
        "                summation occurring within the cell.\n",
        "            \"\"\"\n",
        "            self._transition_params = array_ops.expand_dims(transition_params, 0)\n",
        "            self._num_tags = transition_params.get_shape()[0].value\n",
        "\n",
        "        @property\n",
        "        def state_size(self):\n",
        "            return self._num_tags\n",
        "\n",
        "        @property\n",
        "        def output_size(self):\n",
        "            return self._num_tags\n",
        "\n",
        "        def __call__(self, inputs, state, scope=None):\n",
        "            \"\"\"Build the CrfDecodeForwardRnnCell.\n",
        "            Args:\n",
        "              inputs: A [batch_size, num_tags] matrix of unary potentials.\n",
        "              state: A [batch_size, num_tags] matrix containing the previous step's\n",
        "                    score values.\n",
        "              scope: Unused variable scope of this cell.\n",
        "            Returns:\n",
        "              backpointers: [batch_size, num_tags], containing backpointers.\n",
        "              new_state: [batch_size, num_tags], containing new score values.\n",
        "            \"\"\"\n",
        "            # For simplicity, in shape comments, denote:\n",
        "            # 'batch_size' by 'B', 'max_seq_len' by 'T' , 'num_tags' by 'O' (output).\n",
        "            state = array_ops.expand_dims(state, 2)  # [B, O, 1]\n",
        "\n",
        "            # This addition op broadcasts self._transitions_params along the zeroth\n",
        "            # dimension and state along the second dimension.\n",
        "            # [B, O, 1] + [1, O, O] -> [B, O, O]\n",
        "            transition_scores = state + self._transition_params  # [B, O, O]\n",
        "            new_state = inputs + math_ops.reduce_max(transition_scores, [1])  # [B, O]\n",
        "            backpointers = math_ops.argmax(transition_scores, 1)\n",
        "            backpointers = math_ops.cast(backpointers, dtype=dtypes.int32)  # [B, O]\n",
        "            return backpointers, new_state\n",
        "\n",
        "\n",
        "    class CrfDecodeBackwardRnnCell(rnn_cell.RNNCell):\n",
        "        \"\"\"Computes backward decoding in a linear-chain CRF.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, num_tags):\n",
        "            \"\"\"Initialize the CrfDecodeBackwardRnnCell.\n",
        "            Args:\n",
        "              num_tags\n",
        "            \"\"\"\n",
        "            self._num_tags = num_tags\n",
        "\n",
        "        @property\n",
        "        def state_size(self):\n",
        "            return 1\n",
        "\n",
        "        @property\n",
        "        def output_size(self):\n",
        "            return 1\n",
        "\n",
        "        def __call__(self, inputs, state, scope=None):\n",
        "            \"\"\"Build the CrfDecodeBackwardRnnCell.\n",
        "            Args:\n",
        "              inputs: [batch_size, num_tags], backpointer of next step (in time order).\n",
        "              state: [batch_size, 1], next position's tag index.\n",
        "              scope: Unused variable scope of this cell.\n",
        "            Returns:\n",
        "              new_tags, new_tags: A pair of [batch_size, num_tags]\n",
        "                tensors containing the new tag indices.\n",
        "            \"\"\"\n",
        "            state = array_ops.squeeze(state, axis=[1])  # [B]\n",
        "            batch_size = array_ops.shape(inputs)[0]\n",
        "            b_indices = math_ops.range(batch_size)  # [B]\n",
        "            indices = array_ops.stack([b_indices, state], axis=1)  # [B, 2]\n",
        "            new_tags = array_ops.expand_dims(\n",
        "                gen_array_ops.gather_nd(inputs, indices),  # [B]\n",
        "                axis=-1)  # [B, 1]\n",
        "\n",
        "            return new_tags, new_tags\n",
        "\n",
        "\n",
        "    def crf_decode(potentials, transition_params, sequence_length):\n",
        "        \"\"\"Decode the highest scoring sequence of tags in TensorFlow.\n",
        "        This is a function for tensor.\n",
        "        Args:\n",
        "        potentials: A [batch_size, max_seq_len, num_tags] tensor, matrix of\n",
        "                  unary potentials.\n",
        "        transition_params: A [num_tags, num_tags] tensor, matrix of\n",
        "                  binary potentials.\n",
        "        sequence_length: A [batch_size] tensor, containing sequence lengths.\n",
        "        Returns:\n",
        "        decode_tags: A [batch_size, max_seq_len] tensor, with dtype tf.int32.\n",
        "                    Contains the highest scoring tag indicies.\n",
        "        best_score: A [batch_size] tensor, containing the score of decode_tags.\n",
        "        \"\"\"\n",
        "        # For simplicity, in shape comments, denote:\n",
        "        # 'batch_size' by 'B', 'max_seq_len' by 'T' , 'num_tags' by 'O' (output).\n",
        "        num_tags = potentials.get_shape()[2].value\n",
        "\n",
        "        # Computes forward decoding. Get last score and backpointers.\n",
        "        crf_fwd_cell = CrfDecodeForwardRnnCell(transition_params)\n",
        "        initial_state = array_ops.slice(potentials, [0, 0, 0], [-1, 1, -1])\n",
        "        initial_state = array_ops.squeeze(initial_state, axis=[1])  # [B, O]\n",
        "        inputs = array_ops.slice(potentials, [0, 1, 0], [-1, -1, -1])  # [B, T-1, O]\n",
        "        backpointers, last_score = rnn.dynamic_rnn(\n",
        "            crf_fwd_cell,\n",
        "            inputs=inputs,\n",
        "            sequence_length=sequence_length - 1,\n",
        "            initial_state=initial_state,\n",
        "            time_major=False,\n",
        "            dtype=dtypes.int32)  # [B, T - 1, O], [B, O]\n",
        "        backpointers = gen_array_ops.reverse_sequence(backpointers, sequence_length - 1, seq_dim=1)  # [B, T-1, O]\n",
        "\n",
        "        # Computes backward decoding. Extract tag indices from backpointers.\n",
        "        crf_bwd_cell = CrfDecodeBackwardRnnCell(num_tags)\n",
        "        initial_state = math_ops.cast(math_ops.argmax(last_score, axis=1), dtype=dtypes.int32)  # [B]\n",
        "        initial_state = array_ops.expand_dims(initial_state, axis=-1)  # [B, 1]\n",
        "        decode_tags, _ = rnn.dynamic_rnn(\n",
        "            crf_bwd_cell,\n",
        "            inputs=backpointers,\n",
        "            sequence_length=sequence_length - 1,\n",
        "            initial_state=initial_state,\n",
        "            time_major=False,\n",
        "            dtype=dtypes.int32)  # [B, T - 1, 1]\n",
        "        decode_tags = array_ops.squeeze(decode_tags, axis=[2])  # [B, T - 1]\n",
        "        decode_tags = array_ops.concat([initial_state, decode_tags], axis=1)  # [B, T]\n",
        "        decode_tags = gen_array_ops.reverse_sequence(decode_tags, sequence_length, seq_dim=1)  # [B, T]\n",
        "\n",
        "        best_score = math_ops.reduce_max(last_score, axis=1)  # [B]\n",
        "        return decode_tags, best_score\n",
        "\n",
        "\n",
        "class CRFLayer(Layer):\n",
        "\n",
        "    def __init__(self, transition_params=None, **kwargs):\n",
        "        super(CRFLayer, self).__init__(**kwargs)\n",
        "        self.transition_params = transition_params\n",
        "        self.input_spec = [InputSpec(ndim=3), InputSpec(ndim=2)]\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape[0]) == 3\n",
        "\n",
        "        return input_shape[0]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Creates the layer weights.\n",
        "        Args:\n",
        "            input_shape (list(tuple, tuple)): [(batch_size, n_steps, n_classes), (batch_size, 1)]\n",
        "        \"\"\"\n",
        "        assert len(input_shape) == 2\n",
        "        assert len(input_shape[0]) == 3\n",
        "        assert len(input_shape[1]) == 2\n",
        "        n_steps = input_shape[0][1]\n",
        "        n_classes = input_shape[0][2]\n",
        "        assert n_steps is None or n_steps >= 2\n",
        "\n",
        "        self.transition_params = self.add_weight(shape=(n_classes, n_classes),\n",
        "                                                 initializer='uniform',\n",
        "                                                 name='transition')\n",
        "        self.input_spec = [InputSpec(dtype=K.floatx(), shape=(None, n_steps, n_classes)),\n",
        "                           InputSpec(dtype='int32', shape=(None, 1))]\n",
        "        self.built = True\n",
        "\n",
        "    def viterbi_decode(self, potentials, sequence_length):\n",
        "        \"\"\"Decode the highest scoring sequence of tags in TensorFlow.\n",
        "        This is a function for tensor.\n",
        "        Args:\n",
        "            potentials: A [batch_size, max_seq_len, num_tags] tensor, matrix of unary potentials.\n",
        "            sequence_length: A [batch_size] tensor, containing sequence lengths.\n",
        "        Returns:\n",
        "            decode_tags: A [batch_size, max_seq_len] tensor, with dtype tf.int32.\n",
        "                         Contains the highest scoring tag indicies.\n",
        "        \"\"\"\n",
        "        decode_tags, best_score = crf_decode(potentials, self.transition_params, sequence_length)\n",
        "\n",
        "        return decode_tags\n",
        "\n",
        "    def call(self, inputs, mask=None, **kwargs):\n",
        "        inputs, sequence_lengths = inputs\n",
        "        self.sequence_lengths = K.flatten(sequence_lengths)\n",
        "        y_pred = self.viterbi_decode(inputs, self.sequence_lengths)\n",
        "        nb_classes = self.input_spec[0].shape[2]\n",
        "        y_pred_one_hot = K.one_hot(y_pred, nb_classes)\n",
        "\n",
        "        return K.in_train_phase(inputs, y_pred_one_hot)\n",
        "\n",
        "    def loss(self, y_true, y_pred):\n",
        "        \"\"\"Computes the log-likelihood of tag sequences in a CRF.\n",
        "        Args:\n",
        "            y_true : A (batch_size, n_steps, n_classes) tensor.\n",
        "            y_pred : A (batch_size, n_steps, n_classes) tensor.\n",
        "        Returns:\n",
        "            loss: A scalar containing the log-likelihood of the given sequence of tag indices.\n",
        "        \"\"\"\n",
        "        y_true = K.cast(K.argmax(y_true, axis=-1), dtype='int32')\n",
        "        log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(\n",
        "            y_pred, y_true, self.sequence_lengths, self.transition_params)\n",
        "        loss = tf.reduce_mean(-log_likelihood)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'transition_params': K.eval(self.transition_params),\n",
        "        }\n",
        "        base_config = super(CRFLayer, self).get_config()\n",
        "\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def create_custom_objects():\n",
        "    \"\"\"Returns the custom objects, needed for loading a persisted model.\"\"\"\n",
        "    instanceHolder = {'instance': None}\n",
        "\n",
        "    class ClassWrapper(CRFLayer):\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            instanceHolder['instance'] = self\n",
        "            super(ClassWrapper, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def loss(*args):\n",
        "        method = getattr(instanceHolder['instance'], 'loss')\n",
        "        return method(*args)\n",
        "\n",
        "    return {'CRFLayer': ClassWrapper, 'loss': loss}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC5Bpsphc103",
        "colab_type": "code",
        "outputId": "2510401c-b67d-4f9a-ad47-192bd0fef058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Embedding, Input\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Hyperparameter settings.\n",
        "vocab_size = 20\n",
        "n_classes = 11\n",
        "batch_size = 2\n",
        "maxlen = 2\n",
        "\n",
        "# Random features.\n",
        "x = np.random.randint(1, vocab_size, size=(batch_size, maxlen))\n",
        "\n",
        "# Random tag indices representing the gold sequence.\n",
        "y = np.random.randint(n_classes, size=(batch_size, maxlen))\n",
        "y = np.eye(n_classes)[y]\n",
        "\n",
        "# All sequences in this example have the same length, but they can be variable in a real model.\n",
        "s = np.asarray([maxlen] * batch_size, dtype='int32')\n",
        "\n",
        "# Build an example model.\n",
        "word_ids = Input(batch_shape=(batch_size, maxlen), dtype='int32')\n",
        "sequence_lengths = Input(batch_shape=[batch_size, 1], dtype='int32')\n",
        "\n",
        "word_embeddings = Embedding(vocab_size, n_classes)(word_ids)\n",
        "crf = CRFLayer()\n",
        "pred = crf(inputs=[word_embeddings, sequence_lengths])\n",
        "model = Model(inputs=[word_ids, sequence_lengths], outputs=[pred])\n",
        "model.compile(loss=crf.loss, optimizer='sgd')\n",
        "\n",
        "# Train first 1 batch.\n",
        "model.train_on_batch([x, s], y)\n",
        "\n",
        "# Save the model\n",
        "model.save('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py:567: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf7NFa1QcYJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import Embedding, Input, LSTM, Dropout, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "\n",
        "\n",
        "class LayerTest(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.filename = 'test.h5'\n",
        "\n",
        "    def test_crf_layer(self):\n",
        "\n",
        "        # Hyperparameter settings.\n",
        "        vocab_size = 20\n",
        "        n_classes = 11\n",
        "        batch_size = 2\n",
        "        maxlen = 2\n",
        "\n",
        "        # Random features.\n",
        "        x = np.random.randint(1, vocab_size, size=(batch_size, maxlen))\n",
        "\n",
        "        # Random tag indices representing the gold sequence.\n",
        "        y = np.random.randint(n_classes, size=(batch_size, maxlen))\n",
        "        y = np.eye(n_classes)[y]\n",
        "\n",
        "        # All sequences in this example have the same length, but they can be variable in a real model.\n",
        "        s = np.asarray([maxlen] * batch_size, dtype='int32')\n",
        "\n",
        "        # Build a model.\n",
        "        word_ids = Input(batch_shape=(batch_size, maxlen), dtype='int32')\n",
        "        word_embeddings = Embedding(vocab_size, n_classes)(word_ids)\n",
        "        sequence_lengths = Input(batch_shape=[batch_size, 1], dtype='int32')\n",
        "        crf = CRFLayer()\n",
        "        pred = crf([word_embeddings, sequence_lengths])\n",
        "        model = Model(inputs=[word_ids, sequence_lengths], outputs=[pred])\n",
        "        model.compile(loss=crf.loss, optimizer='sgd')\n",
        "\n",
        "        # Train first 1 batch.\n",
        "        model.train_on_batch([x, s], y)\n",
        "\n",
        "        # Save the model.\n",
        "        model.save(self.filename)\n",
        "\n",
        "    def test_load_model(self):\n",
        "        model = load_model(self.filename, custom_objects=create_custom_objects())\n",
        "\n",
        "    def test_bilstm_crf(self):\n",
        "\n",
        "        # Hyperparameter settings.\n",
        "        vocab_size = 10000\n",
        "        word_embedding_size = 100\n",
        "        num_word_lstm_units = 100\n",
        "        dropout = 0.5\n",
        "        ntags = 10\n",
        "\n",
        "        # Build bidirectional lstm-crf model.\n",
        "        word_ids = Input(batch_shape=(None, None), dtype='int32')\n",
        "        word_embeddings = Embedding(input_dim=vocab_size,\n",
        "                                    output_dim=word_embedding_size,\n",
        "                                    mask_zero=True)(word_ids)\n",
        "\n",
        "        x = Bidirectional(LSTM(units=num_word_lstm_units, return_sequences=True))(word_embeddings)\n",
        "        x = Dropout(dropout)(x)\n",
        "        x = Dense(ntags)(x)\n",
        "\n",
        "        sequence_lengths = Input(batch_shape=(None, 1), dtype='int32')\n",
        "\n",
        "        crf = CRFLayer()\n",
        "        pred = crf([x, sequence_lengths])\n",
        "\n",
        "        model = Model(inputs=[word_ids, sequence_lengths], outputs=[pred])\n",
        "        model.compile(loss=crf.loss, optimizer='sgd')\n",
        "\n",
        "        model.save(self.filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6XFuo6G-1Li",
        "colab_type": "code",
        "outputId": "593eab56-aed2-4c05-8135-f984a96d2735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd/content/drive/My Drive\n",
        "base_path = \"pure_data\"\n",
        "\n",
        "files = sorted(glob(os.path.join(base_path, \"*.npz\")))\n",
        "\n",
        "ids = sorted(list(set([x.split(\"/\")[-1][:5] for x in files])))\n",
        "##ids, SVM_data = train_test_split(ids, test_size=0.09, random_state=1337)\n",
        "#split by test subject\n",
        "train_ids, test_ids = train_test_split(ids, test_size=0.4, random_state=0)\n",
        "\n",
        "train_val, test = [x for x in files if x.split(\"/\")[-1][:5] in train_ids],\\\n",
        "                  [x for x in files if x.split(\"/\")[-1][:5] in test_ids]\n",
        "\n",
        "train, val = train_test_split(train_val, test_size=0.15, random_state=1)\n",
        "\n",
        "train_dict = {k: np.load(k) for k in train}\n",
        "test_dict = {k: np.load(k) for k in test}\n",
        "val_dict = {k: np.load(k) for k in val}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mDdpf8CAkwI",
        "colab_type": "code",
        "outputId": "588ddc7f-f9ad-4efc-d2af-ee9c1fed6296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "test_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pure_data/SC4021E0.npz': <numpy.lib.npyio.NpzFile at 0x7f361c823d68>,\n",
              " 'pure_data/SC4022E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360615db00>,\n",
              " 'pure_data/SC4101E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360615dc88>,\n",
              " 'pure_data/SC4102E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360615de10>,\n",
              " 'pure_data/SC4111E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360615df98>,\n",
              " 'pure_data/SC4112E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616a160>,\n",
              " 'pure_data/SC4151E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616a2e8>,\n",
              " 'pure_data/SC4152E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616a470>,\n",
              " 'pure_data/SC4161E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616a5f8>,\n",
              " 'pure_data/SC4162E0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616a780>,\n",
              " 'pure_data/ST7011J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616a908>,\n",
              " 'pure_data/ST7012J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616aa90>,\n",
              " 'pure_data/ST7041J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616ac18>,\n",
              " 'pure_data/ST7042J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616ada0>,\n",
              " 'pure_data/ST7091J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360616af28>,\n",
              " 'pure_data/ST7092J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617a0f0>,\n",
              " 'pure_data/ST7101J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617a278>,\n",
              " 'pure_data/ST7102J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617a400>,\n",
              " 'pure_data/ST7111J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617a588>,\n",
              " 'pure_data/ST7112J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617a710>,\n",
              " 'pure_data/ST7121J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617a898>,\n",
              " 'pure_data/ST7122J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617aa20>,\n",
              " 'pure_data/ST7131J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617aba8>,\n",
              " 'pure_data/ST7132J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617ad30>,\n",
              " 'pure_data/ST7141J0.npz': <numpy.lib.npyio.NpzFile at 0x7f360617aeb8>,\n",
              " 'pure_data/ST7142J0.npz': <numpy.lib.npyio.NpzFile at 0x7f3606188080>,\n",
              " 'pure_data/ST7151J0.npz': <numpy.lib.npyio.NpzFile at 0x7f3606188208>,\n",
              " 'pure_data/ST7152J0.npz': <numpy.lib.npyio.NpzFile at 0x7f3606188390>,\n",
              " 'pure_data/ST7171J0.npz': <numpy.lib.npyio.NpzFile at 0x7f3606188518>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Yxbk9nxn59",
        "colab_type": "text"
      },
      "source": [
        "# SGD\n",
        "https://ksaluja15.github.io/Learning-Rate-Multipliers-in-Keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Y8Rp-qgLMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.legacy import interfaces\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Optimizer\n",
        "\n",
        "class LR_SGD(Optimizer):\n",
        "    \"\"\"Stochastic gradient descent optimizer.\n",
        "\n",
        "    Includes support for momentum,\n",
        "    learning rate decay, and Nesterov momentum.\n",
        "\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        momentum: float >= 0. Parameter updates momentum.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0., decay=0.,\n",
        "                 nesterov=False,multipliers=None,**kwargs):\n",
        "        super(LR_SGD, self).__init__(**kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.lr = K.variable(lr, name='lr')\n",
        "            self.momentum = K.variable(momentum, name='momentum')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "        self.initial_decay = decay\n",
        "        self.nesterov = nesterov\n",
        "        self.lr_multipliers = multipliers\n",
        "\n",
        "    @interfaces.legacy_get_updates_support\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                  K.dtype(self.decay))))\n",
        "        # momentum\n",
        "        shapes = [K.int_shape(p) for p in params]\n",
        "        moments = [K.zeros(shape) for shape in shapes]\n",
        "        self.weights = [self.iterations] + moments\n",
        "        for p, g, m in zip(params, grads, moments):\n",
        "            \n",
        "            matched_layer = [x for x in self.lr_multipliers.keys() if x in p.name]\n",
        "            if matched_layer:\n",
        "                new_lr = lr * self.lr_multipliers[matched_layer[0]]\n",
        "            else:\n",
        "                new_lr = lr\n",
        "\n",
        "            v = self.momentum * m - new_lr * g  # velocity\n",
        "            self.updates.append(K.update(m, v))\n",
        "\n",
        "            if self.nesterov:\n",
        "                new_p = p + self.momentum * v - new_lr * g\n",
        "            else:\n",
        "                new_p = p + v\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'momentum': float(K.get_value(self.momentum)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'nesterov': self.nesterov}\n",
        "        base_config = super(LR_SGD, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items())) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm7urQlIxrQv",
        "colab_type": "text"
      },
      "source": [
        "# Adam\n",
        "https://erikbrorson.github.io/2018/04/30/Adam-with-learning-rate-multipliers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-K2un6kxtkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.legacy import interfaces\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Optimizer\n",
        "\n",
        "class Adam_lr_mult(Optimizer):\n",
        "    \"\"\"Adam optimizer.\n",
        "    Adam optimizer, with learning rate multipliers built on Keras implementation\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
        "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
        "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
        "            algorithm from the paper \"On the Convergence of Adam and\n",
        "            Beyond\".\n",
        "    # References\n",
        "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
        "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
        "        \n",
        "    AUTHOR: Erik Brorson\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, beta_1=0.9, beta_2=0.999,\n",
        "                 epsilon=None, decay=0.5, amsgrad=False,\n",
        "                 multipliers=None, debug_verbose=False,**kwargs):\n",
        "        super(Adam_lr_mult, self).__init__(**kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.lr = K.variable(lr, name='lr')\n",
        "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
        "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "        if epsilon is None:\n",
        "            epsilon = K.epsilon()\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_decay = decay\n",
        "        self.amsgrad = amsgrad\n",
        "        self.multipliers = multipliers\n",
        "        self.debug_verbose = debug_verbose\n",
        "\n",
        "    @interfaces.legacy_get_updates_support\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                  K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
        "                     (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        if self.amsgrad:\n",
        "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        else:\n",
        "            vhats = [K.zeros(1) for _ in params]\n",
        "        self.weights = [self.iterations] + ms + vs + vhats\n",
        "\n",
        "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
        "\n",
        "            # Learning rate multipliers\n",
        "            if self.multipliers:\n",
        "                multiplier = [mult for mult in self.multipliers if mult in p.name]\n",
        "            else:\n",
        "                multiplier = None\n",
        "            if multiplier:\n",
        "                new_lr_t = lr_t * self.multipliers[multiplier[0]]\n",
        "                if self.debug_verbose:\n",
        "                    print('Setting {} to learning rate {}'.format(multiplier[0], new_lr_t))\n",
        "                    print(K.get_value(new_lr_t))\n",
        "            else:\n",
        "                new_lr_t = lr_t\n",
        "                if self.debug_verbose:\n",
        "                    print('No change in learning rate {}'.format(p.name))\n",
        "                    print(K.get_value(new_lr_t))\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "            if self.amsgrad:\n",
        "                vhat_t = K.maximum(vhat, v_t)\n",
        "                p_t = p - new_lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
        "                self.updates.append(K.update(vhat, vhat_t))\n",
        "            else:\n",
        "                p_t = p - new_lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            new_p = p_t\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'beta_1': float(K.get_value(self.beta_1)),\n",
        "                  'beta_2': float(K.get_value(self.beta_2)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'epsilon': self.epsilon,\n",
        "                  'amsgrad': self.amsgrad,\n",
        "                  'multipliers':self.multipliers}\n",
        "        base_config = super(Adam_lr_mult, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrSes62___zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPgxOSZ3q4Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balance(dict_files):\n",
        "  pre0=[]\n",
        "  y0_pre=[]\n",
        "  pre1=[]\n",
        "  y1_pre=[]\n",
        "  pre2=[]\n",
        "  y2_pre=[]\n",
        "  pre3=[]\n",
        "  y3_pre=[]\n",
        "  pre4=[]\n",
        "  y4_pre=[]\n",
        "  u=0\n",
        "  v=0\n",
        "  w=0\n",
        "  x=0\n",
        "  y=0\n",
        "  for record in tqdm(dict_files):\n",
        "    all_rows = dict_files[record]['x']\n",
        "    all_cols=dict_files[record]['y']\n",
        "    for i in range(len(all_cols)):\n",
        "      if all_cols[i]==0 :\n",
        "        if x==0:\n",
        "          pre0=all_rows[i]\n",
        "          x=x+1\n",
        "          y0_pre+=[0]\n",
        "          \n",
        "        else: \n",
        "             pre0 = np.concatenate((pre0,all_rows[i]),axis=0)\n",
        "             y0_pre+=[0]\n",
        "      \n",
        "      if all_cols[i]==1 :\n",
        "        if u==0:\n",
        "          pre1=all_rows[i]\n",
        "          u=u+1\n",
        "          y1_pre+=[1]\n",
        "          \n",
        "        else: \n",
        "             pre1 = np.concatenate((pre1,all_rows[i]),axis=0)\n",
        "             y1_pre+=[1]\n",
        "              \n",
        "      if all_cols[i]==2 :\n",
        "        if y==0:\n",
        "          pre2=all_rows[i]\n",
        "          y=y+1\n",
        "          y2_pre+=[2]\n",
        "          \n",
        "        else: \n",
        "             pre2 = np.concatenate((pre2,all_rows[i]),axis=0)\n",
        "             y2_pre+=[2]       \n",
        "       \n",
        "      \n",
        "      \n",
        "      \n",
        "      if all_cols[i]==3 :\n",
        "        if v==0:\n",
        "          pre3=all_rows[i]\n",
        "          v=v+1\n",
        "          y3_pre+=[3]\n",
        "          \n",
        "        else: \n",
        "             pre3 = np.concatenate((pre3,all_rows[i]),axis=0)\n",
        "             y3_pre+=[3]\n",
        "              \n",
        "              \n",
        "              \n",
        "      if all_cols[i]==4 :\n",
        "        if w==0:\n",
        "          pre4=all_rows[i]\n",
        "          w=w+1\n",
        "          y4_pre+=[4]\n",
        "          \n",
        "        else: \n",
        "             pre4 = np.concatenate((pre4,all_rows[i]),axis=0)\n",
        "             y4_pre+=[4] \n",
        "  pre0= pre0.reshape(int(len(pre0)/3000),3000,1)      \n",
        "  pre1= pre1.reshape(int(len(pre1)/3000),3000,1)\n",
        "  pre2= pre2.reshape(int(len(pre2)/3000),3000,1)\n",
        "  pre3= pre3.reshape(int(len(pre3)/3000),3000,1)\n",
        "  pre4= pre4.reshape(int(len(pre4)/3000),3000,1)\n",
        "  \n",
        "  return pre0,pre1,pre2,pre3,pre4            \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgWB_eoDuuVR",
        "colab_type": "code",
        "outputId": "768a95a6-d8bc-40aa-c561-ad96d90354d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "balance=balance(train_dict)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 34/34 [10:30<00:00, 32.65s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2a_brgIvU-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFgBpxTumRY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pywt\n",
        "# creat a matrix\n",
        "def wtdata(X1):\n",
        "    h=0\n",
        "    for i in range(len(X1[0])):\n",
        "    \n",
        "        h=h+1\n",
        "        a=X1[0][i].reshape(3000)\n",
        "        cA5, cD5,cD4, cD3, cD2, cD1= pywt.wavedec(a, 'db4', level=5)\n",
        "        F_C0 = np.concatenate((cA5.transpose(), cD5.transpose(),cD4.transpose(), cD3.transpose(), cD2.transpose(), cD1.transpose()))\n",
        "        if h >1:\n",
        "          X=np.concatenate((X,F_C0.reshape(len(F_C0),1)),axis=1)\n",
        "        else:\n",
        "          X=F_C0.reshape(len(F_C0),1)\n",
        "    \n",
        "   \n",
        "    \n",
        "\n",
        "    return X\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBfy2BOIP7p6",
        "colab_type": "text"
      },
      "source": [
        "# balance data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49yoxGMLD8dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            from sklearn.utils import shuffle \n",
        "            \n",
        "            import random\n",
        "            from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "            WINDOW_SIZE = 100\n",
        "            WINDOW_SIZE1 = 500\n",
        "\n",
        "            def rescale_array(X):\n",
        "                X = X / 20\n",
        "                X = np.clip(X, -10, 10)\n",
        "                return X\n",
        "\n",
        "  \n",
        "            idx = np.random.randint(len(balance[0]), size=4000)\n",
        "            a=balance[0][idx,:]\n",
        "            aa=numpy.ones((4000))*0\n",
        "  \n",
        "            idx = np.random.randint(len(balance[1]), size=4000)\n",
        "            b=balance[1][idx,:]\n",
        "            bb=numpy.ones((4000))*1\n",
        "  \n",
        "            idx = np.random.randint(len(balance[2]), size=4000)\n",
        "            c=balance[2][idx,:]\n",
        "            cc=numpy.ones((4000))*2\n",
        "  \n",
        "            idx = np.random.randint(len(balance[3]), size=4000)\n",
        "            d=balance[3][idx,:]\n",
        "            dd=numpy.ones((4000))*3\n",
        "  \n",
        "            idx = np.random.randint(len(balance[4]), size=4000)\n",
        "            e=balance[4][idx,:]\n",
        "            ee=numpy.ones((4000))*4\n",
        "  \n",
        "            X1 = np.concatenate((a,b,c,d,e),axis=0)\n",
        "            Y1=np.concatenate((aa,bb,cc,dd,ee),axis=0)\n",
        "            X1, Y1= shuffle(X1, Y1, random_state=0)\n",
        "            X1 = np.expand_dims(X1, 0)\n",
        "           \n",
        "            Y1 = np.expand_dims(Y1, -1)\n",
        "            Y1 = np.expand_dims(Y1, 0)\n",
        "\n",
        "            X1=aug_X(X1)\n",
        "                \n",
        "            X1 = rescale_array(X1)\n",
        "            \n",
        "            a=wtdata(X1).transpose()\n",
        "            X = np.expand_dims(a, 0)\n",
        "            X= np.expand_dims(X, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WD4ny9Q-yzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 100\n",
        "WINDOW_SIZE1 = 500\n",
        "\n",
        "def rescale_array(X):\n",
        "    X = X / 20\n",
        "    X = np.clip(X, -10, 10)\n",
        "    return X\n",
        "\n",
        "\n",
        "def aug_X(X):\n",
        "    scale = 1 + np.random.uniform(-0.1, 0.1)\n",
        "    offset = np.random.uniform(-0.1, 0.1)\n",
        "    noise = np.random.normal(scale=0.05, size=X.shape)\n",
        "    X = scale * X + offset + noise\n",
        "    return X\n",
        "\n",
        "def gen(dict_files, aug=False):\n",
        "    j=0\n",
        "    while True:\n",
        "        \n",
        "          record_name = random.choice(list(dict_files.keys()))\n",
        "          batch_data = dict_files[record_name]\n",
        "          all_rows = batch_data['x']\n",
        "        \n",
        "          for i in range(100):\n",
        "            idx = np.random.randint(len(balance[0]), size=20)\n",
        "            a=balance[0][idx,:]\n",
        "            aa=numpy.ones((20))*0\n",
        "  \n",
        "            idx = np.random.randint(len(balance[1]), size=20)\n",
        "            b=balance[1][idx,:]\n",
        "            bb=numpy.ones((20))*1\n",
        "  \n",
        "            idx = np.random.randint(len(balance[2]), size=20)\n",
        "            c=balance[2][idx,:]\n",
        "            cc=numpy.ones((20))*2\n",
        "  \n",
        "            idx = np.random.randint(len(balance[3]), size=20)\n",
        "            d=balance[3][idx,:]\n",
        "            dd=numpy.ones((20))*3\n",
        "  \n",
        "            idx = np.random.randint(len(balance[4]), size=20)\n",
        "            e=balance[4][idx,:]\n",
        "            ee=numpy.ones((20))*4\n",
        "  \n",
        "            X1 = np.concatenate((a,b,c,d,e),axis=0)\n",
        "            Y1=np.concatenate((aa,bb,cc,dd,ee),axis=0)\n",
        "            X1, Y1= shuffle(X1, Y1, random_state=0)\n",
        "             \n",
        "             \n",
        "          \n",
        "            \n",
        "            X1 = np.expand_dims(X1, 0)\n",
        "            Y1 = np.expand_dims(Y1, -1)\n",
        "            Y1 = np.expand_dims(Y1, 0)\n",
        "\n",
        "            if aug:\n",
        "                X1=aug_X(X1)\n",
        "                \n",
        "            X1 = rescale_array(X1)\n",
        "            \n",
        "            a=wtdata(X1).transpose()\n",
        "            X = np.expand_dims(a, 0)\n",
        "            X= np.expand_dims(X, -1)\n",
        "    \n",
        "            X2 = rescale_array(X)\n",
        "            \n",
        "            \n",
        "\n",
        "            yield [X1,X1,X2],Y1\n",
        "            \n",
        "        \n",
        "            \n",
        "\n",
        "\n",
        "def chunker(seq, size=WINDOW_SIZE):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXtnZDDb-1N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 100\n",
        "WINDOW_SIZE1 = 500\n",
        "\n",
        "def rescale_array(X):\n",
        "    X = X / 20\n",
        "    X = np.clip(X, -10, 10)\n",
        "    return X\n",
        "\n",
        "\n",
        "def aug_X(X):\n",
        "    scale = 1 + np.random.uniform(-0.1, 0.1)\n",
        "    offset = np.random.uniform(-0.1, 0.1)\n",
        "    noise = np.random.normal(scale=0.05, size=X.shape)\n",
        "    X = scale * X + offset + noise\n",
        "    return X\n",
        "\n",
        "def gen(dict_files, aug=False):\n",
        "    j=0\n",
        "    while True:\n",
        "        \n",
        "        record_name = random.choice(list(dict_files.keys()))\n",
        "        batch_data = dict_files[record_name]\n",
        "        all_rows = batch_data['x']\n",
        "        if j<50:\n",
        "          j=j+1\n",
        "          for i in range(2):\n",
        "            idx = np.random.randint(len(balance[0]), size=20)\n",
        "            a=balance[0][idx,:]\n",
        "            aa=numpy.ones((20))*0\n",
        "  \n",
        "            idx = np.random.randint(len(balance[1]), size=20)\n",
        "            b=balance[1][idx,:]\n",
        "            bb=numpy.ones((20))*1\n",
        "  \n",
        "            idx = np.random.randint(len(balance[2]), size=20)\n",
        "            c=balance[2][idx,:]\n",
        "            cc=numpy.ones((20))*2\n",
        "  \n",
        "            idx = np.random.randint(len(balance[3]), size=20)\n",
        "            d=balance[3][idx,:]\n",
        "            dd=numpy.ones((20))*3\n",
        "  \n",
        "            idx = np.random.randint(len(balance[4]), size=20)\n",
        "            e=balance[4][idx,:]\n",
        "            ee=numpy.ones((20))*4\n",
        "  \n",
        "            X1 = np.concatenate((a,b,c,d,e),axis=0)\n",
        "            Y1=np.concatenate((aa,bb,cc,dd,ee),axis=0)\n",
        "            X1, Y1= shuffle(X1, Y1, random_state=0)\n",
        "             \n",
        "             \n",
        "          \n",
        "            \n",
        "            X1 = np.expand_dims(X1, 0)\n",
        "            Y1 = np.expand_dims(Y1, -1)\n",
        "            Y1 = np.expand_dims(Y1, 0)\n",
        "\n",
        "            if aug:\n",
        "                X1=aug_X(X1)\n",
        "                \n",
        "            X1 = rescale_array(X1)\n",
        "            \n",
        "            a=wtdata(X1).transpose()\n",
        "            X = np.expand_dims(a, 0)\n",
        "            X= np.expand_dims(X, -1)\n",
        "    \n",
        "            X2 = rescale_array(X)\n",
        "            \n",
        "            \n",
        "\n",
        "            yield [X1,X2],Y1\n",
        "            \n",
        "        else:\n",
        "          for i in range(7):\n",
        "            start_index = random.choice(range(all_rows.shape[0]-WINDOW_SIZE))\n",
        "\n",
        "            X1 = all_rows[start_index:start_index+WINDOW_SIZE, ...]\n",
        "            Y1 = batch_data['y'][start_index:start_index+WINDOW_SIZE]\n",
        "            \n",
        "            ############\n",
        "            \n",
        "            \n",
        "            s=record_name+'.npy'\n",
        "            X2=np.load(s)\n",
        "            \n",
        "            X2=X2.reshape((len(X2)),92,1)\n",
        "            X2=X2[start_index:start_index+WINDOW_SIZE, ...]\n",
        "            \n",
        "\n",
        "            X1 = np.expand_dims(X1, 0)\n",
        "            X2 = np.expand_dims(X2, 0)\n",
        "            Y1 = np.expand_dims(Y1, -1)\n",
        "            Y1 = np.expand_dims(Y1, 0)\n",
        "\n",
        "            if aug:\n",
        "                X1=aug_X(X1)\n",
        "                \n",
        "            X1 = rescale_array(X1)\n",
        "            \n",
        "            a=wtdata(X1).transpose()\n",
        "            X = np.expand_dims(a, 0)\n",
        "            X= np.expand_dims(X, -1)\n",
        "    \n",
        "            #X2 = rescale_array(X)\n",
        "\n",
        "            yield [X1,X2],Y1\n",
        "            \n",
        "\n",
        "\n",
        "def chunker(seq, size=WINDOW_SIZE):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Z4sGQzQDZZ",
        "colab_type": "text"
      },
      "source": [
        "# Data generat to model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4loSCYwzeWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 100\n",
        "WINDOW_SIZE1 = 500\n",
        "\n",
        "def rescale_array(X):\n",
        "    X = X / 20\n",
        "    X = np.clip(X, -10, 10)\n",
        "    return X\n",
        "\n",
        "\n",
        "def aug_X(X):\n",
        "    scale = 1 + np.random.uniform(-0.1, 0.1)\n",
        "    offset = np.random.uniform(-0.1, 0.1)\n",
        "    noise = np.random.normal(scale=0.05, size=X.shape)\n",
        "    X = scale * X + offset + noise\n",
        "    return X\n",
        "def gen_j(dict_files, aug=False):\n",
        "    j=0\n",
        "    while True:\n",
        "        \n",
        "        record_name = random.choice(list(dict_files.keys()))\n",
        "        batch_data = dict_files[record_name]\n",
        "        all_rows = batch_data['x']\n",
        "        \n",
        "        for i in range(7):\n",
        "            start_index = random.choice(range(all_rows.shape[0]-WINDOW_SIZE))\n",
        "\n",
        "            X1 = all_rows[start_index:start_index+WINDOW_SIZE, ...]\n",
        "            Y1 = batch_data['y'][start_index:start_index+WINDOW_SIZE]\n",
        "            \n",
        "            ############\n",
        "            from keras.utils import np_utils\n",
        "            y_train_onehot = np_utils.to_categorical(Y1)\n",
        "            \n",
        "            s=record_name+'.npy'\n",
        "            X2=np.load(s)\n",
        "            \n",
        "            X2=X2.reshape((len(X2)),92,1)\n",
        "            X2=X2[start_index:start_index+WINDOW_SIZE, ...]\n",
        "            \n",
        "\n",
        "            X1 = np.expand_dims(X1, 0)\n",
        "            X2 = np.expand_dims(X2, 0)\n",
        "            Y1 = np.expand_dims(Y1, -1)\n",
        "            Y1 = np.expand_dims(Y1, 0)\n",
        "\n",
        "            if aug:\n",
        "                X1=aug_X(X1)\n",
        "                \n",
        "            X1 = rescale_array(X1)\n",
        "            a=wtdata(X1).transpose()\n",
        "            X = np.expand_dims(a, 0)\n",
        "            X= np.expand_dims(X, -1)\n",
        "    \n",
        "            X2 = rescale_array(X)\n",
        "          \n",
        "            \n",
        "\n",
        "            yield [X1,X1,X2],Y1\n",
        "            \n",
        "            \n",
        "            \n",
        "def chunker(seq, size=WINDOW_SIZE):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLUciq1RQWdv",
        "colab_type": "text"
      },
      "source": [
        "# Coefficients of each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qs8ZIrSx9u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_multipliers1 = {}\n",
        "learning_rate_multipliers1['layer_11'] = 1\n",
        "learning_rate_multipliers1['layer_12'] = 5\n",
        "learning_rate_multipliers1['layer_13'] = 1\n",
        "learning_rate_multipliers1['layer_14'] = 1\n",
        "learning_rate_multipliers1['layer_15'] = 5\n",
        "learning_rate_multipliers1['layer_16'] = 1\n",
        "learning_rate_multipliers1['layer_17'] = 1\n",
        "learning_rate_multipliers1['layer_18'] = 5\n",
        "learning_rate_multipliers1['layer_19'] = 1\n",
        "learning_rate_multipliers2 = {}\n",
        "learning_rate_multipliers2['layer_21'] = 1\n",
        "learning_rate_multipliers2['layer_22'] = 5\n",
        "learning_rate_multipliers2['layer_23'] = 1\n",
        "learning_rate_multipliers2['layer_24'] = 1\n",
        "learning_rate_multipliers2['layer_25'] = 5\n",
        "learning_rate_multipliers2['layer_26'] =1\n",
        "learning_rate_multipliers2['layer_27'] =1\n",
        "learning_rate_multipliers2['layer_28'] =5\n",
        "learning_rate_multipliers2['layer_29'] =1\n",
        "learning_rate_multipliers = {}\n",
        "learning_rate_multipliers['layer_11'] = 1\n",
        "learning_rate_multipliers['layer_12'] = 5\n",
        "learning_rate_multipliers['layer_13'] = 1\n",
        "learning_rate_multipliers['layer_14'] = 1\n",
        "learning_rate_multipliers['layer_15'] = 5\n",
        "learning_rate_multipliers['layer_16'] = 1\n",
        "learning_rate_multipliers['layer_17'] = 1\n",
        "learning_rate_multipliers['layer_18'] = 5\n",
        "learning_rate_multipliers['layer_19'] = 1\n",
        "learning_rate_multipliers['layer_21'] = 1\n",
        "learning_rate_multipliers['layer_22'] = 5\n",
        "learning_rate_multipliers['layer_23'] = 1\n",
        "learning_rate_multipliers['layer_24'] = 1\n",
        "learning_rate_multipliers['layer_25'] = 5\n",
        "learning_rate_multipliers['layer_26'] =1\n",
        "learning_rate_multipliers['layer_27'] =1\n",
        "learning_rate_multipliers['layer_28'] =5\n",
        "learning_rate_multipliers['layer_29'] =1\n",
        "learning_rate_multipliers['layer_1'] = 1\n",
        "learning_rate_multipliers['layer_2'] = 1\n",
        "learning_rate_multipliers['layer_3'] = 5\n",
        "learning_rate_multipliers['layer_4'] = 1\n",
        "base_lr = 10\n",
        "momentum = 0.9\n",
        "\n",
        "optimizer1 = LR_SGD(lr=base_lr, momentum=momentum, decay=0.1, nesterov=False,multipliers = learning_rate_multipliers1)\n",
        "optimizer2 = LR_SGD(lr=base_lr, momentum=momentum, decay=0.1, nesterov=False,multipliers = learning_rate_multipliers2)\n",
        "\n",
        "optimizer = LR_SGD(lr=base_lr, momentum=momentum, decay=0.0, nesterov=False,multipliers = learning_rate_multipliers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "adam_with_lr_multipliers1 = Adam_lr_mult(multipliers=learning_rate_multipliers1)\n",
        "adam_with_lr_multipliers2 = Adam_lr_mult(multipliers=learning_rate_multipliers2)\n",
        "adam_with_lr_multipliers = Adam_lr_mult(multipliers=learning_rate_multipliers)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49TZjdxbHgxC",
        "colab_type": "text"
      },
      "source": [
        "# simple two input model-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSdhDlhx-1Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_11',input_shape=(3000, 1)))  \n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_12'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.3))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_13'))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_14'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_15'))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_16'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.2))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_17'))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_18'))\n",
        "    #model1.add(GlobalMaxPool1D())\n",
        "    convout2 = GlobalMaxPool1D()\n",
        "    model1.add(convout2)\n",
        "    model1.add((Dense(64, activation=activations.relu, name='layer_19')))\n",
        "    #model1.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n",
        "    #model1.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers1, metrics=['accuracy'])\n",
        "\n",
        "    #model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_21', input_shape=(3000, 1)))  \n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_22'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.5))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_23'))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_24'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_25'))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_26'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.3))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_27'))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_28'))\n",
        "    model2.add(GlobalMaxPool1D())\n",
        "    model2.add(Dropout(rate=0.4))\n",
        "    model2.add((Dense(64, activation=activations.relu, name='layer_29')))\n",
        "    #model2.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer2, metrics=['accuracy'])\n",
        "    #model2.compile(loss='categorical_crossentropy', optimizer=adam_with_lr_multipliers2, metrics=['accuracy'])\n",
        "\n",
        "    #model2.compile(optimizers.Adam(lr=0.01, decay=0.01), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    #model.summary()\n",
        "    \n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 3000, 1))\n",
        "    seq_input2 = Input(shape=(None, 3000, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    # for layer in base_model.layers:\n",
        "    # layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    \n",
        "    encoded_sequence1 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                              kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_1')(encoded_sequence1))\n",
        "    \n",
        "    \n",
        "    \n",
        "    #encoded_sequence1=BatchNormalization()(encoded_sequence1)\n",
        "\n",
        "    \n",
        "    \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    \n",
        "    encoded_sequence2 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_2')(encoded_sequence2))\n",
        "    \n",
        "    \n",
        "    #encoded_sequence2=BatchNormalization()(encoded_sequence2)\n",
        "\n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2])\n",
        "    #encoded_sequence=BatchNormalization()(encoded_sequence)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_3')(encoded_sequence))\n",
        "    \n",
        "    #encoded_sequence=Dense(64, activation=activations.relu)(encoded_sequence)\n",
        "    \n",
        "    \n",
        "    \n",
        "    out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=3, activation=\"softmax\", padding=\"same\", name='layer_4')(encoded_sequence)\n",
        "    #crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    #out = crf(encoded_sequence)\n",
        "\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2], out)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.optimizer, crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "    #model.compile(crf.loss_function ,optimizer=optimizer, metrics=[crf.accuracy])\n",
        "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CknC-2E4tz7g",
        "colab_type": "text"
      },
      "source": [
        "# Feature+crf+lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTqvCK0rtyJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    #model2.add((Dense(92, activation=activations.relu,input_shape=(92, 1))))\n",
        "    #model2.add(Bidirectional(LSTM(100, return_sequences=True,input_shape=(92, 1))))\n",
        "    model1.add((Dense(100, activation=activations.relu,input_shape=(92, 1))))\n",
        "    #model2.add(Bidirectional(LSTM(90, return_sequences=True,recurrent_dropout=0.1)))\n",
        "    #model2.add(Bidirectional(LSTM(70, return_sequences=True,recurrent_dropout=0.2)))\n",
        "    #model2.add(Bidirectional(LSTM(60, return_sequences=True,recurrent_dropout=0.3)))\n",
        "    #model2.add(GlobalMaxPool1D())\n",
        "    model1.add(GlobalAveragePooling1D())\n",
        "    model1.add((Dense(100, activation=activations.relu)))\n",
        "    model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    #model2.add((Dense(92, activation=activations.relu,input_shape=(92, 1))))\n",
        "    #model2.add(Bidirectional(LSTM(100, return_sequences=True,input_shape=(92, 1))))\n",
        "    model2.add((Dense(100, activation=activations.relu,input_shape=(92, 1))))\n",
        "    #model2.add(Bidirectional(LSTM(90, return_sequences=True,recurrent_dropout=0.1)))\n",
        "    #model2.add(Bidirectional(LSTM(70, return_sequences=True,recurrent_dropout=0.2)))\n",
        "    #model2.add(Bidirectional(LSTM(60, return_sequences=True,recurrent_dropout=0.3)))\n",
        "    #model2.add(GlobalMaxPool1D())\n",
        "    model2.add(GlobalAveragePooling1D())\n",
        "    model2.add((Dense(100, activation=activations.relu)))\n",
        "    model2.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    #model.summary()\n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 92, 1))\n",
        "    seq_input2 = Input(shape=(None, 92, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    for layer in base_model1.layers:\n",
        "         layer.trainable = False\n",
        "        \n",
        "    for layer in base_model2.layers:\n",
        "         layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "    encoded_sequence1 = Dropout(rate=0.5)(encoded_sequence1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "\n",
        "   \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    encoded_sequence2 = Dropout(rate=0.5)(encoded_sequence2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    \n",
        "   \n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2])\n",
        " #   encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "  #                                                            kernel_size=3,\n",
        "   #                                                           activation=\"linear\",\n",
        "    #                                                          padding=\"same\")(encoded_sequence))\n",
        "    \n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    encoded_sequence = Dropout(rate=0.5)(encoded_sequence)\n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "     #                                                         kernel_size=3,\n",
        "      #                                                        activation=\"linear\",\n",
        "       #                                                       padding=\"same\")(encoded_sequence))\n",
        "    crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    out = crf(encoded_sequence)\n",
        "\n",
        "    \n",
        "     \n",
        "    #out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=1, activation=\"softmax\", padding=\"same\")(encoded_sequence)\n",
        "    \n",
        "    #out=encoded_sequence\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2], out)\n",
        "\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    model.compile(optimizers.Adam(0.001,amsgrad=True), crf.loss_function, metrics=[crf.accuracy])\n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcZdFo6dtQtX",
        "colab_type": "text"
      },
      "source": [
        "# EEG+Feature+crf+lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hitHDGKw2UHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", input_shape=(3000, 1)))  \n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.01))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.01))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(GlobalMaxPool1D())\n",
        "    model1.add(Dropout(rate=0.01))\n",
        "    model1.add((Dense(64, activation=activations.relu)))\n",
        "   \n",
        "    model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    #model2.add((Dense(92, activation=activations.relu,input_shape=(92, 1))))\n",
        "    #model2.add(Bidirectional(LSTM(100, return_sequences=True,input_shape=(92, 1))))\n",
        "    model2.add((Dense(100, activation=activations.relu,input_shape=(92, 1))))\n",
        "    #model2.add(Bidirectional(LSTM(90, return_sequences=True,recurrent_dropout=0.1)))\n",
        "    #model2.add(Bidirectional(LSTM(70, return_sequences=True,recurrent_dropout=0.2)))\n",
        "    #model2.add(Bidirectional(LSTM(60, return_sequences=True,recurrent_dropout=0.3)))\n",
        "    #model2.add(GlobalMaxPool1D())\n",
        "    model2.add(GlobalAveragePooling1D())\n",
        "    model2.add((Dense(100, activation=activations.relu)))\n",
        "    model2.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    #model.summary()\n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 3000, 1))\n",
        "    seq_input2 = Input(shape=(None, 92, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    for layer in base_model1.layers:\n",
        "         layer.trainable = False\n",
        "        \n",
        "    for layer in base_model2.layers:\n",
        "         layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "    encoded_sequence1 = Dropout(rate=0.5)(encoded_sequence1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "    encoded_sequence1=BatchNormalization()(encoded_sequence1)\n",
        "\n",
        "   \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    encoded_sequence2 = Dropout(rate=0.5)(encoded_sequence2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    encoded_sequence2=BatchNormalization()(encoded_sequence2)\n",
        "    \n",
        "   \n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2])\n",
        " #   encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "  #                                                            kernel_size=3,\n",
        "   #                                                           activation=\"linear\",\n",
        "    #                                                          padding=\"same\")(encoded_sequence))\n",
        "    \n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    encoded_sequence = Dropout(rate=0.5)(encoded_sequence)\n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "     #                                                         kernel_size=3,\n",
        "      #                                                        activation=\"linear\",\n",
        "       #                                                       padding=\"same\")(encoded_sequence))\n",
        "    crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    out = crf(encoded_sequence)\n",
        "\n",
        "    \n",
        "     \n",
        "    #out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=1, activation=\"softmax\", padding=\"same\")(encoded_sequence)\n",
        "    \n",
        "    #out=encoded_sequence\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2], out)\n",
        "\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    model.compile(optimizers.Adam(0.001,amsgrad=True), crf.loss_function, metrics=[crf.accuracy])\n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ2jDpidtVoI",
        "colab_type": "text"
      },
      "source": [
        "# EEG+crf+lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AOgqJQ4aIbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", input_shape=(3000, 1)))  \n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.01))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.01))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model1.add(GlobalMaxPool1D())\n",
        "    model1.add(Dropout(rate=0.01))\n",
        "    model1.add((Dense(64, activation=activations.relu)))\n",
        "   \n",
        "    model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", input_shape=(3000, 1)))  \n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.01))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.01))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\"))\n",
        "    model2.add(GlobalMaxPool1D())\n",
        "    model2.add(Dropout(rate=0.01))\n",
        "    model2.add((Dense(64, activation=activations.relu)))\n",
        "   \n",
        "    model2.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    #model.summary()\n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 3000, 1))\n",
        "    seq_input2 = Input(shape=(None, 3000, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    for layer in base_model1.layers:\n",
        "         layer.trainable = False\n",
        "        \n",
        "    for layer in base_model2.layers:\n",
        "         layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "    encoded_sequence1 = Dropout(rate=0.5)(encoded_sequence1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "\n",
        "   \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    encoded_sequence2 = Dropout(rate=0.5)(encoded_sequence2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    \n",
        "   \n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2])\n",
        " #   encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "  #                                                            kernel_size=3,\n",
        "   #                                                           activation=\"linear\",\n",
        "    #                                                          padding=\"same\")(encoded_sequence))\n",
        "    \n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    encoded_sequence = Dropout(rate=0.5)(encoded_sequence)\n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "     #                                                         kernel_size=3,\n",
        "      #                                                        activation=\"linear\",\n",
        "       #                                                       padding=\"same\")(encoded_sequence))\n",
        "    crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    out = crf(encoded_sequence)\n",
        "\n",
        "    \n",
        "     \n",
        "    #out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=1, activation=\"softmax\", padding=\"same\")(encoded_sequence)\n",
        "    \n",
        "    #out=encoded_sequence\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2], out)\n",
        "\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    model.compile(optimizers.Adam(0.001,amsgrad=True), crf.loss_function, metrics=[crf.accuracy])\n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4H7nWamqy_O",
        "colab_type": "text"
      },
      "source": [
        "# Diff lr_rate+Lstm+EEG\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCZ7l-6OfJTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_11',input_shape=(3000, 1)))  \n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_12'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.3))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_13'))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_14'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_15'))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_16'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.2))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_17'))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_18'))\n",
        "    #model1.add(GlobalMaxPool1D())\n",
        "    convout2 = GlobalMaxPool1D()\n",
        "    model1.add(convout2)\n",
        "    model1.add((Dense(64, activation=activations.relu, name='layer_19')))\n",
        "    #model1.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n",
        "    #model1.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers1, metrics=['accuracy'])\n",
        "\n",
        "    #model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_21', input_shape=(3000, 1)))  \n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_22'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.5))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_23'))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_24'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_25'))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_26'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.3))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_27'))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_28'))\n",
        "    model2.add(GlobalMaxPool1D())\n",
        "    model2.add(Dropout(rate=0.4))\n",
        "    model2.add((Dense(64, activation=activations.relu, name='layer_29')))\n",
        "    #model2.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer2, metrics=['accuracy'])\n",
        "    #model2.compile(loss='categorical_crossentropy', optimizer=adam_with_lr_multipliers2, metrics=['accuracy'])\n",
        "\n",
        "    #model2.compile(optimizers.Adam(lr=0.01, decay=0.01), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    #model.summary()\n",
        "    \n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 3000, 1))\n",
        "    seq_input2 = Input(shape=(None, 3000, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    for layer in base_model1.layers:\n",
        "         layer.trainable = False\n",
        "        \n",
        "    for layer in base_model2.layers:\n",
        "         layer.trainable = False\n",
        "    # for layer in base_model.layers:\n",
        "    # layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "    encoded_sequence1 = Dropout(rate=0.5)(encoded_sequence1)\n",
        "    encoded_sequence1 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence1)\n",
        "\n",
        "   \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    encoded_sequence2 = Dropout(rate=0.5)(encoded_sequence2)\n",
        "    encoded_sequence2 = Bidirectional(LSTM(50, return_sequences=True))(encoded_sequence2)\n",
        "    \n",
        "   \n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2])\n",
        " #   encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "  #                                                            kernel_size=3,\n",
        "   #                                                           activation=\"linear\",\n",
        "    #                                                          padding=\"same\")(encoded_sequence))\n",
        "    \n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    encoded_sequence = Dropout(rate=0.5)(encoded_sequence)\n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        " \n",
        "    out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=3, activation=\"softmax\", padding=\"same\", name='layer_4')(encoded_sequence)\n",
        "    #crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    #out = crf(encoded_sequence)\n",
        "\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2], out)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.optimizer, crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "    #model.compile(crf.loss_function ,optimizer=optimizer, metrics=[crf.accuracy])\n",
        "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJjLm6PkvhZE",
        "colab_type": "text"
      },
      "source": [
        "# Diff lr_rate+CRF+lstm+EEG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSOQ0HNPvcek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_11',input_shape=(3000, 1)))  \n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_12'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.3))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_13'))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_14'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    #model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_15'))\n",
        "    #model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_16'))\n",
        "    #model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(BatchNormalization())\n",
        "    model1.add(SpatialDropout1D(rate=0.2))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_17'))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_18'))\n",
        "    #model1.add(GlobalMaxPool1D())\n",
        "    convout2 = GlobalMaxPool1D()\n",
        "    model1.add(convout2)\n",
        "    model1.add((Dense(64, activation=activations.relu, name='layer_19')))\n",
        "    #model1.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n",
        "    #model1.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers1, metrics=['accuracy'])\n",
        "\n",
        "    #model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_21', input_shape=(3033, 1)))  \n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_22'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.5))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_23'))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_24'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    #model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_25'))\n",
        "    #model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_26'))\n",
        "    #model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(BatchNormalization())\n",
        "    model2.add(SpatialDropout1D(rate=0.3))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_27'))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_28'))\n",
        "    model2.add(GlobalMaxPool1D())\n",
        "    model2.add(Dropout(rate=0.4))\n",
        "    model2.add((Dense(64, activation=activations.relu, name='layer_29')))\n",
        "    #model2.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer2, metrics=['accuracy'])\n",
        "    #model2.compile(loss='categorical_crossentropy', optimizer=adam_with_lr_multipliers2, metrics=['accuracy'])\n",
        "\n",
        "    #model2.compile(optimizers.Adam(lr=0.01, decay=0.01), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "\n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    #model.summary()\n",
        "    \n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 3000, 1))\n",
        "    seq_input2 = Input(shape=(None, 3033, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    # for layer in base_model.layers:\n",
        "    # layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "   # for layer in base_model1.layers:\n",
        "    #     layer.trainable = False\n",
        "        \n",
        "    #for layer in base_model2.layers:\n",
        "     #    layer.trainable = False\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    \n",
        "    encoded_sequence1 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                              kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_1')(encoded_sequence1))\n",
        "    \n",
        "    \n",
        "    \n",
        "    #encoded_sequence1=BatchNormalization()(encoded_sequence1)\n",
        "\n",
        "    \n",
        "    \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    \n",
        "    encoded_sequence2 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_2')(encoded_sequence2))\n",
        "    \n",
        "    \n",
        "    #encoded_sequence2=BatchNormalization()(encoded_sequence2)\n",
        "\n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2])\n",
        "    encoded_sequence=BatchNormalization()(encoded_sequence)\n",
        "  \n",
        "    \n",
        "    \n",
        "    encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_3')(encoded_sequence))\n",
        "    \n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    encoded_sequence = Dropout(rate=0.5)(encoded_sequence)\n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    \n",
        "    \n",
        "    #crf = CRFLayer()#new\n",
        "    #out = crf(encoded_sequence)\n",
        "    \n",
        "    #out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=3, activation=\"softmax\", padding=\"same\", name='layer_4')(encoded_sequence)\n",
        "    crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    out = crf(encoded_sequence)\n",
        "\n",
        "\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2], [out])\n",
        "    \n",
        "    #model.compile(loss=crf.loss, optimizer='sgd')\n",
        "\n",
        "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.optimizer, crf.loss_function, metrics=[crf.accuracy])\n",
        "    \n",
        "    model.compile(optimizers.Adam(0.001,amsgrad=True), crf.loss_function, metrics=[crf.accuracy])\n",
        "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQgYPELqzNpe",
        "colab_type": "text"
      },
      "source": [
        "# Diff lr_rate+CRF+lstm+EEG+\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Uvpr_lfyzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_multipliers = {}\n",
        "learning_rate_multipliers['layer_11'] = .1\n",
        "learning_rate_multipliers['layer_12'] = .1\n",
        "learning_rate_multipliers['layer_13'] = .1\n",
        "learning_rate_multipliers['layer_14'] = .1\n",
        "learning_rate_multipliers['layer_15'] = .1\n",
        "learning_rate_multipliers['layer_16'] =.1\n",
        "learning_rate_multipliers['layer_17'] =.1\n",
        "learning_rate_multipliers['layer_18'] =.1\n",
        "learning_rate_multipliers['layer_19'] =.1\n",
        "learning_rate_multipliers['layer_21'] = 1\n",
        "learning_rate_multipliers['layer_22'] = 1\n",
        "learning_rate_multipliers['layer_23'] = 1\n",
        "learning_rate_multipliers['layer_24'] = 1\n",
        "learning_rate_multipliers['layer_25'] =1\n",
        "learning_rate_multipliers['layer_26'] =1\n",
        "learning_rate_multipliers['layer_27'] =1\n",
        "learning_rate_multipliers['layer_28'] =1\n",
        "learning_rate_multipliers['layer_29'] =1\n",
        "learning_rate_multipliers['layer_1'] = 5\n",
        "learning_rate_multipliers['layer_2'] = 5\n",
        "learning_rate_multipliers['layer_3'] = 5\n",
        "learning_rate_multipliers['layer_4'] = 5\n",
        "base_lr =.001\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "optimizer = LR_SGD(lr=base_lr, momentum=momentum, decay=0., nesterov=False,multipliers = learning_rate_multipliers)\n",
        "\n",
        "\n",
        "\n",
        "#adam_with_lr_multipliers = Adam_lr_mult(multipliers=learning_rate_multipliers)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sgPlSi5e5XL",
        "colab_type": "text"
      },
      "source": [
        "# train gertor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NEXrjrVeaPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "def get_base_model():\n",
        "    model1 = Sequential()\n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_11',input_shape=(3000, 1)))  \n",
        "    model1.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_12'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    model1.add(SpatialDropout1D(rate=0.3))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_13'))\n",
        "    model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_14'))\n",
        "    model1.add(MaxPool1D(pool_size=2))\n",
        "    #model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_15'))\n",
        "    #model1.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_16'))\n",
        "    #model1.add(MaxPool1D(pool_size=2))\n",
        "    #model1.add(BatchNormalization())\n",
        "    model1.add(SpatialDropout1D(rate=0.2))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_17'))\n",
        "    model1.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_18'))\n",
        "    #model1.add(GlobalMaxPool1D())\n",
        "    convout2 = GlobalMaxPool1D()\n",
        "    model1.add(convout2)\n",
        "    model1.add((Dense(64, activation=activations.relu, name='layer_19')))\n",
        "    #model1.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n",
        "    #model1.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers1, metrics=['accuracy'])\n",
        "\n",
        "    model1.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    #for layer in model1.layers:\n",
        "     #    layer.trainable = False\n",
        "    \n",
        "    #######################3\n",
        "    model2 = Sequential()\n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_21', input_shape=(3000, 1)))  \n",
        "    model2.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_22'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(SpatialDropout1D(rate=0.5))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_23'))\n",
        "    model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_24'))\n",
        "    model2.add(MaxPool1D(pool_size=2))\n",
        "    #model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_25'))\n",
        "    #model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_26'))\n",
        "    #model2.add(MaxPool1D(pool_size=2))\n",
        "    model2.add(BatchNormalization())\n",
        "    model2.add(SpatialDropout1D(rate=0.3))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_27'))\n",
        "    model2.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_28'))\n",
        "    model2.add(GlobalMaxPool1D())\n",
        "    model2.add(Dropout(rate=0.4))\n",
        "    model2.add((Dense(64, activation=activations.relu, name='layer_29')))\n",
        "    #model2.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer2, metrics=['accuracy'])\n",
        "    #model2.compile(loss='categorical_crossentropy', optimizer=adam_with_lr_multipliers2, metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    model2.compile(optimizers.Adam(lr=0.01, decay=0.01), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    #for layer in model2.layers:\n",
        "     #    layer.trainable = False\n",
        "\n",
        "    \n",
        "    #plot_model(model, to_file='base_model.png')\n",
        "    #plot_model(model, to_file='base_model.png', show_shapes=True, show_layer_names=True)\n",
        "    \n",
        "    #model.summary()\n",
        "    model3 = Sequential()\n",
        "    model3.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_31', input_shape=(3033, 1)))  \n",
        "    model3.add(Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", name='layer_32'))\n",
        "    model3.add(BatchNormalization())\n",
        "    model3.add(MaxPool1D(pool_size=2))\n",
        "    model3.add(SpatialDropout1D(rate=0.5))\n",
        "    model3.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_33'))\n",
        "    model3.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_34'))\n",
        "    model3.add(MaxPool1D(pool_size=2))\n",
        "    #model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_25'))\n",
        "    #model2.add(Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_26'))\n",
        "    #model2.add(MaxPool1D(pool_size=2))\n",
        "    \n",
        "    model3.add(SpatialDropout1D(rate=0.3))\n",
        "    model3.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_37'))\n",
        "    model3.add(Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", name='layer_38'))\n",
        "    model3.add(GlobalMaxPool1D())\n",
        "    model3.add(Dropout(rate=0.4))\n",
        "    model3.add((Dense(64, activation=activations.relu, name='layer_39')))\n",
        "    model3.compile(optimizers.Adam(0.001), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    nclass = 5\n",
        "\n",
        "    seq_input1 = Input(shape=(None, 3000, 1))\n",
        "    seq_input2 = Input(shape=(None, 3000, 1))\n",
        "    seq_input3 = Input(shape=(None, 3033, 1))\n",
        "    base_model1 = model1\n",
        "    base_model2 = model2\n",
        "    base_model3 = model3\n",
        "    # for layer in base_model.layers:\n",
        "    # layer.trainable = False\n",
        "    #This wrapper applies a layer to every temporal slice of an input.\n",
        "    #for layer in base_model2.layers:\n",
        "     #    layer.trainable = False\n",
        "        \n",
        "    #for layer in base_model3.layers:\n",
        "     #    layer.trainable = False\n",
        "    encoded_sequence1 = TimeDistributed(base_model1)(seq_input1)\n",
        "    \n",
        "    encoded_sequence1 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                              kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_1')(encoded_sequence1))\n",
        "    \n",
        "    \n",
        "    \n",
        "    #encoded_sequence1=BatchNormalization()(encoded_sequence1)\n",
        "\n",
        "    \n",
        "    \n",
        "    encoded_sequence2 = TimeDistributed(base_model2)(seq_input2)\n",
        "    \n",
        "    encoded_sequence2 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_2')(encoded_sequence2))\n",
        "    \n",
        "    \n",
        "    #encoded_sequence2=BatchNormalization()(encoded_sequence2)\n",
        "\n",
        "    encoded_sequence3 = TimeDistributed(base_model3)(seq_input3)\n",
        "    \n",
        "    encoded_sequence3 = SpatialDropout1D(rate=0.01)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_3')(encoded_sequence3))\n",
        "    \n",
        "    \n",
        "    #encoded_sequence2=BatchNormalization()(encoded_sequence2)\n",
        "    \n",
        "    \n",
        "    encoded_sequence = keras.layers.Concatenate()([encoded_sequence1, encoded_sequence2 , encoded_sequence3])\n",
        "    encoded_sequence=BatchNormalization()(encoded_sequence)\n",
        "  \n",
        "    \n",
        "    \n",
        "    encoded_sequence = Dropout(rate=0.05)(Convolution1D(128,\n",
        "                                                               kernel_size=3,\n",
        "                                                               activation=\"relu\",\n",
        "                                                               padding=\"same\", name='layer_4')(encoded_sequence))\n",
        "    \n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    encoded_sequence = Dropout(rate=0.5)(encoded_sequence)\n",
        "    encoded_sequence = Bidirectional(LSTM(100, return_sequences=True))(encoded_sequence)\n",
        "    \n",
        "    \n",
        "    #crf = CRFLayer()#new\n",
        "    #out = crf(encoded_sequence)\n",
        "    \n",
        "    #out = TimeDistributed(Dense(nclass, activation=\"softmax\"))(encoded_sequence)\n",
        "    #out = Convolution1D(nclass, kernel_size=3, activation=\"softmax\", padding=\"same\", name='layer_4')(encoded_sequence)\n",
        "    crf = CRF(nclass, sparse_target=True)\n",
        "\n",
        "    out = crf(encoded_sequence)\n",
        "\n",
        "\n",
        "    from keras.models import Model\n",
        "\n",
        "    model = Model([seq_input1,seq_input2,seq_input3], [out])\n",
        "    \n",
        "    #model.compile(loss=crf.loss, optimizer='sgd')\n",
        "\n",
        "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.optimizer, crf.loss_function, metrics=[crf.accuracy])\n",
        "    \n",
        "    model.compile(optimizers.Adam(0.01,amsgrad=True), crf.loss_function, metrics=[crf.accuracy])\n",
        "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_with_lr_multipliers, metrics=['accuracy'])\n",
        "    #model.compile(optimizers.Adam(0.001,amsgrad=True), losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    \n",
        "    plot_model(model, to_file='bas_model.png', show_shapes=True, show_layer_names=True)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnqtS6meRAzV",
        "colab_type": "text"
      },
      "source": [
        "# pretrain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8wL7qehGT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "X1=np.load('X1.npy')\n",
        "X=np.load('X.npy')\n",
        "Y1=np.load('Y1.npy')\n",
        "def tgen():\n",
        "    \n",
        "    while True:\n",
        "      for i in range(200):\n",
        "        x1=X1[:,i:i+100,:,:]\n",
        "        x=X[:,i:i+100,:,:]\n",
        "        y1=Y1[:,i:i+100,:]\n",
        "                  # any required transformation\n",
        "        yield [x1,x1,x], y1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmfyB6D8mkhs",
        "colab_type": "code",
        "outputId": "91215929-2384-4836-a110-5b60ab0ce3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1091
        }
      },
      "source": [
        "model = get_base_model()\n",
        "\n",
        "early = EarlyStopping(monitor='crf_viterbi_accuracy', mode=\"max\", patience=20, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', mode=\"max\", factor=0.5, patience=2, verbose=2)#, factor=0.2\n",
        "callbacks_list = [ early, redonplat]  # early \n",
        "model.fit_generator(tgen(),validation_data=tgen(),epochs=10, verbose=2,\n",
        "                    steps_per_epoch=1000,validation_steps=300, callbacks=callbacks_list)\n",
        "model.save('X_Wavelet_crf_lstm_model_new_model_final_pre.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_36 (InputLayer)           (None, None, 3000, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_37 (InputLayer)           (None, None, 3000, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_38 (InputLayer)           (None, None, 3033, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_34 (TimeDistri (None, None, 64)     244208      input_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_35 (TimeDistri (None, None, 64)     244336      input_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_36 (TimeDistri (None, None, 64)     244272      input_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_1 (Conv1D)                (None, None, 128)    24704       time_distributed_34[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_2 (Conv1D)                (None, None, 128)    24704       time_distributed_35[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_3 (Conv1D)                (None, None, 128)    24704       time_distributed_36[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_106 (SpatialD (None, None, 128)    0           layer_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_107 (SpatialD (None, None, 128)    0           layer_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_108 (SpatialD (None, None, 128)    0           layer_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, None, 384)    0           spatial_dropout1d_106[0][0]      \n",
            "                                                                 spatial_dropout1d_107[0][0]      \n",
            "                                                                 spatial_dropout1d_108[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, None, 384)    1536        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "layer_4 (Conv1D)                (None, None, 128)    147584      batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, None, 128)    0           layer_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_23 (Bidirectional (None, None, 200)    183200      dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, None, 200)    0           bidirectional_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_24 (Bidirectional (None, None, 200)    240800      dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "crf_12 (CRF)                    (None, None, 5)      1040        bidirectional_24[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 1,381,088\n",
            "Trainable params: 1,380,224\n",
            "Non-trainable params: 864\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            " - 721s - loss: 0.8653 - crf_viterbi_accuracy: 0.6495 - val_loss: 1.1342 - val_crf_viterbi_accuracy: 0.5543\n",
            "Epoch 2/10\n",
            " - 690s - loss: 0.8408 - crf_viterbi_accuracy: 0.6588 - val_loss: 0.9044 - val_crf_viterbi_accuracy: 0.6569\n",
            "Epoch 3/10\n",
            " - 684s - loss: 0.8383 - crf_viterbi_accuracy: 0.6528 - val_loss: 1.0632 - val_crf_viterbi_accuracy: 0.5594\n",
            "Epoch 4/10\n",
            " - 684s - loss: 0.8306 - crf_viterbi_accuracy: 0.6490 - val_loss: 0.8512 - val_crf_viterbi_accuracy: 0.6579\n",
            "Epoch 5/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyzOYz8jCGyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('X_Wavelet_crf_lstm_model_new_model_final_pre.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pQOKyEXI6D9",
        "colab_type": "code",
        "outputId": "2cb752ae-0a16-4f0e-8e41-9d35014880d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " array.npuu.npy      output.xlsx\n",
            " artical.rar\t     pure_data\n",
            " balance.npy\t     pyeeg\n",
            " bas_model.png\t     Untitled0.ipynb\n",
            " best_weights.hdf5   X1.npy\n",
            " cnn_model.h5\t     X.npy\n",
            "'Colab Notebooks'    X_Wavelet_crf_lstm_model_copy1.h5\n",
            "'deep sleep.rar'     X_Wavelet_crf_lstm_model_copy.h5\n",
            " dep\t\t     X_Wavelet_crf_lstm_model_new_model_final_pre.h5\n",
            " dep2\t\t     X_Wavelet_crf_lstm_model_new_model.h5\n",
            " Graph\t\t     X_Wavelet_crf_lstm_model_new_model_pre.h5\n",
            " logs\t\t     X_Wavelet_crf_lstm_model_new_model_pretrain.h5\n",
            " lstm1_model.h5      Y1.npy\n",
            " lstm_model.h5\t     پایانامه.rar\n",
            " model.h5\t    'خلاصه مقالات2016و2017و2018ieee.rar'\n",
            " other.rar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjBcb_UhRKZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title in data direction+gen function+DONT CHANGE SPLITTTTTT EXCEPT TO SVM { form-width: \"15%\" }\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import  Input, InputLayer, Reshape, Lambda, Concatenate,ZeroPadding1D,GRU ,SimpleRNN , Flatten\n",
        "model = get_base_model()  #simple\n",
        "#model=get_model_cnn()   #first\n",
        "sample_weight = np.zeros((1,5,1))\n",
        "sample_weight[:, 0] +=4\n",
        "sample_weight[:, 1] +=20\n",
        "sample_weight[:, 2] +=1\n",
        "sample_weight[:, 3] +=4\n",
        "sample_weight[:, 4] +=10\n",
        "# , class_weight=sample_weight\n",
        "#file_path =\"best_weights.hdf5\"\n",
        "file_path = \"'X_Wavelet_crf_lstm_model_new_model_.h5'\"\n",
        "from keras.models import load_model\n",
        "#model = load_model('X_Wavelet_crf_lstm_model_new_model_final_pre.h5')\n",
        "model.load_weights('X_Wavelet_crf_lstm_model_new_model_final_pre.h5')\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='accuracy',verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor='crf_viterbi_accuracy', mode=\"max\", patience=20, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', mode=\"max\", factor=0.5, patience=2, verbose=2)#, factor=0.2\n",
        "callbacks_list = [checkpoint, early, redonplat]  # early \n",
        "\n",
        "history=model.fit_generator(gen_j(train_dict, aug=False),validation_data=gen_j(val_dict),epochs=30, verbose=2,\n",
        "                    steps_per_epoch=2000,validation_steps=300, callbacks=callbacks_list)\n",
        "model.save('X_Wavelet_crf_lstm_model_new_model_.h5')\n",
        "model.load_weights('X_Wavelet_crf_lstm_model_new_model_.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6jUywSb9AgA",
        "colab_type": "code",
        "outputId": "f0765df6-9f93-42f8-8f92-66497d0c47d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import shutil\n",
        "shutil.copy2('X_Wavelet_crf_lstm_model_copy.h5', 'X_Wavelet_crf_lstm_model_copy1.h5') # complete target filename given\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'X_Wavelet_crf_lstm_model_copy1.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORTKuJSwcjNb",
        "colab_type": "code",
        "outputId": "da9eb9e7-afe7-4fae-ec79-45ee739f8aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "##https://github.com/philipperemy/keract\n",
        "!pip install keract"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keract\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/48/abf19d8831357d87266aba859e9f82fc253c77a710b9bc58fc9685949752/keract-2.5.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keract) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keract) (1.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keract) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keract) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keract) (1.2.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keract) (1.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keract) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keract) (1.0.9)\n",
            "Installing collected packages: keract\n",
            "Successfully installed keract-2.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO9m5aKQcwyG",
        "colab_type": "code",
        "outputId": "6635e3d6-dbf8-42ed-cc03-54b2e2651995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 3000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTwP6_DWug6K",
        "colab_type": "code",
        "outputId": "7490833d-dab2-4448-f26e-6292b90f188b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "model = get_base_model()\n",
        "from keract import get_activations\n",
        "z=get_activations(model, [a,a])\n",
        "for key, value in z.items():\n",
        "  #print(key, value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           (None, None, 3000, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           (None, None, 3000, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, None, 64)     250416      input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_10 (TimeDistri (None, None, 64)     250416      input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_1 (Conv1D)                (None, None, 128)    24704       time_distributed_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_2 (Conv1D)                (None, None, 128)    24704       time_distributed_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_31 (SpatialDr (None, None, 128)    0           layer_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_32 (SpatialDr (None, None, 128)    0           layer_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, 128)    512         spatial_dropout1d_31[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, 128)    512         spatial_dropout1d_32[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, None, 256)    0           batch_normalization_10[0][0]     \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, 256)    1024        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_3 (Conv1D)                (None, None, 128)    98432       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, None, 128)    0           layer_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_4 (Conv1D)                (None, None, 5)      645         dropout_14[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 651,365\n",
            "Trainable params: 650,341\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeCjSHv7Rb7d",
        "colab_type": "text"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbCtYiIdhbUo",
        "colab_type": "code",
        "outputId": "a129d7d5-ac5d-4972-91d6-1debbf88f7cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "model = get_base_model()\n",
        "file_path = 'X_Wavelet_crf_lstm_model_copy1.h5'\n",
        "from keras.models import load_model\n",
        "#model = load_model('lstm1_model.h5')\n",
        "model.load_weights(file_path)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, None, 3000, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, None, 3033, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 64)     244208      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, None, 64)     244208      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_1 (Conv1D)                (None, None, 128)    24704       time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_2 (Conv1D)                (None, None, 128)    24704       time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_5 (SpatialDro (None, None, 128)    0           layer_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_6 (SpatialDro (None, None, 128)    0           layer_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, 256)    0           spatial_dropout1d_5[0][0]        \n",
            "                                                                 spatial_dropout1d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_3 (Conv1D)                (None, None, 128)    98432       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, None, 128)    0           layer_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, None, 200)    183200      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, None, 200)    0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, None, 200)    240800      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "crf_1 (CRF)                     (None, None, 5)      1040        bidirectional_2[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 1,061,296\n",
            "Trainable params: 1,061,296\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAuGFeARPh7",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujs3xc9V_c7G",
        "colab_type": "code",
        "outputId": "e437c47e-03b7-4908-feab-857a7782e4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "preds = []\n",
        "gt = []\n",
        "for record in tqdm(test_dict):\n",
        "    all_rows = test_dict[record]['x']\n",
        "    s=record +'.npy'\n",
        "    X2=np.load(s)\n",
        "    X22=X2.reshape((len(X2)),92,1)\n",
        "    for batch_hyp in chunker(range(all_rows.shape[0])):\n",
        "\n",
        "        X1 = all_rows[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "        Y1 = test_dict[record]['y'][min(batch_hyp):max(batch_hyp)+1]\n",
        "        ##########3\n",
        "        \n",
        "        \n",
        "        X2=X22[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "        \n",
        "        X2 = np.expand_dims(X2,0)\n",
        "\n",
        "        X1 = np.expand_dims(X1, 0)\n",
        "\n",
        "        X1 = rescale_array(X1)\n",
        "        \n",
        "        a=wtdata(X1).transpose()\n",
        "        X = np.expand_dims(a, 0)\n",
        "        X= np.expand_dims(X, -1)\n",
        "    \n",
        "        X2 = rescale_array(X)\n",
        "\n",
        "        Y_pred = model.predict([X1,X1,X2])\n",
        "        Y_pred = Y_pred.argmax(axis=-1).ravel().tolist()\n",
        "\n",
        "        \n",
        "        \n",
        "        gt += Y1.ravel().tolist()\n",
        "        preds += Y_pred\n",
        "\n",
        "\n",
        "\n",
        "f1 = f1_score(gt, preds, average=\"macro\")\n",
        "\n",
        "print(\"Seq Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(gt, preds)\n",
        "\n",
        "print(\"Seq Test accuracy score : %s \"% acc)\n",
        "\n",
        "print(classification_report(gt, preds))\n",
        "\n",
        "\n",
        "class_names=np.array([[\"W\"], [\"S1\"],[\"S2\"],[\"S3\"],[\"R\"]])\n",
        "# Compute confusion matrix\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    \n",
        "\n",
        "cnf_matrix = confusion_matrix(gt, preds)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " array.npuu.npy    'deep sleep.rar'   pyeeg\n",
            " artical.rar\t    dep\t\t      Untitled0.ipynb\n",
            " bas_model.png\t    dep2\t     'خلاصه مقالات2016و2017و2018ieee.rar'\n",
            " cnn_model.h5\t    other.rar\n",
            "'Colab Notebooks'   pure_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9DhHWiRt5_",
        "colab_type": "text"
      },
      "source": [
        "# scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfcl1kGPNGHj",
        "colab_type": "code",
        "outputId": "beb1bb74-1672-4678-e40f-5b43352ba2f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1265
        }
      },
      "source": [
        "xxx=[]\n",
        "preds = []\n",
        "gt = []\n",
        "u=[]\n",
        "total_max=[]\n",
        "j=-1\n",
        "for record in tqdm(test_dict):\n",
        "    all_rows = test_dict[record]['x']\n",
        "    s=record +'.npy'\n",
        "    X2=np.load(s)\n",
        "    X22=X2.reshape((len(X2)),92,1)\n",
        "    #j=-1\n",
        "    \n",
        "    \n",
        "    \n",
        "    for batch_hyp in chunker(range(all_rows.shape[0])):\n",
        "\n",
        "\n",
        "        X = all_rows[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "        d=X\n",
        "        Y = test_dict[record]['y'][min(batch_hyp):max(batch_hyp)+1]\n",
        "        \n",
        "        X2=X22[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "        \n",
        "        X2 = np.expand_dims(X2,0)\n",
        "\n",
        "        X = np.expand_dims(X, 0)\n",
        "\n",
        "        X = rescale_array(X)\n",
        "\n",
        "        Y_pred = model.predict([X,X2])\n",
        "        j=j+1\n",
        "        t=Y_pred\n",
        "        if j>=1:\n",
        "          a = t.reshape(t.shape[0]*t.shape[1],5)\n",
        "          u = np.concatenate((u,a),axis=0)\n",
        "          \n",
        "          d= d.reshape(d.shape[0],3000)\n",
        "          xxx = np.concatenate((xxx,d),axis=0)\n",
        "          \n",
        "        else :\n",
        "          a = t.reshape(t.shape[0]*t.shape[1],5)\n",
        "          u = a\n",
        "          \n",
        "          d= d.reshape(d.shape[0],3000)\n",
        "          xxx = d\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        Y_pred = Y_pred.argmax(axis=-1).ravel().tolist()\n",
        "    \n",
        "        gt += Y.ravel().tolist()\n",
        "        preds += Y_pred\n",
        "         \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "f1 = f1_score(gt, preds, average=\"macro\")\n",
        "\n",
        "print(\"Seq Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(gt, preds)\n",
        "\n",
        "print(\"Seq Test accuracy score : %s \"% acc)\n",
        "\n",
        "print(classification_report(gt, preds))\n",
        "\n",
        "\n",
        "class_names=np.array([[\"W\"], [\"S1\"],[\"S2\"],[\"S3\"],[\"R\"]])\n",
        "# Compute confusion matrix\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    \n",
        "\n",
        "cnf_matrix = confusion_matrix(gt, preds)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:39<00:00,  2.06s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test f1 score : 0.6906357679074638 \n",
            "Seq Test accuracy score : 0.759576567686373 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.82      0.75      3300\n",
            "           1       0.48      0.28      0.36      1687\n",
            "           2       0.78      0.86      0.82      8928\n",
            "           3       0.85      0.68      0.76      4104\n",
            "           4       0.78      0.77      0.77      4275\n",
            "\n",
            "   micro avg       0.76      0.76      0.76     22294\n",
            "   macro avg       0.72      0.68      0.69     22294\n",
            "weighted avg       0.76      0.76      0.75     22294\n",
            "\n",
            "Confusion matrix, without normalization\n",
            "[[2718  238   90   45  209]\n",
            " [ 438  479  424   16  330]\n",
            " [ 393  169 7671  384  311]\n",
            " [  34   15 1178 2787   90]\n",
            " [ 338   98  526   34 3279]]\n",
            "Normalized confusion matrix\n",
            "[[0.82 0.07 0.03 0.01 0.06]\n",
            " [0.26 0.28 0.25 0.01 0.2 ]\n",
            " [0.04 0.02 0.86 0.04 0.03]\n",
            " [0.01 0.   0.29 0.68 0.02]\n",
            " [0.08 0.02 0.12 0.01 0.77]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAGACAYAAAAwIRxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGACAYAAADMNDeHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDxor9hcRjmvSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u83Ej86Z_c9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = np.zeros(len(u))##true or false\n",
        "p = np.zeros(len(u))## Ratio\n",
        "g=np.zeros(len(u)) ###Class\n",
        "sec=np.zeros(len(u)) ###Secend prediction\n",
        "for i in range(len(u)):\n",
        "        z=max(u[i])\n",
        "        ee = np.delete(u[i], np.argmax(u[i]))\n",
        "      \n",
        "      \n",
        "      \n",
        "        if np.argmax(u[i])>np.argmax(ee):\n",
        "          sec[i]=np.argmax(ee)\n",
        "        elif np.argmax(u[i])<np.argmax(ee):\n",
        "          sec[i]=np.argmax(ee)+1\n",
        "        elif np.argmax(u[i])==np.argmax(ee):\n",
        "          sec[i]=np.argmax(ee)+1\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "       \n",
        "        o=max(ee)\n",
        "        p[i]=z/o\n",
        "        g[i]=gt[i]\n",
        "        if gt[i]==np.argmax(u[i]):\n",
        "          q[i]=1\n",
        "        else :q[i]=0\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35AbShNg1T8k",
        "colab_type": "code",
        "outputId": "3d46992e-8f49-467f-e19e-228a0edf100a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "u.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29943, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrmGi-HC_dAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "F = np.concatenate([[q],[p],[g],[preds],[sec]],axis=0)          \n",
        "F=np.transpose(F)\n",
        "F = pd.DataFrame(F,columns=['true|false','Ratio','Class','Prediction','Secend prediction'])        \n",
        "TotalDataset= pd.concat([F],ignore_index=False)\n",
        "F.to_excel(\"output.xlsx\") \n",
        "from google.colab import files\n",
        "files.download(\"output.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRemfLf-7Jzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Pretrain model { form-width: \"20%\" }\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D,concatenate, SpatialDropout1D, TimeDistributed, Bidirectional, LSTM\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers, losses, activations, models\n",
        "#y_ptrain = keras.utils.to_categorical(y_ptrain, num_classes=5)\n",
        "#y_ptest = keras.utils.to_categorical(y_ptest, num_classes=5)\n",
        "\n",
        "\n",
        "mode = Sequential()\n",
        "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
        "# in the first layer, you must specify the expected input data shape:\n",
        "# here, 20-dimensional vectors.\n",
        "mode.add(Dense(50, activation='relu', input_dim=92))\n",
        "mode.add(Dropout(0.5))\n",
        "mode.add(Dense(20, activation='relu'))\n",
        "mode.add(Dense(20, activation='relu'))\n",
        "mode.add(Dense(10, activation='relu'))\n",
        "#mode.add(Dense(30, activation='relu'))\n",
        "mode.add(Dropout(0.2))\n",
        "mode.add(Dense(5, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "mode.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtXNUSLU5mT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre1=[]\n",
        "y1_pre=[]\n",
        "pre3=[]\n",
        "y3_pre=[]\n",
        "pre4=[]\n",
        "y4_pre=[]\n",
        "u=0\n",
        "v=0\n",
        "w=0\n",
        "for record in tqdm(train_dict):\n",
        "    s=record +'.npy'\n",
        "    Fe=np.load(s)\n",
        "    all_cols=train_dict[record]['y']\n",
        "    for i in range(len(all_cols)):\n",
        "      if all_cols[i]==1 & all_cols[i-1]==1 & all_cols[i-2]==1:\n",
        "        if u==0:\n",
        "          pre1=all_rows[i-1]\n",
        "          u=u+1\n",
        "          y1_pre+=[1]\n",
        "          \n",
        "        else: \n",
        "             pre1 = np.concatenate((pre1,all_rows[i]),axis=0)\n",
        "             y1_pre+=[1]\n",
        "              \n",
        "      if all_cols[i]==3 & all_cols[i-1]==3 & all_cols[i-2]==3 :\n",
        "        if v==0:\n",
        "          pre3=all_rows[i]\n",
        "          v=v+1\n",
        "          y3_pre+=[3]\n",
        "          \n",
        "        else: \n",
        "             pre3 = np.concatenate((pre3,all_rows[i]),axis=0)\n",
        "             y3_pre+=[3]\n",
        "              \n",
        "              \n",
        "              \n",
        "      if all_cols[i]==4 & all_cols[i-1]==4 & all_cols[i-2]==4:\n",
        "        if w==0:\n",
        "          pre4=all_rows[i]\n",
        "          w=w+1\n",
        "          y4_pre+=[4]\n",
        "          \n",
        "        else: \n",
        "             pre4 = np.concatenate((pre4,all_rows[i]),axis=0)\n",
        "             y4_pre+=[4]        \n",
        "          \n",
        "pre1= pre1.reshape(int(len(pre1)/3000),3000)\n",
        "pre3= pre3.reshape(int(len(pre3)/3000),3000)\n",
        "pre4= pre4.reshape(int(len(pre4)/3000),3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o55h2K6e5nOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.random.randint(len(pre1), size=150)\n",
        "a=pre1[idx,:]\n",
        "asim=a\n",
        "aa=numpy.ones((150,1))*1\n",
        "\n",
        "idx = np.random.randint(len(pre3), size=120)\n",
        "b=pre3[idx,:]\n",
        "bb=numpy.ones((120,1))*3\n",
        "\n",
        "idx = np.random.randint(len(pre4), size=120)\n",
        "c=pre4[idx,:]\n",
        "cc=numpy.ones((120,1))*4\n",
        "pre_x = np.concatenate((a,b,c),axis=0)\n",
        "pre_y=np.concatenate((aa,bb,cc),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdbMHjJ82ooF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for record in tqdm(train_dict):\n",
        "    y= train_dict[record]['y']\n",
        "    s=record +'.npy'\n",
        "    Fe=np.load(s)\n",
        "    for i in range(len(y)):\n",
        "      if y[i]==1:\n",
        "        \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP--Jr5Z73l0",
        "colab_type": "code",
        "outputId": "7ffb4df0-431a-4a80-d3ff-3ffd048891bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207774
        }
      },
      "source": [
        "#@title Smi super { form-width: \"20%\" }\n",
        "#model.load_weights(file_path)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OutputCodeClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "xxx=[]\n",
        "preds = []\n",
        "gt = []\n",
        "u=[]\n",
        "total_max=[]\n",
        "Y_true=[]\n",
        "j=-1\n",
        "\n",
        "\n",
        "\n",
        "for record in tqdm(test_dict):\n",
        "    all_rows = test_dict[record]['x']\n",
        "    j=-1\n",
        "    s=record +'.npy'\n",
        "    X2=np.load(s)\n",
        "    Fe=np.load(s)\n",
        "    X22=X2.reshape((len(X2)),92,1)\n",
        "    \n",
        "    \n",
        "    for batch_hyp in chunker(range(all_rows.shape[0])):\n",
        "        X2=X22[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "        \n",
        "        X2 = np.expand_dims(X2,0)\n",
        "\n",
        "        X = all_rows[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "        d=X\n",
        "        Y = test_dict[record]['y'][min(batch_hyp):max(batch_hyp)+1]\n",
        "\n",
        "        X = np.expand_dims(X, 0)\n",
        "\n",
        "        X = rescale_array(X)\n",
        "\n",
        "        Y_pred = model.predict([X,X2])\n",
        "        j=j+1\n",
        "        t=Y_pred\n",
        "        if j>=1:\n",
        "                \n",
        "          \n",
        "          d= d.reshape(d.shape[0],3000)\n",
        "          xxx = np.concatenate((xxx,d),axis=0)\n",
        "          \n",
        "          Y_true = np.concatenate((Y_true,Y),axis=0)\n",
        "          \n",
        "          \n",
        "          a = t.reshape(t.shape[0]*t.shape[1],5)\n",
        "          u = np.concatenate((u,a),axis=0)\n",
        "    \n",
        "          \n",
        "        else :\n",
        "          Y_true=Y\n",
        "          \n",
        "          d= d.reshape(d.shape[0],3000)\n",
        "          xxx = d\n",
        "          \n",
        "          a = t.reshape(t.shape[0]*t.shape[1],5)\n",
        "          u = a\n",
        "\n",
        "        \n",
        "        Y_pred = Y_pred.argmax(axis=-1).ravel().tolist()\n",
        "    \n",
        "        gt += Y.ravel().tolist()\n",
        "        preds += Y_pred\n",
        "    #######soft_max analyis     \n",
        "    sec=np.zeros(len(u))\n",
        "    p=np.zeros((len(u),5))\n",
        "    q=np.zeros(len(u))\n",
        "    for i in range(len(u)):\n",
        "        z=max(u[i])\n",
        "        ee = np.delete(u[i], np.argmax(u[i]))\n",
        "        o=max(ee)\n",
        "       \n",
        "        if np.argmax(u[i])>np.argmax(ee):\n",
        "            sec[i]=np.argmax(ee)\n",
        "        elif np.argmax(u[i])<np.argmax(ee):\n",
        "            sec[i]=np.argmax(u[i])+1\n",
        "        elif np.argmax(u[i])==np.argmax(ee) :\n",
        "            sec[i]=np.argmax(u[i])+1\n",
        "        p[i]=[z/o,np.argmax(u[i]),sec[i],i,Y_true[i]]\n",
        "    \n",
        "    p=p[p[:, 0].argsort()]\n",
        "    \n",
        "    \n",
        "    zx=(len(p)-(len(p)%2))/2\n",
        "    zx=int(zx)\n",
        "   \n",
        "    x1=np.zeros((zx,92)) \n",
        "    y1=np.zeros(zx)\n",
        "    y_true1=np.zeros(zx)\n",
        "    for i in range(zx):\n",
        "        x1[i]=Fe[int(p[i,3])]\n",
        "        y1[i]=p[i,1]\n",
        "        y_true1[i]=p[i,4]\n",
        "        \n",
        "    x2=np.zeros((zx,92)) \n",
        "    y2=np.zeros(zx)\n",
        "    y_true2=np.zeros(zx)\n",
        "    for i in range(zx):\n",
        "        x2[i]=Fe[int(p[len(p)-1-i,3])]\n",
        "        y2[i]=p[len(p)-1-i,1]\n",
        "        y_true2[i]=p[len(p)-1-i,4]\n",
        "        \n",
        "        \n",
        "    x3=np.zeros((len(xxx)-2*zx,3000)) \n",
        "    y3=np.ones(len(xxx)-2*zx)\n",
        "    y3=-y3\n",
        "    y_true3=np.zeros(len(xxx)-2*zx)\n",
        "    eee=p[:,4]\n",
        "   \n",
        "   # for i in range(zx):\n",
        "    #    eee= np.delete(eee,0)\n",
        "     #   eee=np.delete(eee,len(eee)-1)\n",
        "    #y_true3=eee\n",
        "    #y_true2=np.concatenate((y_true3,y_true3),axis=0)\n",
        "    gu2=numpy.ones((len(y2),1))\n",
        "    for i in range(len(y2)):\n",
        "      gu2[i]=[y2[i]]\n",
        "      \n",
        "    gu1=numpy.ones((len(y1),1))\n",
        "    for i in range(len(y1)):\n",
        "      gu1[i]=[y1[i]]\n",
        "    \n",
        "    ####### \n",
        "    F_train=x2\n",
        "    F_test=x1\n",
        "    #F_semi=com_fea(x1)\n",
        "    \n",
        "    #F_train_new = np.concatenate((F_train,F_pre),axis=0)\n",
        "    #y_train_new = np.concatenate((gu2,pre_y),axis=0)\n",
        "    \n",
        "    ############### \n",
        "    y_train_new = keras.utils.to_categorical(gu2, num_classes=5)\n",
        "    y1 = keras.utils.to_categorical(gu1, num_classes=5)\n",
        "    mode.fit(x2, y_train_new,\n",
        "          epochs=250,\n",
        "          batch_size=128,class_weight=\"balanced\")\n",
        "    score = mode.evaluate(F_test, y1, batch_size=128)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    Y_p = mode.predict(F_test)\n",
        "    Y_p=Y_p.argmax(axis=-1)\n",
        "    #y_ptest=y_ptest.argmax(axis=-1)\n",
        "    f1 = f1_score(y_true1, Y_p, average=\"macro\")\n",
        "\n",
        "    #print(\"Seq Test f1 score : %s \"% f1)\n",
        "\n",
        "    acc = accuracy_score(y_true1, Y_p)\n",
        "\n",
        "    print(\"Seq Test accuracy score : %s \"% acc)\n",
        "\n",
        "    print(classification_report(y_true1, Y_p))\n",
        "    \n",
        "    \n",
        "    \n",
        "    k=confusion_matrix(y_true1, Y_p)\n",
        "    print(k)\n",
        "    \n",
        "    k=confusion_matrix(y_true2, y2)\n",
        "    print(k)\n",
        " \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "f1 = f1_score(gt, preds, average=\"macro\")\n",
        "\n",
        "print(\"Seq Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(gt, preds)\n",
        "\n",
        "print(\"Seq Test accuracy score : %s \"% acc)\n",
        "\n",
        "print(classification_report(gt, preds))\n",
        "\n",
        "\n",
        "class_names=np.array([[\"W\"], [\"S1\"],[\"S2\"],[\"S3\"],[\"R\"]])\n",
        "# Compute confusion matrix\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    \n",
        "\n",
        "cnf_matrix = confusion_matrix(gt, preds)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/22 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "420/420 [==============================] - 0s 48us/step - loss: 0.5039 - acc: 0.8929\n",
            "Epoch 2/250\n",
            "420/420 [==============================] - 0s 49us/step - loss: 0.5364 - acc: 0.8833\n",
            "Epoch 3/250\n",
            "420/420 [==============================] - 0s 50us/step - loss: 0.4418 - acc: 0.9071\n",
            "Epoch 4/250\n",
            "420/420 [==============================] - 0s 43us/step - loss: 0.3650 - acc: 0.9071\n",
            "Epoch 5/250\n",
            "420/420 [==============================] - 0s 47us/step - loss: 0.2555 - acc: 0.9143\n",
            "Epoch 6/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.2413 - acc: 0.9524\n",
            "Epoch 7/250\n",
            "420/420 [==============================] - 0s 48us/step - loss: 0.2232 - acc: 0.9452\n",
            "Epoch 8/250\n",
            "420/420 [==============================] - 0s 44us/step - loss: 0.2169 - acc: 0.9381\n",
            "Epoch 9/250\n",
            "420/420 [==============================] - 0s 41us/step - loss: 0.2279 - acc: 0.9333\n",
            "Epoch 10/250\n",
            "420/420 [==============================] - 0s 45us/step - loss: 0.1967 - acc: 0.9333\n",
            "Epoch 11/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.2605 - acc: 0.9262\n",
            "Epoch 12/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1932 - acc: 0.9381\n",
            "Epoch 13/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.1937 - acc: 0.9476\n",
            "Epoch 14/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1790 - acc: 0.9548\n",
            "Epoch 15/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.2443 - acc: 0.9286\n",
            "Epoch 16/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1820 - acc: 0.9595\n",
            "Epoch 17/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.1939 - acc: 0.9571\n",
            "Epoch 18/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.2288 - acc: 0.9333\n",
            "Epoch 19/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.2228 - acc: 0.9381\n",
            "Epoch 20/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.1466 - acc: 0.9595\n",
            "Epoch 21/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1800 - acc: 0.9643\n",
            "Epoch 22/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.1584 - acc: 0.9548\n",
            "Epoch 23/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.2162 - acc: 0.9452\n",
            "Epoch 24/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.1498 - acc: 0.9571\n",
            "Epoch 25/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.2268 - acc: 0.9381\n",
            "Epoch 26/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.1756 - acc: 0.9595\n",
            "Epoch 27/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1621 - acc: 0.9595\n",
            "Epoch 28/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1223 - acc: 0.9762\n",
            "Epoch 29/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1838 - acc: 0.9548\n",
            "Epoch 30/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1901 - acc: 0.9571\n",
            "Epoch 31/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1824 - acc: 0.9571\n",
            "Epoch 32/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1612 - acc: 0.9571\n",
            "Epoch 33/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1727 - acc: 0.9571\n",
            "Epoch 34/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1526 - acc: 0.9619\n",
            "Epoch 35/250\n",
            "420/420 [==============================] - 0s 65us/step - loss: 0.1273 - acc: 0.9714\n",
            "Epoch 36/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.1808 - acc: 0.9548\n",
            "Epoch 37/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1745 - acc: 0.9619\n",
            "Epoch 38/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1862 - acc: 0.9476\n",
            "Epoch 39/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.0893 - acc: 0.9810\n",
            "Epoch 40/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.1651 - acc: 0.9524\n",
            "Epoch 41/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1476 - acc: 0.9714\n",
            "Epoch 42/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.1312 - acc: 0.9619\n",
            "Epoch 43/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.1585 - acc: 0.9667\n",
            "Epoch 44/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1767 - acc: 0.9667\n",
            "Epoch 45/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1412 - acc: 0.9667\n",
            "Epoch 46/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1332 - acc: 0.9643\n",
            "Epoch 47/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.1571 - acc: 0.9690\n",
            "Epoch 48/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1128 - acc: 0.9714\n",
            "Epoch 49/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.1104 - acc: 0.9619\n",
            "Epoch 50/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1439 - acc: 0.9643\n",
            "Epoch 51/250\n",
            "420/420 [==============================] - 0s 79us/step - loss: 0.1271 - acc: 0.9762\n",
            "Epoch 52/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1551 - acc: 0.9571\n",
            "Epoch 53/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1042 - acc: 0.9714\n",
            "Epoch 54/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1468 - acc: 0.9571\n",
            "Epoch 55/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1652 - acc: 0.9548\n",
            "Epoch 56/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1402 - acc: 0.9714\n",
            "Epoch 57/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1534 - acc: 0.9690\n",
            "Epoch 58/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.1313 - acc: 0.9714\n",
            "Epoch 59/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1787 - acc: 0.9571\n",
            "Epoch 60/250\n",
            "420/420 [==============================] - 0s 64us/step - loss: 0.1350 - acc: 0.9690\n",
            "Epoch 61/250\n",
            "420/420 [==============================] - 0s 68us/step - loss: 0.1223 - acc: 0.9810\n",
            "Epoch 62/250\n",
            "420/420 [==============================] - 0s 48us/step - loss: 0.1587 - acc: 0.9619\n",
            "Epoch 63/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1526 - acc: 0.9738\n",
            "Epoch 64/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1210 - acc: 0.9738\n",
            "Epoch 65/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.1585 - acc: 0.9595\n",
            "Epoch 66/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1571 - acc: 0.9667\n",
            "Epoch 67/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1285 - acc: 0.9690\n",
            "Epoch 68/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1343 - acc: 0.9786\n",
            "Epoch 69/250\n",
            "420/420 [==============================] - 0s 67us/step - loss: 0.1550 - acc: 0.9667\n",
            "Epoch 70/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1359 - acc: 0.9762\n",
            "Epoch 71/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1550 - acc: 0.9524\n",
            "Epoch 72/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1671 - acc: 0.9690\n",
            "Epoch 73/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.1728 - acc: 0.9595\n",
            "Epoch 74/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1195 - acc: 0.9571\n",
            "Epoch 75/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.1214 - acc: 0.9786\n",
            "Epoch 76/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1395 - acc: 0.9667\n",
            "Epoch 77/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.1824 - acc: 0.9524\n",
            "Epoch 78/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1224 - acc: 0.9786\n",
            "Epoch 79/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0782 - acc: 0.9810\n",
            "Epoch 80/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1083 - acc: 0.9690\n",
            "Epoch 81/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1341 - acc: 0.9738\n",
            "Epoch 82/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.1512 - acc: 0.9714\n",
            "Epoch 83/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.1338 - acc: 0.9738\n",
            "Epoch 84/250\n",
            "420/420 [==============================] - 0s 64us/step - loss: 0.1271 - acc: 0.9762\n",
            "Epoch 85/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1179 - acc: 0.9738\n",
            "Epoch 86/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1365 - acc: 0.9690\n",
            "Epoch 87/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1309 - acc: 0.9786\n",
            "Epoch 88/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.1259 - acc: 0.9667\n",
            "Epoch 89/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1414 - acc: 0.9714\n",
            "Epoch 90/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1162 - acc: 0.9738\n",
            "Epoch 91/250\n",
            "420/420 [==============================] - 0s 72us/step - loss: 0.1250 - acc: 0.9738\n",
            "Epoch 92/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1444 - acc: 0.9667\n",
            "Epoch 93/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1348 - acc: 0.9667\n",
            "Epoch 94/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1477 - acc: 0.9619\n",
            "Epoch 95/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.1096 - acc: 0.9833\n",
            "Epoch 96/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1068 - acc: 0.9738\n",
            "Epoch 97/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1290 - acc: 0.9738\n",
            "Epoch 98/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.0939 - acc: 0.9762\n",
            "Epoch 99/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1220 - acc: 0.9738\n",
            "Epoch 100/250\n",
            "420/420 [==============================] - 0s 66us/step - loss: 0.1159 - acc: 0.9714\n",
            "Epoch 101/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0964 - acc: 0.9833\n",
            "Epoch 102/250\n",
            "420/420 [==============================] - 0s 64us/step - loss: 0.1013 - acc: 0.9714\n",
            "Epoch 103/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1360 - acc: 0.9667\n",
            "Epoch 104/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0894 - acc: 0.9738\n",
            "Epoch 105/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0806 - acc: 0.9833\n",
            "Epoch 106/250\n",
            "420/420 [==============================] - 0s 50us/step - loss: 0.1144 - acc: 0.9810\n",
            "Epoch 107/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1107 - acc: 0.9786\n",
            "Epoch 108/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1243 - acc: 0.9690\n",
            "Epoch 109/250\n",
            "420/420 [==============================] - 0s 65us/step - loss: 0.1197 - acc: 0.9738\n",
            "Epoch 110/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0677 - acc: 0.9833\n",
            "Epoch 111/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1168 - acc: 0.9738\n",
            "Epoch 112/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.0882 - acc: 0.9881\n",
            "Epoch 113/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1319 - acc: 0.9714\n",
            "Epoch 114/250\n",
            "420/420 [==============================] - 0s 50us/step - loss: 0.1133 - acc: 0.9667\n",
            "Epoch 115/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1129 - acc: 0.9667\n",
            "Epoch 116/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.0814 - acc: 0.9881\n",
            "Epoch 117/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.1372 - acc: 0.9690\n",
            "Epoch 118/250\n",
            "420/420 [==============================] - 0s 72us/step - loss: 0.0628 - acc: 0.9857\n",
            "Epoch 119/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.1016 - acc: 0.9762\n",
            "Epoch 120/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.0758 - acc: 0.9833\n",
            "Epoch 121/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1384 - acc: 0.9714\n",
            "Epoch 122/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1303 - acc: 0.9667\n",
            "Epoch 123/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1031 - acc: 0.9786\n",
            "Epoch 124/250\n",
            "420/420 [==============================] - 0s 50us/step - loss: 0.0920 - acc: 0.9714\n",
            "Epoch 125/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0992 - acc: 0.9762\n",
            "Epoch 126/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0905 - acc: 0.9833\n",
            "Epoch 127/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0901 - acc: 0.9738\n",
            "Epoch 128/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1323 - acc: 0.9762\n",
            "Epoch 129/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.0937 - acc: 0.9833\n",
            "Epoch 130/250\n",
            "420/420 [==============================] - 0s 66us/step - loss: 0.0915 - acc: 0.9738\n",
            "Epoch 131/250\n",
            "420/420 [==============================] - 0s 83us/step - loss: 0.1062 - acc: 0.9786\n",
            "Epoch 132/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0937 - acc: 0.9738\n",
            "Epoch 133/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.1333 - acc: 0.9690\n",
            "Epoch 134/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1418 - acc: 0.9810\n",
            "Epoch 135/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0934 - acc: 0.9690\n",
            "Epoch 136/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1045 - acc: 0.9810\n",
            "Epoch 137/250\n",
            "420/420 [==============================] - 0s 48us/step - loss: 0.0883 - acc: 0.9810\n",
            "Epoch 138/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0615 - acc: 0.9833\n",
            "Epoch 139/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1172 - acc: 0.9643\n",
            "Epoch 140/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.1352 - acc: 0.9714\n",
            "Epoch 141/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.0793 - acc: 0.9905\n",
            "Epoch 142/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0991 - acc: 0.9786\n",
            "Epoch 143/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1366 - acc: 0.9738\n",
            "Epoch 144/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.1029 - acc: 0.9738\n",
            "Epoch 145/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.0903 - acc: 0.9786\n",
            "Epoch 146/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.1025 - acc: 0.9810\n",
            "Epoch 147/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.1056 - acc: 0.9738\n",
            "Epoch 148/250\n",
            "420/420 [==============================] - 0s 50us/step - loss: 0.0851 - acc: 0.9738\n",
            "Epoch 149/250\n",
            "420/420 [==============================] - 0s 66us/step - loss: 0.1036 - acc: 0.9810\n",
            "Epoch 150/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0776 - acc: 0.9833\n",
            "Epoch 151/250\n",
            "420/420 [==============================] - 0s 64us/step - loss: 0.1001 - acc: 0.9810\n",
            "Epoch 152/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.1020 - acc: 0.9786\n",
            "Epoch 153/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0870 - acc: 0.9833\n",
            "Epoch 154/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0878 - acc: 0.9810\n",
            "Epoch 155/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1205 - acc: 0.9738\n",
            "Epoch 156/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0901 - acc: 0.9857\n",
            "Epoch 157/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0999 - acc: 0.9738\n",
            "Epoch 158/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0949 - acc: 0.9786\n",
            "Epoch 159/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1111 - acc: 0.9714\n",
            "Epoch 160/250\n",
            "420/420 [==============================] - 0s 75us/step - loss: 0.0956 - acc: 0.9762\n",
            "Epoch 161/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0629 - acc: 0.9905\n",
            "Epoch 162/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1269 - acc: 0.9667\n",
            "Epoch 163/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0836 - acc: 0.9810\n",
            "Epoch 164/250\n",
            "420/420 [==============================] - 0s 51us/step - loss: 0.1106 - acc: 0.9738\n",
            "Epoch 165/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.1338 - acc: 0.9714\n",
            "Epoch 166/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0840 - acc: 0.9786\n",
            "Epoch 167/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.0822 - acc: 0.9833\n",
            "Epoch 168/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0892 - acc: 0.9786\n",
            "Epoch 169/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0555 - acc: 0.9857\n",
            "Epoch 170/250\n",
            "420/420 [==============================] - 0s 73us/step - loss: 0.0939 - acc: 0.9762\n",
            "Epoch 171/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1159 - acc: 0.9690\n",
            "Epoch 172/250\n",
            "420/420 [==============================] - 0s 49us/step - loss: 0.1249 - acc: 0.9667\n",
            "Epoch 173/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0757 - acc: 0.9833\n",
            "Epoch 174/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.0763 - acc: 0.9810\n",
            "Epoch 175/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0790 - acc: 0.9833\n",
            "Epoch 176/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0974 - acc: 0.9714\n",
            "Epoch 177/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.0616 - acc: 0.9857\n",
            "Epoch 178/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0892 - acc: 0.9714\n",
            "Epoch 179/250\n",
            "420/420 [==============================] - 0s 66us/step - loss: 0.0918 - acc: 0.9762\n",
            "Epoch 180/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0725 - acc: 0.9833\n",
            "Epoch 181/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.0971 - acc: 0.9762\n",
            "Epoch 182/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0672 - acc: 0.9857\n",
            "Epoch 183/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0752 - acc: 0.9762\n",
            "Epoch 184/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0790 - acc: 0.9786\n",
            "Epoch 185/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.1048 - acc: 0.9738\n",
            "Epoch 186/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0801 - acc: 0.9857\n",
            "Epoch 187/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0768 - acc: 0.9810\n",
            "Epoch 188/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0786 - acc: 0.9786\n",
            "Epoch 189/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0597 - acc: 0.9857\n",
            "Epoch 190/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.1066 - acc: 0.9667\n",
            "Epoch 191/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0740 - acc: 0.9810\n",
            "Epoch 192/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.1045 - acc: 0.9738\n",
            "Epoch 193/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0518 - acc: 0.9905\n",
            "Epoch 194/250\n",
            "420/420 [==============================] - 0s 48us/step - loss: 0.0849 - acc: 0.9762\n",
            "Epoch 195/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1040 - acc: 0.9810\n",
            "Epoch 196/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.0634 - acc: 0.9833\n",
            "Epoch 197/250\n",
            "420/420 [==============================] - 0s 73us/step - loss: 0.0771 - acc: 0.9786\n",
            "Epoch 198/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0698 - acc: 0.9810\n",
            "Epoch 199/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1146 - acc: 0.9667\n",
            "Epoch 200/250\n",
            "420/420 [==============================] - 0s 64us/step - loss: 0.0812 - acc: 0.9786\n",
            "Epoch 201/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.1027 - acc: 0.9667\n",
            "Epoch 202/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0803 - acc: 0.9833\n",
            "Epoch 203/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.1024 - acc: 0.9690\n",
            "Epoch 204/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0892 - acc: 0.9762\n",
            "Epoch 205/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0960 - acc: 0.9786\n",
            "Epoch 206/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0654 - acc: 0.9810\n",
            "Epoch 207/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0901 - acc: 0.9786\n",
            "Epoch 208/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0955 - acc: 0.9738\n",
            "Epoch 209/250\n",
            "420/420 [==============================] - 0s 59us/step - loss: 0.0812 - acc: 0.9810\n",
            "Epoch 210/250\n",
            "420/420 [==============================] - 0s 75us/step - loss: 0.0675 - acc: 0.9857\n",
            "Epoch 211/250\n",
            "420/420 [==============================] - 0s 72us/step - loss: 0.1433 - acc: 0.9667\n",
            "Epoch 212/250\n",
            "420/420 [==============================] - 0s 66us/step - loss: 0.0837 - acc: 0.9690\n",
            "Epoch 213/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0989 - acc: 0.9714\n",
            "Epoch 214/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0843 - acc: 0.9786\n",
            "Epoch 215/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0749 - acc: 0.9833\n",
            "Epoch 216/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0914 - acc: 0.9833\n",
            "Epoch 217/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0853 - acc: 0.9762\n",
            "Epoch 218/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0536 - acc: 0.9833\n",
            "Epoch 219/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0768 - acc: 0.9857\n",
            "Epoch 220/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0681 - acc: 0.9881\n",
            "Epoch 221/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0576 - acc: 0.9905\n",
            "Epoch 222/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0715 - acc: 0.9833\n",
            "Epoch 223/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0599 - acc: 0.9881\n",
            "Epoch 224/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0769 - acc: 0.9833\n",
            "Epoch 225/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0977 - acc: 0.9786\n",
            "Epoch 226/250\n",
            "420/420 [==============================] - 0s 63us/step - loss: 0.0868 - acc: 0.9833\n",
            "Epoch 227/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0649 - acc: 0.9881\n",
            "Epoch 228/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0646 - acc: 0.9857\n",
            "Epoch 229/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0752 - acc: 0.9833\n",
            "Epoch 230/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0699 - acc: 0.9833\n",
            "Epoch 231/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.0668 - acc: 0.9881\n",
            "Epoch 232/250\n",
            "420/420 [==============================] - 0s 52us/step - loss: 0.0941 - acc: 0.9714\n",
            "Epoch 233/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0759 - acc: 0.9810\n",
            "Epoch 234/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0736 - acc: 0.9810\n",
            "Epoch 235/250\n",
            "420/420 [==============================] - 0s 60us/step - loss: 0.0423 - acc: 0.9905\n",
            "Epoch 236/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0738 - acc: 0.9810\n",
            "Epoch 237/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0919 - acc: 0.9762\n",
            "Epoch 238/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0915 - acc: 0.9786\n",
            "Epoch 239/250\n",
            "420/420 [==============================] - 0s 57us/step - loss: 0.0707 - acc: 0.9786\n",
            "Epoch 240/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0759 - acc: 0.9810\n",
            "Epoch 241/250\n",
            "420/420 [==============================] - 0s 61us/step - loss: 0.0519 - acc: 0.9905\n",
            "Epoch 242/250\n",
            "420/420 [==============================] - 0s 58us/step - loss: 0.0645 - acc: 0.9833\n",
            "Epoch 243/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.1027 - acc: 0.9714\n",
            "Epoch 244/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.0536 - acc: 0.9929\n",
            "Epoch 245/250\n",
            "420/420 [==============================] - 0s 55us/step - loss: 0.0766 - acc: 0.9833\n",
            "Epoch 246/250\n",
            "420/420 [==============================] - 0s 53us/step - loss: 0.0863 - acc: 0.9833\n",
            "Epoch 247/250\n",
            "420/420 [==============================] - 0s 56us/step - loss: 0.0873 - acc: 0.9786\n",
            "Epoch 248/250\n",
            "420/420 [==============================] - 0s 54us/step - loss: 0.0875 - acc: 0.9738\n",
            "Epoch 249/250\n",
            "420/420 [==============================] - 0s 79us/step - loss: 0.0533 - acc: 0.9929\n",
            "Epoch 250/250\n",
            "420/420 [==============================] - 0s 62us/step - loss: 0.1105 - acc: 0.9738\n",
            "420/420 [==============================] - 0s 28us/step\n",
            "Test loss: 4.165554283914112\n",
            "Test accuracy: 0.561904760201772\n",
            "Test loss: 4.165554283914112\n",
            "Test accuracy: 0.561904760201772\n",
            "Seq Test accuracy score : 0.6523809523809524 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.42      0.77      0.55        35\n",
            "         1.0       0.00      0.00      0.00        52\n",
            "         2.0       0.68      0.83      0.75       139\n",
            "         3.0       0.99      0.57      0.72       121\n",
            "         4.0       0.53      0.86      0.66        73\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       420\n",
            "   macro avg       0.53      0.61      0.54       420\n",
            "weighted avg       0.64      0.65      0.62       420\n",
            "\n",
            "[[ 27   0   1   0   7]\n",
            " [ 19   0   7   0  26]\n",
            " [  5   0 115   1  18]\n",
            " [  6   0  42  69   4]\n",
            " [  7   0   3   0  63]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "\r  5%|▍         | 1/22 [00:07<02:43,  7.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[153   0   0   0   0]\n",
            " [  5   0   1   0   0]\n",
            " [  0   0 110   0   1]\n",
            " [  0   0   8  90   0]\n",
            " [  0   0   0   0  52]]\n",
            "Epoch 1/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.6331 - acc: 0.8845\n",
            "Epoch 2/250\n",
            "563/563 [==============================] - 0s 46us/step - loss: 0.5947 - acc: 0.8934\n",
            "Epoch 3/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.4416 - acc: 0.9165\n",
            "Epoch 4/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.4555 - acc: 0.9041\n",
            "Epoch 5/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.4540 - acc: 0.9272\n",
            "Epoch 6/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.4267 - acc: 0.9272\n",
            "Epoch 7/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.4311 - acc: 0.9147\n",
            "Epoch 8/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.4461 - acc: 0.9183\n",
            "Epoch 9/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.3525 - acc: 0.9361\n",
            "Epoch 10/250\n",
            "563/563 [==============================] - 0s 46us/step - loss: 0.4002 - acc: 0.9343\n",
            "Epoch 11/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.3857 - acc: 0.9290\n",
            "Epoch 12/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.4029 - acc: 0.9183\n",
            "Epoch 13/250\n",
            "563/563 [==============================] - 0s 66us/step - loss: 0.3385 - acc: 0.9343\n",
            "Epoch 14/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.3976 - acc: 0.9325\n",
            "Epoch 15/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.4056 - acc: 0.9076\n",
            "Epoch 16/250\n",
            "563/563 [==============================] - 0s 46us/step - loss: 0.3755 - acc: 0.9307\n",
            "Epoch 17/250\n",
            "563/563 [==============================] - 0s 44us/step - loss: 0.3046 - acc: 0.9361\n",
            "Epoch 18/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.3915 - acc: 0.9290\n",
            "Epoch 19/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.3686 - acc: 0.9218\n",
            "Epoch 20/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.3106 - acc: 0.9414\n",
            "Epoch 21/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.3681 - acc: 0.9254\n",
            "Epoch 22/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.3337 - acc: 0.9325\n",
            "Epoch 23/250\n",
            "563/563 [==============================] - 0s 43us/step - loss: 0.3279 - acc: 0.9378\n",
            "Epoch 24/250\n",
            "563/563 [==============================] - 0s 45us/step - loss: 0.2680 - acc: 0.9556\n",
            "Epoch 25/250\n",
            "563/563 [==============================] - 0s 46us/step - loss: 0.3570 - acc: 0.9307\n",
            "Epoch 26/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.3506 - acc: 0.9378\n",
            "Epoch 27/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.3637 - acc: 0.9378\n",
            "Epoch 28/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.2811 - acc: 0.9272\n",
            "Epoch 29/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2827 - acc: 0.9467\n",
            "Epoch 30/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2842 - acc: 0.9325\n",
            "Epoch 31/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.3009 - acc: 0.9414\n",
            "Epoch 32/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2730 - acc: 0.9396\n",
            "Epoch 33/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2816 - acc: 0.9396\n",
            "Epoch 34/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2662 - acc: 0.9503\n",
            "Epoch 35/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.3129 - acc: 0.9361\n",
            "Epoch 36/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2860 - acc: 0.9414\n",
            "Epoch 37/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2995 - acc: 0.9414\n",
            "Epoch 38/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2839 - acc: 0.9396\n",
            "Epoch 39/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.3063 - acc: 0.9432\n",
            "Epoch 40/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.3183 - acc: 0.9343\n",
            "Epoch 41/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.3147 - acc: 0.9361\n",
            "Epoch 42/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2519 - acc: 0.9538\n",
            "Epoch 43/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2998 - acc: 0.9325\n",
            "Epoch 44/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2906 - acc: 0.9414\n",
            "Epoch 45/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.3019 - acc: 0.9378\n",
            "Epoch 46/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2534 - acc: 0.9449\n",
            "Epoch 47/250\n",
            "563/563 [==============================] - 0s 71us/step - loss: 0.3127 - acc: 0.9343\n",
            "Epoch 48/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.2754 - acc: 0.9432\n",
            "Epoch 49/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2663 - acc: 0.9449\n",
            "Epoch 50/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2955 - acc: 0.9343\n",
            "Epoch 51/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.2797 - acc: 0.9432\n",
            "Epoch 52/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2782 - acc: 0.9520\n",
            "Epoch 53/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2360 - acc: 0.9520\n",
            "Epoch 54/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2757 - acc: 0.9467\n",
            "Epoch 55/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2700 - acc: 0.9378\n",
            "Epoch 56/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.2939 - acc: 0.9236\n",
            "Epoch 57/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.2222 - acc: 0.9556\n",
            "Epoch 58/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2862 - acc: 0.9432\n",
            "Epoch 59/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2765 - acc: 0.9467\n",
            "Epoch 60/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.2395 - acc: 0.9538\n",
            "Epoch 61/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2696 - acc: 0.9520\n",
            "Epoch 62/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.2660 - acc: 0.9467\n",
            "Epoch 63/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2620 - acc: 0.9485\n",
            "Epoch 64/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2412 - acc: 0.9520\n",
            "Epoch 65/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2572 - acc: 0.9414\n",
            "Epoch 66/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2286 - acc: 0.9609\n",
            "Epoch 67/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2254 - acc: 0.9538\n",
            "Epoch 68/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2542 - acc: 0.9449\n",
            "Epoch 69/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2253 - acc: 0.9538\n",
            "Epoch 70/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2448 - acc: 0.9609\n",
            "Epoch 71/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.2568 - acc: 0.9432\n",
            "Epoch 72/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2335 - acc: 0.9609\n",
            "Epoch 73/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2537 - acc: 0.9414\n",
            "Epoch 74/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2484 - acc: 0.9609\n",
            "Epoch 75/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2485 - acc: 0.9520\n",
            "Epoch 76/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2070 - acc: 0.9449\n",
            "Epoch 77/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.1945 - acc: 0.9574\n",
            "Epoch 78/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2382 - acc: 0.9432\n",
            "Epoch 79/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1996 - acc: 0.9556\n",
            "Epoch 80/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2078 - acc: 0.9520\n",
            "Epoch 81/250\n",
            "563/563 [==============================] - 0s 81us/step - loss: 0.2140 - acc: 0.9485\n",
            "Epoch 82/250\n",
            "563/563 [==============================] - 0s 63us/step - loss: 0.2194 - acc: 0.9520\n",
            "Epoch 83/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2313 - acc: 0.9432\n",
            "Epoch 84/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.2263 - acc: 0.9538\n",
            "Epoch 85/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2536 - acc: 0.9485\n",
            "Epoch 86/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2340 - acc: 0.9485\n",
            "Epoch 87/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2292 - acc: 0.9574\n",
            "Epoch 88/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2394 - acc: 0.9485\n",
            "Epoch 89/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.2230 - acc: 0.9556\n",
            "Epoch 90/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.2298 - acc: 0.9538\n",
            "Epoch 91/250\n",
            "563/563 [==============================] - 0s 61us/step - loss: 0.2336 - acc: 0.9556\n",
            "Epoch 92/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.2401 - acc: 0.9556\n",
            "Epoch 93/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.2281 - acc: 0.9609\n",
            "Epoch 94/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1956 - acc: 0.9645\n",
            "Epoch 95/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2506 - acc: 0.9645\n",
            "Epoch 96/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2203 - acc: 0.9485\n",
            "Epoch 97/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.2270 - acc: 0.9538\n",
            "Epoch 98/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.2035 - acc: 0.9574\n",
            "Epoch 99/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2336 - acc: 0.9485\n",
            "Epoch 100/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1950 - acc: 0.9645\n",
            "Epoch 101/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2042 - acc: 0.9609\n",
            "Epoch 102/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2687 - acc: 0.9432\n",
            "Epoch 103/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2604 - acc: 0.9432\n",
            "Epoch 104/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2468 - acc: 0.9538\n",
            "Epoch 105/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2367 - acc: 0.9432\n",
            "Epoch 106/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2044 - acc: 0.9556\n",
            "Epoch 107/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2055 - acc: 0.9591\n",
            "Epoch 108/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.2547 - acc: 0.9503\n",
            "Epoch 109/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2219 - acc: 0.9538\n",
            "Epoch 110/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2303 - acc: 0.9556\n",
            "Epoch 111/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1966 - acc: 0.9574\n",
            "Epoch 112/250\n",
            "563/563 [==============================] - 0s 71us/step - loss: 0.2073 - acc: 0.9645\n",
            "Epoch 113/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.2049 - acc: 0.9485\n",
            "Epoch 114/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2036 - acc: 0.9556\n",
            "Epoch 115/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2259 - acc: 0.9520\n",
            "Epoch 116/250\n",
            "563/563 [==============================] - 0s 64us/step - loss: 0.2043 - acc: 0.9663\n",
            "Epoch 117/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2129 - acc: 0.9556\n",
            "Epoch 118/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2279 - acc: 0.9556\n",
            "Epoch 119/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.2176 - acc: 0.9556\n",
            "Epoch 120/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2230 - acc: 0.9396\n",
            "Epoch 121/250\n",
            "563/563 [==============================] - 0s 66us/step - loss: 0.2103 - acc: 0.9538\n",
            "Epoch 122/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.2350 - acc: 0.9520\n",
            "Epoch 123/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2269 - acc: 0.9556\n",
            "Epoch 124/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2446 - acc: 0.9449\n",
            "Epoch 125/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.2029 - acc: 0.9556\n",
            "Epoch 126/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2152 - acc: 0.9574\n",
            "Epoch 127/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.2084 - acc: 0.9591\n",
            "Epoch 128/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2170 - acc: 0.9520\n",
            "Epoch 129/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1966 - acc: 0.9556\n",
            "Epoch 130/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1827 - acc: 0.9645\n",
            "Epoch 131/250\n",
            "563/563 [==============================] - 0s 61us/step - loss: 0.2140 - acc: 0.9591\n",
            "Epoch 132/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.2351 - acc: 0.9503\n",
            "Epoch 133/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.1924 - acc: 0.9627\n",
            "Epoch 134/250\n",
            "563/563 [==============================] - 0s 63us/step - loss: 0.2036 - acc: 0.9556\n",
            "Epoch 135/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1981 - acc: 0.9556\n",
            "Epoch 136/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2057 - acc: 0.9645\n",
            "Epoch 137/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1961 - acc: 0.9520\n",
            "Epoch 138/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2195 - acc: 0.9538\n",
            "Epoch 139/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1776 - acc: 0.9645\n",
            "Epoch 140/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.2280 - acc: 0.9467\n",
            "Epoch 141/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2157 - acc: 0.9538\n",
            "Epoch 142/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.2228 - acc: 0.9503\n",
            "Epoch 143/250\n",
            "563/563 [==============================] - 0s 63us/step - loss: 0.1999 - acc: 0.9591\n",
            "Epoch 144/250\n",
            "563/563 [==============================] - 0s 67us/step - loss: 0.2068 - acc: 0.9503\n",
            "Epoch 145/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2017 - acc: 0.9520\n",
            "Epoch 146/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1941 - acc: 0.9574\n",
            "Epoch 147/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.2069 - acc: 0.9538\n",
            "Epoch 148/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.1965 - acc: 0.9645\n",
            "Epoch 149/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1978 - acc: 0.9680\n",
            "Epoch 150/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.1994 - acc: 0.9627\n",
            "Epoch 151/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1901 - acc: 0.9538\n",
            "Epoch 152/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.2004 - acc: 0.9556\n",
            "Epoch 153/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1885 - acc: 0.9609\n",
            "Epoch 154/250\n",
            "563/563 [==============================] - 0s 64us/step - loss: 0.2091 - acc: 0.9538\n",
            "Epoch 155/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1863 - acc: 0.9627\n",
            "Epoch 156/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1763 - acc: 0.9716\n",
            "Epoch 157/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2281 - acc: 0.9538\n",
            "Epoch 158/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2250 - acc: 0.9556\n",
            "Epoch 159/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.1817 - acc: 0.9591\n",
            "Epoch 160/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1863 - acc: 0.9485\n",
            "Epoch 161/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1881 - acc: 0.9645\n",
            "Epoch 162/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1746 - acc: 0.9645\n",
            "Epoch 163/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2051 - acc: 0.9503\n",
            "Epoch 164/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2264 - acc: 0.9449\n",
            "Epoch 165/250\n",
            "563/563 [==============================] - 0s 63us/step - loss: 0.1831 - acc: 0.9574\n",
            "Epoch 166/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1982 - acc: 0.9591\n",
            "Epoch 167/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.2126 - acc: 0.9485\n",
            "Epoch 168/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1770 - acc: 0.9591\n",
            "Epoch 169/250\n",
            "563/563 [==============================] - 0s 65us/step - loss: 0.2094 - acc: 0.9591\n",
            "Epoch 170/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2035 - acc: 0.9574\n",
            "Epoch 171/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2082 - acc: 0.9574\n",
            "Epoch 172/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1876 - acc: 0.9627\n",
            "Epoch 173/250\n",
            "563/563 [==============================] - 0s 48us/step - loss: 0.1916 - acc: 0.9645\n",
            "Epoch 174/250\n",
            "563/563 [==============================] - 0s 68us/step - loss: 0.1853 - acc: 0.9574\n",
            "Epoch 175/250\n",
            "563/563 [==============================] - 0s 62us/step - loss: 0.1966 - acc: 0.9520\n",
            "Epoch 176/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.1682 - acc: 0.9663\n",
            "Epoch 177/250\n",
            "563/563 [==============================] - 0s 61us/step - loss: 0.2076 - acc: 0.9556\n",
            "Epoch 178/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.2099 - acc: 0.9485\n",
            "Epoch 179/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.1633 - acc: 0.9716\n",
            "Epoch 180/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1827 - acc: 0.9627\n",
            "Epoch 181/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.1901 - acc: 0.9627\n",
            "Epoch 182/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.1738 - acc: 0.9716\n",
            "Epoch 183/250\n",
            "563/563 [==============================] - 0s 69us/step - loss: 0.1571 - acc: 0.9716\n",
            "Epoch 184/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2111 - acc: 0.9556\n",
            "Epoch 185/250\n",
            "563/563 [==============================] - 0s 62us/step - loss: 0.2259 - acc: 0.9538\n",
            "Epoch 186/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1850 - acc: 0.9591\n",
            "Epoch 187/250\n",
            "563/563 [==============================] - 0s 64us/step - loss: 0.1609 - acc: 0.9680\n",
            "Epoch 188/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1800 - acc: 0.9574\n",
            "Epoch 189/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1643 - acc: 0.9627\n",
            "Epoch 190/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1593 - acc: 0.9698\n",
            "Epoch 191/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.2131 - acc: 0.9556\n",
            "Epoch 192/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1725 - acc: 0.9538\n",
            "Epoch 193/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1885 - acc: 0.9663\n",
            "Epoch 194/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.2074 - acc: 0.9627\n",
            "Epoch 195/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1610 - acc: 0.9627\n",
            "Epoch 196/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.1858 - acc: 0.9627\n",
            "Epoch 197/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.1933 - acc: 0.9574\n",
            "Epoch 198/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1593 - acc: 0.9663\n",
            "Epoch 199/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1895 - acc: 0.9627\n",
            "Epoch 200/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.1808 - acc: 0.9574\n",
            "Epoch 201/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.1520 - acc: 0.9698\n",
            "Epoch 202/250\n",
            "563/563 [==============================] - 0s 53us/step - loss: 0.1662 - acc: 0.9591\n",
            "Epoch 203/250\n",
            "563/563 [==============================] - 0s 47us/step - loss: 0.1704 - acc: 0.9627\n",
            "Epoch 204/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1714 - acc: 0.9627\n",
            "Epoch 205/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.1768 - acc: 0.9591\n",
            "Epoch 206/250\n",
            "563/563 [==============================] - 0s 66us/step - loss: 0.1880 - acc: 0.9574\n",
            "Epoch 207/250\n",
            "563/563 [==============================] - 0s 64us/step - loss: 0.1930 - acc: 0.9591\n",
            "Epoch 208/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.2377 - acc: 0.9432\n",
            "Epoch 209/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2035 - acc: 0.9520\n",
            "Epoch 210/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1612 - acc: 0.9609\n",
            "Epoch 211/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.1593 - acc: 0.9645\n",
            "Epoch 212/250\n",
            "563/563 [==============================] - 0s 56us/step - loss: 0.1521 - acc: 0.9680\n",
            "Epoch 213/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1520 - acc: 0.9663\n",
            "Epoch 214/250\n",
            "563/563 [==============================] - 0s 60us/step - loss: 0.1699 - acc: 0.9627\n",
            "Epoch 215/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1948 - acc: 0.9520\n",
            "Epoch 216/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.1546 - acc: 0.9574\n",
            "Epoch 217/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1827 - acc: 0.9627\n",
            "Epoch 218/250\n",
            "563/563 [==============================] - 0s 63us/step - loss: 0.1863 - acc: 0.9538\n",
            "Epoch 219/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1960 - acc: 0.9663\n",
            "Epoch 220/250\n",
            "563/563 [==============================] - 0s 63us/step - loss: 0.1653 - acc: 0.9556\n",
            "Epoch 221/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.1850 - acc: 0.9609\n",
            "Epoch 222/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1676 - acc: 0.9609\n",
            "Epoch 223/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1898 - acc: 0.9520\n",
            "Epoch 224/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.2001 - acc: 0.9538\n",
            "Epoch 225/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1485 - acc: 0.9698\n",
            "Epoch 226/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1750 - acc: 0.9520\n",
            "Epoch 227/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.2057 - acc: 0.9609\n",
            "Epoch 228/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1850 - acc: 0.9609\n",
            "Epoch 229/250\n",
            "563/563 [==============================] - 0s 61us/step - loss: 0.1534 - acc: 0.9680\n",
            "Epoch 230/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1700 - acc: 0.9698\n",
            "Epoch 231/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.1661 - acc: 0.9698\n",
            "Epoch 232/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1631 - acc: 0.9591\n",
            "Epoch 233/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.1618 - acc: 0.9698\n",
            "Epoch 234/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.1670 - acc: 0.9485\n",
            "Epoch 235/250\n",
            "563/563 [==============================] - 0s 59us/step - loss: 0.1523 - acc: 0.9645\n",
            "Epoch 236/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1803 - acc: 0.9609\n",
            "Epoch 237/250\n",
            "563/563 [==============================] - 0s 83us/step - loss: 0.1552 - acc: 0.9645\n",
            "Epoch 238/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1907 - acc: 0.9520\n",
            "Epoch 239/250\n",
            "563/563 [==============================] - 0s 54us/step - loss: 0.1738 - acc: 0.9663\n",
            "Epoch 240/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.1584 - acc: 0.9574\n",
            "Epoch 241/250\n",
            "563/563 [==============================] - 0s 58us/step - loss: 0.1703 - acc: 0.9627\n",
            "Epoch 242/250\n",
            "563/563 [==============================] - 0s 55us/step - loss: 0.1959 - acc: 0.9609\n",
            "Epoch 243/250\n",
            "563/563 [==============================] - 0s 46us/step - loss: 0.1917 - acc: 0.9591\n",
            "Epoch 244/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1607 - acc: 0.9663\n",
            "Epoch 245/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1617 - acc: 0.9574\n",
            "Epoch 246/250\n",
            "563/563 [==============================] - 0s 49us/step - loss: 0.1634 - acc: 0.9627\n",
            "Epoch 247/250\n",
            "563/563 [==============================] - 0s 50us/step - loss: 0.1791 - acc: 0.9645\n",
            "Epoch 248/250\n",
            "563/563 [==============================] - 0s 57us/step - loss: 0.1577 - acc: 0.9627\n",
            "Epoch 249/250\n",
            "563/563 [==============================] - 0s 52us/step - loss: 0.1699 - acc: 0.9591\n",
            "Epoch 250/250\n",
            "563/563 [==============================] - 0s 51us/step - loss: 0.1881 - acc: 0.9556\n",
            "563/563 [==============================] - 0s 25us/step\n",
            "Test loss: 2.3496875361163916\n",
            "Test accuracy: 0.6767317942785327\n",
            "Test loss: 2.3496875361163916\n",
            "Test accuracy: 0.6767317942785327\n",
            "Seq Test accuracy score : 0.7104795737122558 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.71      0.66        65\n",
            "         1.0       0.00      0.00      0.00        49\n",
            "         2.0       0.78      0.82      0.80       210\n",
            "         3.0       0.94      0.75      0.84       156\n",
            "         4.0       0.45      0.77      0.57        83\n",
            "\n",
            "   micro avg       0.71      0.71      0.71       563\n",
            "   macro avg       0.56      0.61      0.57       563\n",
            "weighted avg       0.69      0.71      0.69       563\n",
            "\n",
            "[[ 46   0   0   1  18]\n",
            " [  5   0  11   0  33]\n",
            " [  6   0 173   5  26]\n",
            " [  7   0  32 117   0]\n",
            " [ 11   0   7   1  64]]\n",
            "[[117   0   0   0   1]\n",
            " [  0   1   0   0   9]\n",
            " [  0   0 156   0   7]\n",
            " [  0   0   3 137   0]\n",
            " [  0   6   0   0 126]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 2/22 [00:17<02:47,  8.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.7824 - acc: 0.8183\n",
            "Epoch 2/250\n",
            "567/567 [==============================] - 0s 74us/step - loss: 0.6710 - acc: 0.8589\n",
            "Epoch 3/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.4991 - acc: 0.8836\n",
            "Epoch 4/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.5102 - acc: 0.9030\n",
            "Epoch 5/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.4888 - acc: 0.9083\n",
            "Epoch 6/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.4824 - acc: 0.9012\n",
            "Epoch 7/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.3943 - acc: 0.9012\n",
            "Epoch 8/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.3209 - acc: 0.9206\n",
            "Epoch 9/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.4038 - acc: 0.9206\n",
            "Epoch 10/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.3669 - acc: 0.9153\n",
            "Epoch 11/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.3235 - acc: 0.9259\n",
            "Epoch 12/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.3528 - acc: 0.9206\n",
            "Epoch 13/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.3485 - acc: 0.9312\n",
            "Epoch 14/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.3532 - acc: 0.9206\n",
            "Epoch 15/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.3136 - acc: 0.9259\n",
            "Epoch 16/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.2864 - acc: 0.9206\n",
            "Epoch 17/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.2770 - acc: 0.9312\n",
            "Epoch 18/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.2660 - acc: 0.9259\n",
            "Epoch 19/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.3029 - acc: 0.9383\n",
            "Epoch 20/250\n",
            "567/567 [==============================] - 0s 59us/step - loss: 0.2341 - acc: 0.9400\n",
            "Epoch 21/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.2438 - acc: 0.9400\n",
            "Epoch 22/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.2430 - acc: 0.9347\n",
            "Epoch 23/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.2971 - acc: 0.9383\n",
            "Epoch 24/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.2207 - acc: 0.9400\n",
            "Epoch 25/250\n",
            "567/567 [==============================] - 0s 46us/step - loss: 0.2296 - acc: 0.9330\n",
            "Epoch 26/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.2254 - acc: 0.9418\n",
            "Epoch 27/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.2131 - acc: 0.9418\n",
            "Epoch 28/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.2617 - acc: 0.9242\n",
            "Epoch 29/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.2629 - acc: 0.9418\n",
            "Epoch 30/250\n",
            "567/567 [==============================] - 0s 46us/step - loss: 0.2430 - acc: 0.9418\n",
            "Epoch 31/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.2238 - acc: 0.9471\n",
            "Epoch 32/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.3021 - acc: 0.9365\n",
            "Epoch 33/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.2167 - acc: 0.9312\n",
            "Epoch 34/250\n",
            "567/567 [==============================] - 0s 45us/step - loss: 0.2696 - acc: 0.9418\n",
            "Epoch 35/250\n",
            "567/567 [==============================] - 0s 66us/step - loss: 0.2626 - acc: 0.9365\n",
            "Epoch 36/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.2083 - acc: 0.9453\n",
            "Epoch 37/250\n",
            "567/567 [==============================] - 0s 59us/step - loss: 0.2010 - acc: 0.9471\n",
            "Epoch 38/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1790 - acc: 0.9577\n",
            "Epoch 39/250\n",
            "567/567 [==============================] - 0s 61us/step - loss: 0.2395 - acc: 0.9471\n",
            "Epoch 40/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1630 - acc: 0.9453\n",
            "Epoch 41/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1890 - acc: 0.9436\n",
            "Epoch 42/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.2112 - acc: 0.9524\n",
            "Epoch 43/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.2068 - acc: 0.9453\n",
            "Epoch 44/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.2493 - acc: 0.9365\n",
            "Epoch 45/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.2081 - acc: 0.9489\n",
            "Epoch 46/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.2121 - acc: 0.9400\n",
            "Epoch 47/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1902 - acc: 0.9559\n",
            "Epoch 48/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.2178 - acc: 0.9418\n",
            "Epoch 49/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1779 - acc: 0.9524\n",
            "Epoch 50/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1791 - acc: 0.9506\n",
            "Epoch 51/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1624 - acc: 0.9559\n",
            "Epoch 52/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1655 - acc: 0.9577\n",
            "Epoch 53/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.2113 - acc: 0.9453\n",
            "Epoch 54/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.2074 - acc: 0.9453\n",
            "Epoch 55/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1641 - acc: 0.9665\n",
            "Epoch 56/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1889 - acc: 0.9436\n",
            "Epoch 57/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1637 - acc: 0.9541\n",
            "Epoch 58/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1684 - acc: 0.9489\n",
            "Epoch 59/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1410 - acc: 0.9541\n",
            "Epoch 60/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1715 - acc: 0.9489\n",
            "Epoch 61/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1719 - acc: 0.9559\n",
            "Epoch 62/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.2212 - acc: 0.9365\n",
            "Epoch 63/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1651 - acc: 0.9471\n",
            "Epoch 64/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1616 - acc: 0.9506\n",
            "Epoch 65/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1594 - acc: 0.9524\n",
            "Epoch 66/250\n",
            "567/567 [==============================] - 0s 44us/step - loss: 0.1981 - acc: 0.9436\n",
            "Epoch 67/250\n",
            "567/567 [==============================] - 0s 68us/step - loss: 0.1617 - acc: 0.9524\n",
            "Epoch 68/250\n",
            "567/567 [==============================] - 0s 88us/step - loss: 0.1620 - acc: 0.9577\n",
            "Epoch 69/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1638 - acc: 0.9630\n",
            "Epoch 70/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1450 - acc: 0.9541\n",
            "Epoch 71/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1668 - acc: 0.9577\n",
            "Epoch 72/250\n",
            "567/567 [==============================] - 0s 58us/step - loss: 0.1377 - acc: 0.9612\n",
            "Epoch 73/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1835 - acc: 0.9489\n",
            "Epoch 74/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1461 - acc: 0.9630\n",
            "Epoch 75/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1418 - acc: 0.9559\n",
            "Epoch 76/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1545 - acc: 0.9612\n",
            "Epoch 77/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1544 - acc: 0.9612\n",
            "Epoch 78/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1755 - acc: 0.9436\n",
            "Epoch 79/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1483 - acc: 0.9559\n",
            "Epoch 80/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1638 - acc: 0.9541\n",
            "Epoch 81/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.2043 - acc: 0.9365\n",
            "Epoch 82/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1638 - acc: 0.9506\n",
            "Epoch 83/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1333 - acc: 0.9718\n",
            "Epoch 84/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1589 - acc: 0.9541\n",
            "Epoch 85/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1511 - acc: 0.9594\n",
            "Epoch 86/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1124 - acc: 0.9683\n",
            "Epoch 87/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1688 - acc: 0.9489\n",
            "Epoch 88/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1463 - acc: 0.9559\n",
            "Epoch 89/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1716 - acc: 0.9506\n",
            "Epoch 90/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1760 - acc: 0.9524\n",
            "Epoch 91/250\n",
            "567/567 [==============================] - 0s 61us/step - loss: 0.1603 - acc: 0.9541\n",
            "Epoch 92/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1271 - acc: 0.9683\n",
            "Epoch 93/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1647 - acc: 0.9559\n",
            "Epoch 94/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1602 - acc: 0.9453\n",
            "Epoch 95/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1765 - acc: 0.9541\n",
            "Epoch 96/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1478 - acc: 0.9647\n",
            "Epoch 97/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1330 - acc: 0.9594\n",
            "Epoch 98/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1652 - acc: 0.9541\n",
            "Epoch 99/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1340 - acc: 0.9612\n",
            "Epoch 100/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1512 - acc: 0.9524\n",
            "Epoch 101/250\n",
            "567/567 [==============================] - 0s 64us/step - loss: 0.1513 - acc: 0.9524\n",
            "Epoch 102/250\n",
            "567/567 [==============================] - 0s 67us/step - loss: 0.1505 - acc: 0.9524\n",
            "Epoch 103/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1552 - acc: 0.9541\n",
            "Epoch 104/250\n",
            "567/567 [==============================] - 0s 59us/step - loss: 0.1399 - acc: 0.9630\n",
            "Epoch 105/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1057 - acc: 0.9788\n",
            "Epoch 106/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1203 - acc: 0.9665\n",
            "Epoch 107/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1644 - acc: 0.9436\n",
            "Epoch 108/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.1050 - acc: 0.9771\n",
            "Epoch 109/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.1475 - acc: 0.9647\n",
            "Epoch 110/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1461 - acc: 0.9559\n",
            "Epoch 111/250\n",
            "567/567 [==============================] - 0s 46us/step - loss: 0.1252 - acc: 0.9683\n",
            "Epoch 112/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1459 - acc: 0.9506\n",
            "Epoch 113/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1156 - acc: 0.9718\n",
            "Epoch 114/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1120 - acc: 0.9647\n",
            "Epoch 115/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1823 - acc: 0.9559\n",
            "Epoch 116/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.1226 - acc: 0.9647\n",
            "Epoch 117/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1534 - acc: 0.9436\n",
            "Epoch 118/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1344 - acc: 0.9594\n",
            "Epoch 119/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1152 - acc: 0.9683\n",
            "Epoch 120/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.1162 - acc: 0.9700\n",
            "Epoch 121/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1190 - acc: 0.9647\n",
            "Epoch 122/250\n",
            "567/567 [==============================] - 0s 58us/step - loss: 0.1253 - acc: 0.9541\n",
            "Epoch 123/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1028 - acc: 0.9735\n",
            "Epoch 124/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1016 - acc: 0.9753\n",
            "Epoch 125/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1077 - acc: 0.9683\n",
            "Epoch 126/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1378 - acc: 0.9630\n",
            "Epoch 127/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1195 - acc: 0.9700\n",
            "Epoch 128/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.0814 - acc: 0.9753\n",
            "Epoch 129/250\n",
            "567/567 [==============================] - 0s 59us/step - loss: 0.1044 - acc: 0.9735\n",
            "Epoch 130/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1179 - acc: 0.9753\n",
            "Epoch 131/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.1232 - acc: 0.9753\n",
            "Epoch 132/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1256 - acc: 0.9612\n",
            "Epoch 133/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1219 - acc: 0.9683\n",
            "Epoch 134/250\n",
            "567/567 [==============================] - 0s 79us/step - loss: 0.1232 - acc: 0.9647\n",
            "Epoch 135/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1136 - acc: 0.9630\n",
            "Epoch 136/250\n",
            "567/567 [==============================] - 0s 62us/step - loss: 0.1074 - acc: 0.9718\n",
            "Epoch 137/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.1037 - acc: 0.9718\n",
            "Epoch 138/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1031 - acc: 0.9683\n",
            "Epoch 139/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1346 - acc: 0.9718\n",
            "Epoch 140/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1252 - acc: 0.9594\n",
            "Epoch 141/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.0909 - acc: 0.9753\n",
            "Epoch 142/250\n",
            "567/567 [==============================] - 0s 64us/step - loss: 0.1314 - acc: 0.9612\n",
            "Epoch 143/250\n",
            "567/567 [==============================] - 0s 60us/step - loss: 0.1306 - acc: 0.9612\n",
            "Epoch 144/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1153 - acc: 0.9683\n",
            "Epoch 145/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1247 - acc: 0.9594\n",
            "Epoch 146/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1338 - acc: 0.9630\n",
            "Epoch 147/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1149 - acc: 0.9683\n",
            "Epoch 148/250\n",
            "567/567 [==============================] - 0s 61us/step - loss: 0.1097 - acc: 0.9771\n",
            "Epoch 149/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1090 - acc: 0.9700\n",
            "Epoch 150/250\n",
            "567/567 [==============================] - 0s 45us/step - loss: 0.1230 - acc: 0.9753\n",
            "Epoch 151/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1076 - acc: 0.9683\n",
            "Epoch 152/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1106 - acc: 0.9647\n",
            "Epoch 153/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1215 - acc: 0.9735\n",
            "Epoch 154/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1223 - acc: 0.9612\n",
            "Epoch 155/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1059 - acc: 0.9665\n",
            "Epoch 156/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1161 - acc: 0.9612\n",
            "Epoch 157/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1510 - acc: 0.9612\n",
            "Epoch 158/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1033 - acc: 0.9735\n",
            "Epoch 159/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1264 - acc: 0.9647\n",
            "Epoch 160/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1210 - acc: 0.9612\n",
            "Epoch 161/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1212 - acc: 0.9647\n",
            "Epoch 162/250\n",
            "567/567 [==============================] - 0s 46us/step - loss: 0.1154 - acc: 0.9683\n",
            "Epoch 163/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1170 - acc: 0.9665\n",
            "Epoch 164/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1205 - acc: 0.9647\n",
            "Epoch 165/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.0966 - acc: 0.9700\n",
            "Epoch 166/250\n",
            "567/567 [==============================] - 0s 95us/step - loss: 0.1302 - acc: 0.9665\n",
            "Epoch 167/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1081 - acc: 0.9683\n",
            "Epoch 168/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1261 - acc: 0.9718\n",
            "Epoch 169/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1178 - acc: 0.9683\n",
            "Epoch 170/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1380 - acc: 0.9612\n",
            "Epoch 171/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.0994 - acc: 0.9718\n",
            "Epoch 172/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1265 - acc: 0.9683\n",
            "Epoch 173/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1293 - acc: 0.9594\n",
            "Epoch 174/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.0979 - acc: 0.9753\n",
            "Epoch 175/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0830 - acc: 0.9735\n",
            "Epoch 176/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.0909 - acc: 0.9806\n",
            "Epoch 177/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1376 - acc: 0.9559\n",
            "Epoch 178/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1147 - acc: 0.9665\n",
            "Epoch 179/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0797 - acc: 0.9824\n",
            "Epoch 180/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1162 - acc: 0.9612\n",
            "Epoch 181/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.0945 - acc: 0.9735\n",
            "Epoch 182/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.0976 - acc: 0.9753\n",
            "Epoch 183/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.0948 - acc: 0.9753\n",
            "Epoch 184/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0849 - acc: 0.9753\n",
            "Epoch 185/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1157 - acc: 0.9718\n",
            "Epoch 186/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1114 - acc: 0.9630\n",
            "Epoch 187/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0863 - acc: 0.9841\n",
            "Epoch 188/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1017 - acc: 0.9753\n",
            "Epoch 189/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.1089 - acc: 0.9647\n",
            "Epoch 190/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1034 - acc: 0.9700\n",
            "Epoch 191/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0931 - acc: 0.9735\n",
            "Epoch 192/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1043 - acc: 0.9718\n",
            "Epoch 193/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1059 - acc: 0.9700\n",
            "Epoch 194/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0975 - acc: 0.9718\n",
            "Epoch 195/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.0935 - acc: 0.9788\n",
            "Epoch 196/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.0881 - acc: 0.9718\n",
            "Epoch 197/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1004 - acc: 0.9718\n",
            "Epoch 198/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1245 - acc: 0.9683\n",
            "Epoch 199/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1038 - acc: 0.9665\n",
            "Epoch 200/250\n",
            "567/567 [==============================] - 0s 64us/step - loss: 0.0988 - acc: 0.9735\n",
            "Epoch 201/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.0984 - acc: 0.9753\n",
            "Epoch 202/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.0779 - acc: 0.9824\n",
            "Epoch 203/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.1269 - acc: 0.9683\n",
            "Epoch 204/250\n",
            "567/567 [==============================] - 0s 57us/step - loss: 0.1239 - acc: 0.9683\n",
            "Epoch 205/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.0901 - acc: 0.9683\n",
            "Epoch 206/250\n",
            "567/567 [==============================] - 0s 56us/step - loss: 0.1120 - acc: 0.9683\n",
            "Epoch 207/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1059 - acc: 0.9735\n",
            "Epoch 208/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.0797 - acc: 0.9771\n",
            "Epoch 209/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1040 - acc: 0.9700\n",
            "Epoch 210/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.1041 - acc: 0.9753\n",
            "Epoch 211/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1232 - acc: 0.9683\n",
            "Epoch 212/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1166 - acc: 0.9700\n",
            "Epoch 213/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.0789 - acc: 0.9806\n",
            "Epoch 214/250\n",
            "567/567 [==============================] - 0s 58us/step - loss: 0.0955 - acc: 0.9735\n",
            "Epoch 215/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.0812 - acc: 0.9806\n",
            "Epoch 216/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0944 - acc: 0.9753\n",
            "Epoch 217/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.0934 - acc: 0.9753\n",
            "Epoch 218/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1028 - acc: 0.9665\n",
            "Epoch 219/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1116 - acc: 0.9718\n",
            "Epoch 220/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.0852 - acc: 0.9753\n",
            "Epoch 221/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1056 - acc: 0.9665\n",
            "Epoch 222/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.0977 - acc: 0.9665\n",
            "Epoch 223/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.0796 - acc: 0.9841\n",
            "Epoch 224/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.1143 - acc: 0.9753\n",
            "Epoch 225/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1258 - acc: 0.9683\n",
            "Epoch 226/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.0969 - acc: 0.9683\n",
            "Epoch 227/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.0887 - acc: 0.9735\n",
            "Epoch 228/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1051 - acc: 0.9735\n",
            "Epoch 229/250\n",
            "567/567 [==============================] - 0s 46us/step - loss: 0.1089 - acc: 0.9788\n",
            "Epoch 230/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1189 - acc: 0.9718\n",
            "Epoch 231/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.0995 - acc: 0.9700\n",
            "Epoch 232/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.0831 - acc: 0.9771\n",
            "Epoch 233/250\n",
            "567/567 [==============================] - 0s 68us/step - loss: 0.0911 - acc: 0.9753\n",
            "Epoch 234/250\n",
            "567/567 [==============================] - 0s 55us/step - loss: 0.0748 - acc: 0.9771\n",
            "Epoch 235/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.0837 - acc: 0.9735\n",
            "Epoch 236/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.0950 - acc: 0.9718\n",
            "Epoch 237/250\n",
            "567/567 [==============================] - 0s 54us/step - loss: 0.1055 - acc: 0.9665\n",
            "Epoch 238/250\n",
            "567/567 [==============================] - 0s 46us/step - loss: 0.0849 - acc: 0.9771\n",
            "Epoch 239/250\n",
            "567/567 [==============================] - 0s 58us/step - loss: 0.0824 - acc: 0.9771\n",
            "Epoch 240/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.0909 - acc: 0.9718\n",
            "Epoch 241/250\n",
            "567/567 [==============================] - 0s 52us/step - loss: 0.1082 - acc: 0.9718\n",
            "Epoch 242/250\n",
            "567/567 [==============================] - 0s 53us/step - loss: 0.1022 - acc: 0.9735\n",
            "Epoch 243/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.1074 - acc: 0.9612\n",
            "Epoch 244/250\n",
            "567/567 [==============================] - 0s 49us/step - loss: 0.0767 - acc: 0.9841\n",
            "Epoch 245/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.1030 - acc: 0.9735\n",
            "Epoch 246/250\n",
            "567/567 [==============================] - 0s 47us/step - loss: 0.0757 - acc: 0.9788\n",
            "Epoch 247/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.0928 - acc: 0.9718\n",
            "Epoch 248/250\n",
            "567/567 [==============================] - 0s 50us/step - loss: 0.0792 - acc: 0.9806\n",
            "Epoch 249/250\n",
            "567/567 [==============================] - 0s 51us/step - loss: 0.0819 - acc: 0.9700\n",
            "Epoch 250/250\n",
            "567/567 [==============================] - 0s 48us/step - loss: 0.1065 - acc: 0.9683\n",
            "567/567 [==============================] - 0s 25us/step\n",
            "Test loss: 2.2177730467058785\n",
            "Test accuracy: 0.6984126976768386\n",
            "Test loss: 2.2177730467058785\n",
            "Test accuracy: 0.6984126976768386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▎        | 3/22 [00:26<02:44,  8.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.6825396825396826 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.47      0.77      0.59        56\n",
            "         1.0       0.00      0.00      0.00        67\n",
            "         2.0       0.64      0.90      0.74       182\n",
            "         3.0       0.91      0.68      0.78       205\n",
            "         4.0       0.63      0.74      0.68        57\n",
            "\n",
            "   micro avg       0.68      0.68      0.68       567\n",
            "   macro avg       0.53      0.62      0.56       567\n",
            "weighted avg       0.64      0.68      0.65       567\n",
            "\n",
            "[[ 43   0   4   6   3]\n",
            " [ 40   0  12   2  13]\n",
            " [  4   0 163   6   9]\n",
            " [  1   0  65 139   0]\n",
            " [  3   0  12   0  42]]\n",
            "[[262   0   0   5   0]\n",
            " [  1   0   0   0   0]\n",
            " [  0   0  80   0   0]\n",
            " [  0   0   5 140   0]\n",
            " [  0   0  21   0  53]]\n",
            "Epoch 1/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.8417 - acc: 0.8330\n",
            "Epoch 2/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.7295 - acc: 0.8520\n",
            "Epoch 3/250\n",
            "527/527 [==============================] - 0s 65us/step - loss: 0.4695 - acc: 0.8710\n",
            "Epoch 4/250\n",
            "527/527 [==============================] - 0s 65us/step - loss: 0.4076 - acc: 0.9051\n",
            "Epoch 5/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.3318 - acc: 0.9184\n",
            "Epoch 6/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.2911 - acc: 0.9203\n",
            "Epoch 7/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.2330 - acc: 0.9355\n",
            "Epoch 8/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.2689 - acc: 0.9108\n",
            "Epoch 9/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.2794 - acc: 0.9279\n",
            "Epoch 10/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.2746 - acc: 0.9317\n",
            "Epoch 11/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.2097 - acc: 0.9355\n",
            "Epoch 12/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.2511 - acc: 0.9070\n",
            "Epoch 13/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.2428 - acc: 0.9222\n",
            "Epoch 14/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.2405 - acc: 0.9051\n",
            "Epoch 15/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.2210 - acc: 0.9260\n",
            "Epoch 16/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.2167 - acc: 0.9336\n",
            "Epoch 17/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1929 - acc: 0.9393\n",
            "Epoch 18/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1710 - acc: 0.9488\n",
            "Epoch 19/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1667 - acc: 0.9545\n",
            "Epoch 20/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.2171 - acc: 0.9260\n",
            "Epoch 21/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1648 - acc: 0.9545\n",
            "Epoch 22/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.2454 - acc: 0.9298\n",
            "Epoch 23/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.2045 - acc: 0.9450\n",
            "Epoch 24/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1474 - acc: 0.9488\n",
            "Epoch 25/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.2167 - acc: 0.9355\n",
            "Epoch 26/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.2005 - acc: 0.9336\n",
            "Epoch 27/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1737 - acc: 0.9393\n",
            "Epoch 28/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1494 - acc: 0.9469\n",
            "Epoch 29/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.2131 - acc: 0.9298\n",
            "Epoch 30/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.1764 - acc: 0.9412\n",
            "Epoch 31/250\n",
            "527/527 [==============================] - 0s 60us/step - loss: 0.1861 - acc: 0.9431\n",
            "Epoch 32/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1780 - acc: 0.9488\n",
            "Epoch 33/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1709 - acc: 0.9545\n",
            "Epoch 34/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.1882 - acc: 0.9450\n",
            "Epoch 35/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1550 - acc: 0.9526\n",
            "Epoch 36/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9355\n",
            "Epoch 37/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1738 - acc: 0.9355\n",
            "Epoch 38/250\n",
            "527/527 [==============================] - 0s 71us/step - loss: 0.1800 - acc: 0.9507\n",
            "Epoch 39/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.2363 - acc: 0.9260\n",
            "Epoch 40/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1864 - acc: 0.9412\n",
            "Epoch 41/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1524 - acc: 0.9469\n",
            "Epoch 42/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1881 - acc: 0.9355\n",
            "Epoch 43/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1668 - acc: 0.9469\n",
            "Epoch 44/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1492 - acc: 0.9583\n",
            "Epoch 45/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1558 - acc: 0.9488\n",
            "Epoch 46/250\n",
            "527/527 [==============================] - 0s 59us/step - loss: 0.1654 - acc: 0.9450\n",
            "Epoch 47/250\n",
            "527/527 [==============================] - 0s 63us/step - loss: 0.1339 - acc: 0.9507\n",
            "Epoch 48/250\n",
            "527/527 [==============================] - 0s 59us/step - loss: 0.1581 - acc: 0.9507\n",
            "Epoch 49/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1897 - acc: 0.9431\n",
            "Epoch 50/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.1455 - acc: 0.9545\n",
            "Epoch 51/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1400 - acc: 0.9583\n",
            "Epoch 52/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1622 - acc: 0.9393\n",
            "Epoch 53/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1426 - acc: 0.9583\n",
            "Epoch 54/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1618 - acc: 0.9564\n",
            "Epoch 55/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1524 - acc: 0.9545\n",
            "Epoch 56/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1711 - acc: 0.9488\n",
            "Epoch 57/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1372 - acc: 0.9620\n",
            "Epoch 58/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1340 - acc: 0.9583\n",
            "Epoch 59/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1404 - acc: 0.9620\n",
            "Epoch 60/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1642 - acc: 0.9450\n",
            "Epoch 61/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1567 - acc: 0.9545\n",
            "Epoch 62/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1414 - acc: 0.9507\n",
            "Epoch 63/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.2002 - acc: 0.9374\n",
            "Epoch 64/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1537 - acc: 0.9469\n",
            "Epoch 65/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1535 - acc: 0.9469\n",
            "Epoch 66/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.1189 - acc: 0.9696\n",
            "Epoch 67/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1121 - acc: 0.9734\n",
            "Epoch 68/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1832 - acc: 0.9545\n",
            "Epoch 69/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1224 - acc: 0.9583\n",
            "Epoch 70/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1573 - acc: 0.9564\n",
            "Epoch 71/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1248 - acc: 0.9639\n",
            "Epoch 72/250\n",
            "527/527 [==============================] - 0s 66us/step - loss: 0.1332 - acc: 0.9620\n",
            "Epoch 73/250\n",
            "527/527 [==============================] - 0s 60us/step - loss: 0.1066 - acc: 0.9677\n",
            "Epoch 74/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1419 - acc: 0.9507\n",
            "Epoch 75/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.1489 - acc: 0.9583\n",
            "Epoch 76/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1613 - acc: 0.9564\n",
            "Epoch 77/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1626 - acc: 0.9545\n",
            "Epoch 78/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1949 - acc: 0.9526\n",
            "Epoch 79/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1112 - acc: 0.9658\n",
            "Epoch 80/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1374 - acc: 0.9602\n",
            "Epoch 81/250\n",
            "527/527 [==============================] - 0s 62us/step - loss: 0.1541 - acc: 0.9488\n",
            "Epoch 82/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1579 - acc: 0.9526\n",
            "Epoch 83/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1226 - acc: 0.9639\n",
            "Epoch 84/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9469\n",
            "Epoch 85/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1145 - acc: 0.9620\n",
            "Epoch 86/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1330 - acc: 0.9639\n",
            "Epoch 87/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.1291 - acc: 0.9620\n",
            "Epoch 88/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1165 - acc: 0.9696\n",
            "Epoch 89/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1261 - acc: 0.9639\n",
            "Epoch 90/250\n",
            "527/527 [==============================] - 0s 61us/step - loss: 0.1400 - acc: 0.9545\n",
            "Epoch 91/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1176 - acc: 0.9583\n",
            "Epoch 92/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.1246 - acc: 0.9583\n",
            "Epoch 93/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1052 - acc: 0.9696\n",
            "Epoch 94/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1085 - acc: 0.9734\n",
            "Epoch 95/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1328 - acc: 0.9620\n",
            "Epoch 96/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1376 - acc: 0.9602\n",
            "Epoch 97/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1456 - acc: 0.9526\n",
            "Epoch 98/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1404 - acc: 0.9658\n",
            "Epoch 99/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1061 - acc: 0.9677\n",
            "Epoch 100/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1373 - acc: 0.9526\n",
            "Epoch 101/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1232 - acc: 0.9658\n",
            "Epoch 102/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1040 - acc: 0.9677\n",
            "Epoch 103/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1054 - acc: 0.9677\n",
            "Epoch 104/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1323 - acc: 0.9658\n",
            "Epoch 105/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1222 - acc: 0.9620\n",
            "Epoch 106/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.0979 - acc: 0.9696\n",
            "Epoch 107/250\n",
            "527/527 [==============================] - 0s 71us/step - loss: 0.1153 - acc: 0.9602\n",
            "Epoch 108/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1240 - acc: 0.9564\n",
            "Epoch 109/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.1140 - acc: 0.9639\n",
            "Epoch 110/250\n",
            "527/527 [==============================] - 0s 59us/step - loss: 0.1134 - acc: 0.9677\n",
            "Epoch 111/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1702 - acc: 0.9526\n",
            "Epoch 112/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1065 - acc: 0.9696\n",
            "Epoch 113/250\n",
            "527/527 [==============================] - 0s 59us/step - loss: 0.1610 - acc: 0.9488\n",
            "Epoch 114/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1162 - acc: 0.9677\n",
            "Epoch 115/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.1212 - acc: 0.9620\n",
            "Epoch 116/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1227 - acc: 0.9677\n",
            "Epoch 117/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1262 - acc: 0.9639\n",
            "Epoch 118/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1194 - acc: 0.9677\n",
            "Epoch 119/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1421 - acc: 0.9620\n",
            "Epoch 120/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.1133 - acc: 0.9677\n",
            "Epoch 121/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1051 - acc: 0.9772\n",
            "Epoch 122/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1246 - acc: 0.9602\n",
            "Epoch 123/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.0960 - acc: 0.9772\n",
            "Epoch 124/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.0912 - acc: 0.9753\n",
            "Epoch 125/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1321 - acc: 0.9620\n",
            "Epoch 126/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1393 - acc: 0.9620\n",
            "Epoch 127/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.1140 - acc: 0.9658\n",
            "Epoch 128/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.0932 - acc: 0.9791\n",
            "Epoch 129/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0775 - acc: 0.9848\n",
            "Epoch 130/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.0933 - acc: 0.9791\n",
            "Epoch 131/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1192 - acc: 0.9620\n",
            "Epoch 132/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1094 - acc: 0.9677\n",
            "Epoch 133/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1293 - acc: 0.9602\n",
            "Epoch 134/250\n",
            "527/527 [==============================] - 0s 62us/step - loss: 0.1214 - acc: 0.9639\n",
            "Epoch 135/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1110 - acc: 0.9715\n",
            "Epoch 136/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0827 - acc: 0.9772\n",
            "Epoch 137/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.1328 - acc: 0.9488\n",
            "Epoch 138/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1190 - acc: 0.9658\n",
            "Epoch 139/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.1115 - acc: 0.9677\n",
            "Epoch 140/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1051 - acc: 0.9715\n",
            "Epoch 141/250\n",
            "527/527 [==============================] - 0s 66us/step - loss: 0.1023 - acc: 0.9696\n",
            "Epoch 142/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1311 - acc: 0.9583\n",
            "Epoch 143/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1236 - acc: 0.9620\n",
            "Epoch 144/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1439 - acc: 0.9583\n",
            "Epoch 145/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1003 - acc: 0.9715\n",
            "Epoch 146/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1094 - acc: 0.9715\n",
            "Epoch 147/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.1048 - acc: 0.9696\n",
            "Epoch 148/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0879 - acc: 0.9810\n",
            "Epoch 149/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1401 - acc: 0.9507\n",
            "Epoch 150/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.1083 - acc: 0.9696\n",
            "Epoch 151/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1231 - acc: 0.9564\n",
            "Epoch 152/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1131 - acc: 0.9715\n",
            "Epoch 153/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0915 - acc: 0.9753\n",
            "Epoch 154/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1554 - acc: 0.9564\n",
            "Epoch 155/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1223 - acc: 0.9620\n",
            "Epoch 156/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1093 - acc: 0.9658\n",
            "Epoch 157/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1076 - acc: 0.9677\n",
            "Epoch 158/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1229 - acc: 0.9583\n",
            "Epoch 159/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1153 - acc: 0.9639\n",
            "Epoch 160/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1137 - acc: 0.9658\n",
            "Epoch 161/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1121 - acc: 0.9620\n",
            "Epoch 162/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0995 - acc: 0.9791\n",
            "Epoch 163/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1117 - acc: 0.9639\n",
            "Epoch 164/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0937 - acc: 0.9791\n",
            "Epoch 165/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1045 - acc: 0.9715\n",
            "Epoch 166/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1109 - acc: 0.9715\n",
            "Epoch 167/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1051 - acc: 0.9677\n",
            "Epoch 168/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1127 - acc: 0.9677\n",
            "Epoch 169/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1029 - acc: 0.9658\n",
            "Epoch 170/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.1308 - acc: 0.9639\n",
            "Epoch 171/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.0921 - acc: 0.9734\n",
            "Epoch 172/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.0872 - acc: 0.9772\n",
            "Epoch 173/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1425 - acc: 0.9583\n",
            "Epoch 174/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.0901 - acc: 0.9810\n",
            "Epoch 175/250\n",
            "527/527 [==============================] - 0s 66us/step - loss: 0.1231 - acc: 0.9658\n",
            "Epoch 176/250\n",
            "527/527 [==============================] - 0s 62us/step - loss: 0.0973 - acc: 0.9677\n",
            "Epoch 177/250\n",
            "527/527 [==============================] - 0s 65us/step - loss: 0.0970 - acc: 0.9715\n",
            "Epoch 178/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.0892 - acc: 0.9829\n",
            "Epoch 179/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1168 - acc: 0.9620\n",
            "Epoch 180/250\n",
            "527/527 [==============================] - 0s 65us/step - loss: 0.0896 - acc: 0.9791\n",
            "Epoch 181/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0756 - acc: 0.9810\n",
            "Epoch 182/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.0989 - acc: 0.9658\n",
            "Epoch 183/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.0814 - acc: 0.9791\n",
            "Epoch 184/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.0694 - acc: 0.9829\n",
            "Epoch 185/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.0882 - acc: 0.9715\n",
            "Epoch 186/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0933 - acc: 0.9791\n",
            "Epoch 187/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.1182 - acc: 0.9658\n",
            "Epoch 188/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.0901 - acc: 0.9772\n",
            "Epoch 189/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.1019 - acc: 0.9658\n",
            "Epoch 190/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1002 - acc: 0.9677\n",
            "Epoch 191/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0924 - acc: 0.9772\n",
            "Epoch 192/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.0963 - acc: 0.9696\n",
            "Epoch 193/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.0927 - acc: 0.9715\n",
            "Epoch 194/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0869 - acc: 0.9753\n",
            "Epoch 195/250\n",
            "527/527 [==============================] - 0s 62us/step - loss: 0.1138 - acc: 0.9677\n",
            "Epoch 196/250\n",
            "527/527 [==============================] - 0s 46us/step - loss: 0.0778 - acc: 0.9772\n",
            "Epoch 197/250\n",
            "527/527 [==============================] - 0s 60us/step - loss: 0.0805 - acc: 0.9810\n",
            "Epoch 198/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1101 - acc: 0.9696\n",
            "Epoch 199/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.0912 - acc: 0.9734\n",
            "Epoch 200/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.0938 - acc: 0.9734\n",
            "Epoch 201/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0955 - acc: 0.9658\n",
            "Epoch 202/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.0953 - acc: 0.9677\n",
            "Epoch 203/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.0909 - acc: 0.9658\n",
            "Epoch 204/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0883 - acc: 0.9772\n",
            "Epoch 205/250\n",
            "527/527 [==============================] - 0s 56us/step - loss: 0.1059 - acc: 0.9734\n",
            "Epoch 206/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0714 - acc: 0.9829\n",
            "Epoch 207/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.0873 - acc: 0.9734\n",
            "Epoch 208/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0824 - acc: 0.9715\n",
            "Epoch 209/250\n",
            "527/527 [==============================] - 0s 76us/step - loss: 0.1074 - acc: 0.9620\n",
            "Epoch 210/250\n",
            "527/527 [==============================] - 0s 59us/step - loss: 0.0788 - acc: 0.9791\n",
            "Epoch 211/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.1009 - acc: 0.9734\n",
            "Epoch 212/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.0952 - acc: 0.9753\n",
            "Epoch 213/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0947 - acc: 0.9658\n",
            "Epoch 214/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.0600 - acc: 0.9848\n",
            "Epoch 215/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0879 - acc: 0.9772\n",
            "Epoch 216/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.0661 - acc: 0.9848\n",
            "Epoch 217/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0798 - acc: 0.9848\n",
            "Epoch 218/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0828 - acc: 0.9810\n",
            "Epoch 219/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.0825 - acc: 0.9772\n",
            "Epoch 220/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.0840 - acc: 0.9791\n",
            "Epoch 221/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0921 - acc: 0.9848\n",
            "Epoch 222/250\n",
            "527/527 [==============================] - 0s 51us/step - loss: 0.0937 - acc: 0.9772\n",
            "Epoch 223/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0846 - acc: 0.9753\n",
            "Epoch 224/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0706 - acc: 0.9848\n",
            "Epoch 225/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.0787 - acc: 0.9753\n",
            "Epoch 226/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0942 - acc: 0.9734\n",
            "Epoch 227/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0896 - acc: 0.9772\n",
            "Epoch 228/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.0744 - acc: 0.9734\n",
            "Epoch 229/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0999 - acc: 0.9829\n",
            "Epoch 230/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0965 - acc: 0.9734\n",
            "Epoch 231/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.1051 - acc: 0.9715\n",
            "Epoch 232/250\n",
            "527/527 [==============================] - 0s 58us/step - loss: 0.0832 - acc: 0.9810\n",
            "Epoch 233/250\n",
            "527/527 [==============================] - 0s 45us/step - loss: 0.0908 - acc: 0.9677\n",
            "Epoch 234/250\n",
            "527/527 [==============================] - 0s 55us/step - loss: 0.0955 - acc: 0.9677\n",
            "Epoch 235/250\n",
            "527/527 [==============================] - 0s 50us/step - loss: 0.1185 - acc: 0.9658\n",
            "Epoch 236/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0734 - acc: 0.9886\n",
            "Epoch 237/250\n",
            "527/527 [==============================] - 0s 57us/step - loss: 0.0970 - acc: 0.9734\n",
            "Epoch 238/250\n",
            "527/527 [==============================] - 0s 63us/step - loss: 0.0967 - acc: 0.9772\n",
            "Epoch 239/250\n",
            "527/527 [==============================] - 0s 54us/step - loss: 0.0673 - acc: 0.9848\n",
            "Epoch 240/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0848 - acc: 0.9829\n",
            "Epoch 241/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0904 - acc: 0.9734\n",
            "Epoch 242/250\n",
            "527/527 [==============================] - 0s 53us/step - loss: 0.0847 - acc: 0.9658\n",
            "Epoch 243/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0710 - acc: 0.9848\n",
            "Epoch 244/250\n",
            "527/527 [==============================] - 0s 68us/step - loss: 0.0833 - acc: 0.9810\n",
            "Epoch 245/250\n",
            "527/527 [==============================] - 0s 45us/step - loss: 0.0611 - acc: 0.9867\n",
            "Epoch 246/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0746 - acc: 0.9791\n",
            "Epoch 247/250\n",
            "527/527 [==============================] - 0s 52us/step - loss: 0.0826 - acc: 0.9753\n",
            "Epoch 248/250\n",
            "527/527 [==============================] - 0s 47us/step - loss: 0.1025 - acc: 0.9734\n",
            "Epoch 249/250\n",
            "527/527 [==============================] - 0s 49us/step - loss: 0.0824 - acc: 0.9810\n",
            "Epoch 250/250\n",
            "527/527 [==============================] - 0s 48us/step - loss: 0.0918 - acc: 0.9658\n",
            "527/527 [==============================] - 0s 26us/step\n",
            "Test loss: 3.0112041332237633\n",
            "Test accuracy: 0.6204933589730814\n",
            "Test loss: 3.0112041332237633\n",
            "Test accuracy: 0.6204933589730814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 4/22 [00:35<02:37,  8.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.6698292220113852 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.75      0.71        68\n",
            "         1.0       0.00      0.00      0.00        36\n",
            "         2.0       0.75      0.66      0.71       217\n",
            "         3.0       0.66      0.74      0.69       114\n",
            "         4.0       0.56      0.80      0.66        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       527\n",
            "   macro avg       0.53      0.59      0.55       527\n",
            "weighted avg       0.64      0.67      0.65       527\n",
            "\n",
            "[[ 51   0   1   4  12]\n",
            " [ 12   0   5   3  16]\n",
            " [ 11   0 144  33  29]\n",
            " [  1   0  28  84   1]\n",
            " [  1   0  13   4  74]]\n",
            "[[ 76   0   0   0   0]\n",
            " [  0   0   0   0   3]\n",
            " [  0   0  97   9   6]\n",
            " [  0   0   2 166   0]\n",
            " [  0   0   0   0 168]]\n",
            "Epoch 1/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 1.9609 - acc: 0.5000\n",
            "Epoch 2/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 1.1930 - acc: 0.6767\n",
            "Epoch 3/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.7683 - acc: 0.8147\n",
            "Epoch 4/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.5306 - acc: 0.8685\n",
            "Epoch 5/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.4279 - acc: 0.8879\n",
            "Epoch 6/250\n",
            "464/464 [==============================] - 0s 65us/step - loss: 0.3320 - acc: 0.9095\n",
            "Epoch 7/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.2984 - acc: 0.9138\n",
            "Epoch 8/250\n",
            "464/464 [==============================] - 0s 65us/step - loss: 0.2000 - acc: 0.9440\n",
            "Epoch 9/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.1816 - acc: 0.9483\n",
            "Epoch 10/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.1317 - acc: 0.9634\n",
            "Epoch 11/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.1094 - acc: 0.9569\n",
            "Epoch 12/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0491 - acc: 0.9828\n",
            "Epoch 13/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.0791 - acc: 0.9763\n",
            "Epoch 14/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0442 - acc: 0.9849\n",
            "Epoch 15/250\n",
            "464/464 [==============================] - 0s 69us/step - loss: 0.0551 - acc: 0.9828\n",
            "Epoch 16/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0624 - acc: 0.9828\n",
            "Epoch 17/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0266 - acc: 0.9892\n",
            "Epoch 18/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0384 - acc: 0.9849\n",
            "Epoch 19/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0276 - acc: 0.9871\n",
            "Epoch 20/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0298 - acc: 0.9892\n",
            "Epoch 21/250\n",
            "464/464 [==============================] - 0s 78us/step - loss: 0.0192 - acc: 0.9914\n",
            "Epoch 22/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0176 - acc: 0.9935\n",
            "Epoch 23/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0539 - acc: 0.9828\n",
            "Epoch 24/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0207 - acc: 0.9957\n",
            "Epoch 25/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0194 - acc: 0.9935\n",
            "Epoch 26/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0200 - acc: 0.9892\n",
            "Epoch 27/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0203 - acc: 0.9935\n",
            "Epoch 28/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0474 - acc: 0.9849\n",
            "Epoch 29/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0138 - acc: 0.9978\n",
            "Epoch 30/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0479 - acc: 0.9871\n",
            "Epoch 31/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0421 - acc: 0.9914\n",
            "Epoch 32/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0280 - acc: 0.9957\n",
            "Epoch 33/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0201 - acc: 0.9892\n",
            "Epoch 34/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.0211 - acc: 0.9914\n",
            "Epoch 35/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0159 - acc: 0.9978\n",
            "Epoch 36/250\n",
            "464/464 [==============================] - 0s 44us/step - loss: 0.0346 - acc: 0.9849\n",
            "Epoch 37/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0157 - acc: 0.9935\n",
            "Epoch 38/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0141 - acc: 0.9914\n",
            "Epoch 39/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0085 - acc: 0.9957\n",
            "Epoch 40/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0158 - acc: 0.9978\n",
            "Epoch 41/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0259 - acc: 0.9935\n",
            "Epoch 42/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.0250 - acc: 0.9957\n",
            "Epoch 43/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0262 - acc: 0.9935\n",
            "Epoch 44/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0204 - acc: 0.9957\n",
            "Epoch 45/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0195 - acc: 0.9978\n",
            "Epoch 46/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0109 - acc: 0.9978\n",
            "Epoch 47/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0109 - acc: 0.9957\n",
            "Epoch 48/250\n",
            "464/464 [==============================] - 0s 76us/step - loss: 0.0224 - acc: 0.9914\n",
            "Epoch 49/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0126 - acc: 0.9957\n",
            "Epoch 50/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0402 - acc: 0.9914\n",
            "Epoch 51/250\n",
            "464/464 [==============================] - 0s 44us/step - loss: 0.0290 - acc: 0.9892\n",
            "Epoch 52/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 53/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0284 - acc: 0.9914\n",
            "Epoch 54/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0203 - acc: 0.9957\n",
            "Epoch 55/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0240 - acc: 0.9935\n",
            "Epoch 56/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 57/250\n",
            "464/464 [==============================] - 0s 63us/step - loss: 0.0144 - acc: 0.9957\n",
            "Epoch 58/250\n",
            "464/464 [==============================] - 0s 45us/step - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 59/250\n",
            "464/464 [==============================] - 0s 70us/step - loss: 0.0143 - acc: 0.9957\n",
            "Epoch 60/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 61/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9957\n",
            "Epoch 62/250\n",
            "464/464 [==============================] - 0s 45us/step - loss: 0.0064 - acc: 0.9978\n",
            "Epoch 63/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0134 - acc: 0.9957\n",
            "Epoch 64/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0251 - acc: 0.9914\n",
            "Epoch 65/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0103 - acc: 0.9935\n",
            "Epoch 66/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0153 - acc: 0.9957\n",
            "Epoch 67/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0132 - acc: 0.9935\n",
            "Epoch 68/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 69/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0215 - acc: 0.9935\n",
            "Epoch 70/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0149 - acc: 0.9978\n",
            "Epoch 71/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0331 - acc: 0.9914\n",
            "Epoch 72/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0161 - acc: 0.9957\n",
            "Epoch 73/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0206 - acc: 0.9957\n",
            "Epoch 74/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0121 - acc: 0.9978\n",
            "Epoch 75/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 76/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0396 - acc: 0.9914\n",
            "Epoch 77/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0182 - acc: 0.9957\n",
            "Epoch 78/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0151 - acc: 0.9978\n",
            "Epoch 79/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0129 - acc: 0.9957\n",
            "Epoch 80/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0087 - acc: 0.9978\n",
            "Epoch 81/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0160 - acc: 0.9957\n",
            "Epoch 82/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0155 - acc: 0.9957\n",
            "Epoch 83/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0108 - acc: 0.9957\n",
            "Epoch 84/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0078 - acc: 0.9978\n",
            "Epoch 85/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0057 - acc: 0.9978\n",
            "Epoch 86/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0161 - acc: 0.9957\n",
            "Epoch 87/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0066 - acc: 0.9978\n",
            "Epoch 88/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0166 - acc: 0.9978\n",
            "Epoch 89/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0283 - acc: 0.9935\n",
            "Epoch 90/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0049 - acc: 0.9978\n",
            "Epoch 91/250\n",
            "464/464 [==============================] - 0s 45us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 92/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0211 - acc: 0.9935\n",
            "Epoch 93/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0185 - acc: 0.9978\n",
            "Epoch 94/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0232 - acc: 0.9957\n",
            "Epoch 95/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0158 - acc: 0.9957\n",
            "Epoch 96/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0263 - acc: 0.9935\n",
            "Epoch 97/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0139 - acc: 0.9978\n",
            "Epoch 98/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0084 - acc: 0.9957\n",
            "Epoch 99/250\n",
            "464/464 [==============================] - 0s 81us/step - loss: 0.0188 - acc: 0.9978\n",
            "Epoch 100/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0254 - acc: 0.9935\n",
            "Epoch 101/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0122 - acc: 0.9957\n",
            "Epoch 102/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0132 - acc: 0.9978\n",
            "Epoch 103/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0047 - acc: 0.9978\n",
            "Epoch 104/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0306 - acc: 0.9935\n",
            "Epoch 105/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0141 - acc: 0.9978\n",
            "Epoch 106/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0103 - acc: 0.9957\n",
            "Epoch 107/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0070 - acc: 0.9978\n",
            "Epoch 108/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0240 - acc: 0.9935\n",
            "Epoch 109/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 110/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0097 - acc: 0.9957\n",
            "Epoch 111/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0172 - acc: 0.9957\n",
            "Epoch 112/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 113/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0098 - acc: 0.9978\n",
            "Epoch 114/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 115/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0143 - acc: 0.9978\n",
            "Epoch 116/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 5.7842e-04 - acc: 1.0000\n",
            "Epoch 117/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 118/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0120 - acc: 0.9957\n",
            "Epoch 119/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 120/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0027 - acc: 0.9978\n",
            "Epoch 121/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0139 - acc: 0.9978\n",
            "Epoch 122/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0070 - acc: 0.9978\n",
            "Epoch 123/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0123 - acc: 0.9978\n",
            "Epoch 124/250\n",
            "464/464 [==============================] - 0s 45us/step - loss: 0.0320 - acc: 0.9935\n",
            "Epoch 125/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0151 - acc: 0.9978\n",
            "Epoch 126/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0121 - acc: 0.9978\n",
            "Epoch 127/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0135 - acc: 0.9978\n",
            "Epoch 128/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0087 - acc: 0.9978\n",
            "Epoch 129/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0144 - acc: 0.9957\n",
            "Epoch 130/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0326 - acc: 0.9914\n",
            "Epoch 131/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0388 - acc: 0.9914\n",
            "Epoch 132/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 133/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 134/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0054 - acc: 0.9978\n",
            "Epoch 135/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0057 - acc: 0.9978\n",
            "Epoch 136/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0070 - acc: 0.9978\n",
            "Epoch 137/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 138/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 139/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0072 - acc: 0.9957\n",
            "Epoch 140/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0115 - acc: 0.9978\n",
            "Epoch 141/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 142/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0040 - acc: 0.9978\n",
            "Epoch 143/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0252 - acc: 0.9957\n",
            "Epoch 144/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 145/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0354 - acc: 0.9935\n",
            "Epoch 146/250\n",
            "464/464 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 147/250\n",
            "464/464 [==============================] - 0s 59us/step - loss: 0.0260 - acc: 0.9935\n",
            "Epoch 148/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 149/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0117 - acc: 0.9978\n",
            "Epoch 150/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 0.0212 - acc: 0.9957\n",
            "Epoch 151/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0067 - acc: 0.9978\n",
            "Epoch 152/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 153/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 154/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 155/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 6.5330e-04 - acc: 1.0000\n",
            "Epoch 156/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 157/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0157 - acc: 0.9957\n",
            "Epoch 158/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 159/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 8.0791e-04 - acc: 1.0000\n",
            "Epoch 160/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 161/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0039 - acc: 0.9978\n",
            "Epoch 162/250\n",
            "464/464 [==============================] - 0s 45us/step - loss: 0.0061 - acc: 0.9978\n",
            "Epoch 163/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0128 - acc: 0.9978\n",
            "Epoch 164/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 165/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0328 - acc: 0.9935\n",
            "Epoch 166/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0132 - acc: 0.9978\n",
            "Epoch 167/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0125 - acc: 0.9978\n",
            "Epoch 168/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0126 - acc: 0.9978\n",
            "Epoch 169/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0330 - acc: 0.9914\n",
            "Epoch 170/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 171/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0068 - acc: 0.9978\n",
            "Epoch 172/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0118 - acc: 0.9978\n",
            "Epoch 173/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 6.2087e-04 - acc: 1.0000\n",
            "Epoch 174/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0152 - acc: 0.9957\n",
            "Epoch 175/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0222 - acc: 0.9957\n",
            "Epoch 176/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0455 - acc: 0.9914\n",
            "Epoch 177/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0142 - acc: 0.9957\n",
            "Epoch 178/250\n",
            "464/464 [==============================] - 0s 66us/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 179/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0175 - acc: 0.9957\n",
            "Epoch 180/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0143 - acc: 0.9978\n",
            "Epoch 181/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 182/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0033 - acc: 0.9978\n",
            "Epoch 183/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0131 - acc: 0.9978\n",
            "Epoch 184/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 9.2851e-04 - acc: 1.0000\n",
            "Epoch 185/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0049 - acc: 0.9978\n",
            "Epoch 186/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0118 - acc: 0.9978\n",
            "Epoch 187/250\n",
            "464/464 [==============================] - 0s 60us/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 188/250\n",
            "464/464 [==============================] - 0s 57us/step - loss: 0.0066 - acc: 0.9978\n",
            "Epoch 189/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0242 - acc: 0.9935\n",
            "Epoch 190/250\n",
            "464/464 [==============================] - 0s 46us/step - loss: 0.0114 - acc: 0.9978\n",
            "Epoch 191/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 192/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0070 - acc: 0.9978\n",
            "Epoch 193/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0106 - acc: 0.9978\n",
            "Epoch 194/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0127 - acc: 0.9978\n",
            "Epoch 195/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0322 - acc: 0.9935\n",
            "Epoch 196/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 197/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 198/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 8.2965e-04 - acc: 1.0000\n",
            "Epoch 199/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0261 - acc: 0.9935\n",
            "Epoch 200/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0119 - acc: 0.9978\n",
            "Epoch 201/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0044 - acc: 0.9978\n",
            "Epoch 202/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0120 - acc: 0.9978\n",
            "Epoch 203/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 6.8849e-04 - acc: 1.0000\n",
            "Epoch 204/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0111 - acc: 0.9978\n",
            "Epoch 205/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0118 - acc: 0.9978\n",
            "Epoch 206/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0236 - acc: 0.9957\n",
            "Epoch 207/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 208/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0255 - acc: 0.9935\n",
            "Epoch 209/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0230 - acc: 0.9957\n",
            "Epoch 210/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 211/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 212/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0230 - acc: 0.9957\n",
            "Epoch 213/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0225 - acc: 0.9957\n",
            "Epoch 214/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0050 - acc: 0.9978\n",
            "Epoch 215/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 216/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0150 - acc: 0.9957\n",
            "Epoch 217/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 218/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0138 - acc: 0.9978\n",
            "Epoch 219/250\n",
            "464/464 [==============================] - 0s 72us/step - loss: 0.0205 - acc: 0.9957\n",
            "Epoch 220/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0228 - acc: 0.9957\n",
            "Epoch 221/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0110 - acc: 0.9978\n",
            "Epoch 222/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0333 - acc: 0.9935\n",
            "Epoch 223/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0128 - acc: 0.9978\n",
            "Epoch 224/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0165 - acc: 0.9957\n",
            "Epoch 225/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0248 - acc: 0.9957\n",
            "Epoch 226/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0117 - acc: 0.9978\n",
            "Epoch 227/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 228/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 9.7370e-04 - acc: 1.0000\n",
            "Epoch 229/250\n",
            "464/464 [==============================] - 0s 52us/step - loss: 0.0141 - acc: 0.9978\n",
            "Epoch 230/250\n",
            "464/464 [==============================] - 0s 66us/step - loss: 1.4185e-04 - acc: 1.0000\n",
            "Epoch 231/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0069 - acc: 0.9957\n",
            "Epoch 232/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0109 - acc: 0.9978\n",
            "Epoch 233/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0123 - acc: 0.9978\n",
            "Epoch 234/250\n",
            "464/464 [==============================] - 0s 47us/step - loss: 0.0267 - acc: 0.9935\n",
            "Epoch 235/250\n",
            "464/464 [==============================] - 0s 53us/step - loss: 0.0115 - acc: 0.9978\n",
            "Epoch 236/250\n",
            "464/464 [==============================] - 0s 55us/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 237/250\n",
            "464/464 [==============================] - 0s 56us/step - loss: 5.7296e-04 - acc: 1.0000\n",
            "Epoch 238/250\n",
            "464/464 [==============================] - 0s 54us/step - loss: 0.0116 - acc: 0.9978\n",
            "Epoch 239/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0123 - acc: 0.9978\n",
            "Epoch 240/250\n",
            "464/464 [==============================] - 0s 65us/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 241/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0115 - acc: 0.9978\n",
            "Epoch 242/250\n",
            "464/464 [==============================] - 0s 58us/step - loss: 0.0222 - acc: 0.9957\n",
            "Epoch 243/250\n",
            "464/464 [==============================] - 0s 64us/step - loss: 0.0138 - acc: 0.9978\n",
            "Epoch 244/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0243 - acc: 0.9935\n",
            "Epoch 245/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0115 - acc: 0.9978\n",
            "Epoch 246/250\n",
            "464/464 [==============================] - 0s 49us/step - loss: 0.0114 - acc: 0.9978\n",
            "Epoch 247/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0128 - acc: 0.9978\n",
            "Epoch 248/250\n",
            "464/464 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 249/250\n",
            "464/464 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9957\n",
            "Epoch 250/250\n",
            "464/464 [==============================] - 0s 50us/step - loss: 0.0060 - acc: 0.9978\n",
            "464/464 [==============================] - 0s 32us/step\n",
            "Test loss: 2.961682677268982\n",
            "Test accuracy: 0.4784482717514038\n",
            "Test loss: 2.961682677268982\n",
            "Test accuracy: 0.4784482717514038\n",
            "Seq Test accuracy score : 0.3081896551724138 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 5/22 [00:43<02:24,  8.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.10      0.87      0.17        15\n",
            "         1.0       0.00      0.00      0.00        13\n",
            "         2.0       0.66      0.24      0.35       245\n",
            "         3.0       0.98      0.38      0.54       112\n",
            "         4.0       0.15      0.37      0.21        79\n",
            "\n",
            "   micro avg       0.31      0.31      0.31       464\n",
            "   macro avg       0.38      0.37      0.26       464\n",
            "weighted avg       0.61      0.31      0.36       464\n",
            "\n",
            "[[ 13   0   0   0   2]\n",
            " [  3   0   0   0  10]\n",
            " [ 40   0  59   1 145]\n",
            " [ 31   0  29  42  10]\n",
            " [ 49   0   1   0  29]]\n",
            "[[111   0   0   0]\n",
            " [257   0   0   0]\n",
            " [ 17   0   0   0]\n",
            " [ 79   0   0   0]]\n",
            "Epoch 1/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 1.8295 - acc: 0.6933\n",
            "Epoch 2/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 1.5437 - acc: 0.7406\n",
            "Epoch 3/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 1.0372 - acc: 0.8055\n",
            "Epoch 4/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.6590 - acc: 0.8529\n",
            "Epoch 5/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.4055 - acc: 0.8853\n",
            "Epoch 6/250\n",
            "401/401 [==============================] - 0s 82us/step - loss: 0.5523 - acc: 0.8678\n",
            "Epoch 7/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.5837 - acc: 0.8653\n",
            "Epoch 8/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.3934 - acc: 0.8928\n",
            "Epoch 9/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.5115 - acc: 0.8728\n",
            "Epoch 10/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.4764 - acc: 0.8753\n",
            "Epoch 11/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.4295 - acc: 0.9152\n",
            "Epoch 12/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.4307 - acc: 0.9002\n",
            "Epoch 13/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.4020 - acc: 0.9052\n",
            "Epoch 14/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.3996 - acc: 0.8878\n",
            "Epoch 15/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.3901 - acc: 0.8928\n",
            "Epoch 16/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.3682 - acc: 0.9152\n",
            "Epoch 17/250\n",
            "401/401 [==============================] - 0s 84us/step - loss: 0.3747 - acc: 0.8978\n",
            "Epoch 18/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.3557 - acc: 0.9152\n",
            "Epoch 19/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.3584 - acc: 0.8953\n",
            "Epoch 20/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.3441 - acc: 0.9027\n",
            "Epoch 21/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.3691 - acc: 0.8903\n",
            "Epoch 22/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.3517 - acc: 0.9102\n",
            "Epoch 23/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.3750 - acc: 0.9002\n",
            "Epoch 24/250\n",
            "401/401 [==============================] - 0s 63us/step - loss: 0.2676 - acc: 0.9077\n",
            "Epoch 25/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.3322 - acc: 0.9177\n",
            "Epoch 26/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.3911 - acc: 0.8978\n",
            "Epoch 27/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.3127 - acc: 0.9152\n",
            "Epoch 28/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.3451 - acc: 0.9027\n",
            "Epoch 29/250\n",
            "401/401 [==============================] - 0s 67us/step - loss: 0.3268 - acc: 0.9177\n",
            "Epoch 30/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.3075 - acc: 0.9202\n",
            "Epoch 31/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.3605 - acc: 0.9127\n",
            "Epoch 32/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.3283 - acc: 0.9102\n",
            "Epoch 33/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.3255 - acc: 0.9152\n",
            "Epoch 34/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.3024 - acc: 0.8978\n",
            "Epoch 35/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.3738 - acc: 0.9127\n",
            "Epoch 36/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.3244 - acc: 0.9127\n",
            "Epoch 37/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.2931 - acc: 0.9252\n",
            "Epoch 38/250\n",
            "401/401 [==============================] - 0s 67us/step - loss: 0.3441 - acc: 0.9077\n",
            "Epoch 39/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.3204 - acc: 0.9302\n",
            "Epoch 40/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.3296 - acc: 0.9252\n",
            "Epoch 41/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2927 - acc: 0.9177\n",
            "Epoch 42/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.3182 - acc: 0.9302\n",
            "Epoch 43/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.3121 - acc: 0.9077\n",
            "Epoch 44/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.3134 - acc: 0.9252\n",
            "Epoch 45/250\n",
            "401/401 [==============================] - 0s 77us/step - loss: 0.3228 - acc: 0.9127\n",
            "Epoch 46/250\n",
            "401/401 [==============================] - 0s 71us/step - loss: 0.2854 - acc: 0.9227\n",
            "Epoch 47/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.3109 - acc: 0.9202\n",
            "Epoch 48/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.3260 - acc: 0.9202\n",
            "Epoch 49/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.3252 - acc: 0.9177\n",
            "Epoch 50/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.2962 - acc: 0.9127\n",
            "Epoch 51/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.3182 - acc: 0.9202\n",
            "Epoch 52/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.2909 - acc: 0.9327\n",
            "Epoch 53/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.3435 - acc: 0.9102\n",
            "Epoch 54/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.2578 - acc: 0.9352\n",
            "Epoch 55/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.2831 - acc: 0.9352\n",
            "Epoch 56/250\n",
            "401/401 [==============================] - 0s 68us/step - loss: 0.2746 - acc: 0.9302\n",
            "Epoch 57/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2765 - acc: 0.9426\n",
            "Epoch 58/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.3058 - acc: 0.9302\n",
            "Epoch 59/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.2634 - acc: 0.9252\n",
            "Epoch 60/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.3027 - acc: 0.9227\n",
            "Epoch 61/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.3102 - acc: 0.9252\n",
            "Epoch 62/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.2549 - acc: 0.9377\n",
            "Epoch 63/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2875 - acc: 0.9302\n",
            "Epoch 64/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.2657 - acc: 0.9302\n",
            "Epoch 65/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.2935 - acc: 0.9202\n",
            "Epoch 66/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2788 - acc: 0.9302\n",
            "Epoch 67/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.2528 - acc: 0.9252\n",
            "Epoch 68/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.2867 - acc: 0.9377\n",
            "Epoch 69/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.2619 - acc: 0.9377\n",
            "Epoch 70/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.2541 - acc: 0.9377\n",
            "Epoch 71/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.2235 - acc: 0.9377\n",
            "Epoch 72/250\n",
            "401/401 [==============================] - 0s 70us/step - loss: 0.2850 - acc: 0.9476\n",
            "Epoch 73/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.2479 - acc: 0.9401\n",
            "Epoch 74/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2541 - acc: 0.9451\n",
            "Epoch 75/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2858 - acc: 0.9252\n",
            "Epoch 76/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.2922 - acc: 0.9401\n",
            "Epoch 77/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.2527 - acc: 0.9277\n",
            "Epoch 78/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.2282 - acc: 0.9451\n",
            "Epoch 79/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.2168 - acc: 0.9626\n",
            "Epoch 80/250\n",
            "401/401 [==============================] - 0s 67us/step - loss: 0.2121 - acc: 0.9501\n",
            "Epoch 81/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.2517 - acc: 0.9401\n",
            "Epoch 82/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.2489 - acc: 0.9377\n",
            "Epoch 83/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2476 - acc: 0.9451\n",
            "Epoch 84/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.2382 - acc: 0.9426\n",
            "Epoch 85/250\n",
            "401/401 [==============================] - 0s 82us/step - loss: 0.2337 - acc: 0.9377\n",
            "Epoch 86/250\n",
            "401/401 [==============================] - 0s 74us/step - loss: 0.2578 - acc: 0.9377\n",
            "Epoch 87/250\n",
            "401/401 [==============================] - 0s 71us/step - loss: 0.2329 - acc: 0.9476\n",
            "Epoch 88/250\n",
            "401/401 [==============================] - 0s 71us/step - loss: 0.2221 - acc: 0.9576\n",
            "Epoch 89/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2111 - acc: 0.9451\n",
            "Epoch 90/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.2109 - acc: 0.9551\n",
            "Epoch 91/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2996 - acc: 0.9127\n",
            "Epoch 92/250\n",
            "401/401 [==============================] - 0s 63us/step - loss: 0.2105 - acc: 0.9551\n",
            "Epoch 93/250\n",
            "401/401 [==============================] - 0s 72us/step - loss: 0.2069 - acc: 0.9426\n",
            "Epoch 94/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.2072 - acc: 0.9626\n",
            "Epoch 95/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2079 - acc: 0.9526\n",
            "Epoch 96/250\n",
            "401/401 [==============================] - 0s 52us/step - loss: 0.2394 - acc: 0.9451\n",
            "Epoch 97/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2040 - acc: 0.9451\n",
            "Epoch 98/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.2601 - acc: 0.9426\n",
            "Epoch 99/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2247 - acc: 0.9501\n",
            "Epoch 100/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.2069 - acc: 0.9576\n",
            "Epoch 101/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.2500 - acc: 0.9327\n",
            "Epoch 102/250\n",
            "401/401 [==============================] - 0s 76us/step - loss: 0.2316 - acc: 0.9426\n",
            "Epoch 103/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.2166 - acc: 0.9526\n",
            "Epoch 104/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.2075 - acc: 0.9501\n",
            "Epoch 105/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.2286 - acc: 0.9476\n",
            "Epoch 106/250\n",
            "401/401 [==============================] - 0s 67us/step - loss: 0.2172 - acc: 0.9576\n",
            "Epoch 107/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.2189 - acc: 0.9501\n",
            "Epoch 108/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1969 - acc: 0.9526\n",
            "Epoch 109/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.2069 - acc: 0.9476\n",
            "Epoch 110/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1976 - acc: 0.9501\n",
            "Epoch 111/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.2204 - acc: 0.9501\n",
            "Epoch 112/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.2275 - acc: 0.9451\n",
            "Epoch 113/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.2285 - acc: 0.9551\n",
            "Epoch 114/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2111 - acc: 0.9551\n",
            "Epoch 115/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2086 - acc: 0.9551\n",
            "Epoch 116/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.2330 - acc: 0.9426\n",
            "Epoch 117/250\n",
            "401/401 [==============================] - 0s 63us/step - loss: 0.2591 - acc: 0.9327\n",
            "Epoch 118/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.2137 - acc: 0.9501\n",
            "Epoch 119/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.2251 - acc: 0.9377\n",
            "Epoch 120/250\n",
            "401/401 [==============================] - 0s 71us/step - loss: 0.1986 - acc: 0.9626\n",
            "Epoch 121/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.2253 - acc: 0.9526\n",
            "Epoch 122/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.2287 - acc: 0.9501\n",
            "Epoch 123/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1869 - acc: 0.9426\n",
            "Epoch 124/250\n",
            "401/401 [==============================] - 0s 74us/step - loss: 0.2473 - acc: 0.9476\n",
            "Epoch 125/250\n",
            "401/401 [==============================] - 0s 84us/step - loss: 0.1944 - acc: 0.9526\n",
            "Epoch 126/250\n",
            "401/401 [==============================] - 0s 72us/step - loss: 0.2578 - acc: 0.9352\n",
            "Epoch 127/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.2146 - acc: 0.9501\n",
            "Epoch 128/250\n",
            "401/401 [==============================] - 0s 68us/step - loss: 0.2166 - acc: 0.9526\n",
            "Epoch 129/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.2052 - acc: 0.9601\n",
            "Epoch 130/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.2520 - acc: 0.9401\n",
            "Epoch 131/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.2739 - acc: 0.9277\n",
            "Epoch 132/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.1891 - acc: 0.9651\n",
            "Epoch 133/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.1833 - acc: 0.9526\n",
            "Epoch 134/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.1974 - acc: 0.9551\n",
            "Epoch 135/250\n",
            "401/401 [==============================] - 0s 74us/step - loss: 0.2160 - acc: 0.9551\n",
            "Epoch 136/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.2151 - acc: 0.9476\n",
            "Epoch 137/250\n",
            "401/401 [==============================] - 0s 72us/step - loss: 0.2220 - acc: 0.9526\n",
            "Epoch 138/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.1583 - acc: 0.9651\n",
            "Epoch 139/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.2018 - acc: 0.9576\n",
            "Epoch 140/250\n",
            "401/401 [==============================] - 0s 76us/step - loss: 0.2080 - acc: 0.9551\n",
            "Epoch 141/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1719 - acc: 0.9626\n",
            "Epoch 142/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1643 - acc: 0.9676\n",
            "Epoch 143/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.2090 - acc: 0.9551\n",
            "Epoch 144/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.1745 - acc: 0.9676\n",
            "Epoch 145/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.1521 - acc: 0.9726\n",
            "Epoch 146/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.1954 - acc: 0.9551\n",
            "Epoch 147/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.2161 - acc: 0.9377\n",
            "Epoch 148/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.1707 - acc: 0.9551\n",
            "Epoch 149/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.2127 - acc: 0.9451\n",
            "Epoch 150/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.2230 - acc: 0.9352\n",
            "Epoch 151/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.1885 - acc: 0.9576\n",
            "Epoch 152/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.2192 - acc: 0.9601\n",
            "Epoch 153/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1857 - acc: 0.9601\n",
            "Epoch 154/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1903 - acc: 0.9451\n",
            "Epoch 155/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1592 - acc: 0.9551\n",
            "Epoch 156/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1875 - acc: 0.9426\n",
            "Epoch 157/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.1769 - acc: 0.9476\n",
            "Epoch 158/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.1613 - acc: 0.9601\n",
            "Epoch 159/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.1630 - acc: 0.9576\n",
            "Epoch 160/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1712 - acc: 0.9601\n",
            "Epoch 161/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.1780 - acc: 0.9601\n",
            "Epoch 162/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.1642 - acc: 0.9526\n",
            "Epoch 163/250\n",
            "401/401 [==============================] - 0s 77us/step - loss: 0.1082 - acc: 0.9701\n",
            "Epoch 164/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.2329 - acc: 0.9501\n",
            "Epoch 165/250\n",
            "401/401 [==============================] - 0s 49us/step - loss: 0.1674 - acc: 0.9551\n",
            "Epoch 166/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.1257 - acc: 0.9701\n",
            "Epoch 167/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1580 - acc: 0.9601\n",
            "Epoch 168/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1438 - acc: 0.9551\n",
            "Epoch 169/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1688 - acc: 0.9526\n",
            "Epoch 170/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.1456 - acc: 0.9476\n",
            "Epoch 171/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.1606 - acc: 0.9626\n",
            "Epoch 172/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1752 - acc: 0.9576\n",
            "Epoch 173/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1510 - acc: 0.9576\n",
            "Epoch 174/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1657 - acc: 0.9601\n",
            "Epoch 175/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1232 - acc: 0.9501\n",
            "Epoch 176/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1493 - acc: 0.9526\n",
            "Epoch 177/250\n",
            "401/401 [==============================] - 0s 50us/step - loss: 0.1363 - acc: 0.9701\n",
            "Epoch 178/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1738 - acc: 0.9426\n",
            "Epoch 179/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1407 - acc: 0.9626\n",
            "Epoch 180/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1452 - acc: 0.9751\n",
            "Epoch 181/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1546 - acc: 0.9576\n",
            "Epoch 182/250\n",
            "401/401 [==============================] - 0s 49us/step - loss: 0.1473 - acc: 0.9601\n",
            "Epoch 183/250\n",
            "401/401 [==============================] - 0s 49us/step - loss: 0.1348 - acc: 0.9626\n",
            "Epoch 184/250\n",
            "401/401 [==============================] - 0s 62us/step - loss: 0.1294 - acc: 0.9576\n",
            "Epoch 185/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.1477 - acc: 0.9526\n",
            "Epoch 186/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1433 - acc: 0.9526\n",
            "Epoch 187/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1160 - acc: 0.9551\n",
            "Epoch 188/250\n",
            "401/401 [==============================] - 0s 52us/step - loss: 0.1326 - acc: 0.9576\n",
            "Epoch 189/250\n",
            "401/401 [==============================] - 0s 52us/step - loss: 0.1046 - acc: 0.9601\n",
            "Epoch 190/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1335 - acc: 0.9601\n",
            "Epoch 191/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1307 - acc: 0.9676\n",
            "Epoch 192/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1081 - acc: 0.9701\n",
            "Epoch 193/250\n",
            "401/401 [==============================] - 0s 50us/step - loss: 0.1060 - acc: 0.9601\n",
            "Epoch 194/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1463 - acc: 0.9601\n",
            "Epoch 195/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1331 - acc: 0.9526\n",
            "Epoch 196/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1374 - acc: 0.9551\n",
            "Epoch 197/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1790 - acc: 0.9426\n",
            "Epoch 198/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1373 - acc: 0.9651\n",
            "Epoch 199/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.1501 - acc: 0.9701\n",
            "Epoch 200/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1165 - acc: 0.9626\n",
            "Epoch 201/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1309 - acc: 0.9676\n",
            "Epoch 202/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1426 - acc: 0.9551\n",
            "Epoch 203/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1507 - acc: 0.9651\n",
            "Epoch 204/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.1359 - acc: 0.9601\n",
            "Epoch 205/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1954 - acc: 0.9476\n",
            "Epoch 206/250\n",
            "401/401 [==============================] - 0s 89us/step - loss: 0.1433 - acc: 0.9676\n",
            "Epoch 207/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1430 - acc: 0.9626\n",
            "Epoch 208/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1124 - acc: 0.9651\n",
            "Epoch 209/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1444 - acc: 0.9651\n",
            "Epoch 210/250\n",
            "401/401 [==============================] - 0s 60us/step - loss: 0.1547 - acc: 0.9626\n",
            "Epoch 211/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.1311 - acc: 0.9601\n",
            "Epoch 212/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1746 - acc: 0.9576\n",
            "Epoch 213/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.0846 - acc: 0.9825\n",
            "Epoch 214/250\n",
            "401/401 [==============================] - 0s 52us/step - loss: 0.1040 - acc: 0.9751\n",
            "Epoch 215/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1526 - acc: 0.9601\n",
            "Epoch 216/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.1484 - acc: 0.9576\n",
            "Epoch 217/250\n",
            "401/401 [==============================] - 0s 67us/step - loss: 0.1187 - acc: 0.9701\n",
            "Epoch 218/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.1050 - acc: 0.9576\n",
            "Epoch 219/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.1388 - acc: 0.9476\n",
            "Epoch 220/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.1343 - acc: 0.9451\n",
            "Epoch 221/250\n",
            "401/401 [==============================] - 0s 54us/step - loss: 0.1122 - acc: 0.9601\n",
            "Epoch 222/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1286 - acc: 0.9676\n",
            "Epoch 223/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1476 - acc: 0.9501\n",
            "Epoch 224/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1207 - acc: 0.9601\n",
            "Epoch 225/250\n",
            "401/401 [==============================] - 0s 69us/step - loss: 0.1145 - acc: 0.9576\n",
            "Epoch 226/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.1207 - acc: 0.9676\n",
            "Epoch 227/250\n",
            "401/401 [==============================] - 0s 65us/step - loss: 0.1271 - acc: 0.9601\n",
            "Epoch 228/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.1260 - acc: 0.9651\n",
            "Epoch 229/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.1172 - acc: 0.9676\n",
            "Epoch 230/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.1210 - acc: 0.9576\n",
            "Epoch 231/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1151 - acc: 0.9576\n",
            "Epoch 232/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.1012 - acc: 0.9651\n",
            "Epoch 233/250\n",
            "401/401 [==============================] - 0s 55us/step - loss: 0.1104 - acc: 0.9626\n",
            "Epoch 234/250\n",
            "401/401 [==============================] - 0s 56us/step - loss: 0.1057 - acc: 0.9751\n",
            "Epoch 235/250\n",
            "401/401 [==============================] - 0s 59us/step - loss: 0.1529 - acc: 0.9501\n",
            "Epoch 236/250\n",
            "401/401 [==============================] - 0s 61us/step - loss: 0.0959 - acc: 0.9726\n",
            "Epoch 237/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.1317 - acc: 0.9601\n",
            "Epoch 238/250\n",
            "401/401 [==============================] - 0s 57us/step - loss: 0.1286 - acc: 0.9551\n",
            "Epoch 239/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.0922 - acc: 0.9676\n",
            "Epoch 240/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1008 - acc: 0.9726\n",
            "Epoch 241/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.0966 - acc: 0.9701\n",
            "Epoch 242/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.0829 - acc: 0.9776\n",
            "Epoch 243/250\n",
            "401/401 [==============================] - 0s 58us/step - loss: 0.1104 - acc: 0.9676\n",
            "Epoch 244/250\n",
            "401/401 [==============================] - 0s 66us/step - loss: 0.1161 - acc: 0.9701\n",
            "Epoch 245/250\n",
            "401/401 [==============================] - 0s 64us/step - loss: 0.1086 - acc: 0.9726\n",
            "Epoch 246/250\n",
            "401/401 [==============================] - 0s 71us/step - loss: 0.1184 - acc: 0.9651\n",
            "Epoch 247/250\n",
            "401/401 [==============================] - 0s 71us/step - loss: 0.1428 - acc: 0.9651\n",
            "Epoch 248/250\n",
            "401/401 [==============================] - 0s 53us/step - loss: 0.1267 - acc: 0.9601\n",
            "Epoch 249/250\n",
            "401/401 [==============================] - 0s 52us/step - loss: 0.1619 - acc: 0.9451\n",
            "Epoch 250/250\n",
            "401/401 [==============================] - 0s 51us/step - loss: 0.1098 - acc: 0.9651\n",
            "401/401 [==============================] - 0s 26us/step\n",
            "Test loss: 2.272811599800414\n",
            "Test accuracy: 0.688279302191556\n",
            "Test loss: 2.272811599800414\n",
            "Test accuracy: 0.688279302191556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 6/22 [00:51<02:11,  8.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.6907730673316709 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.20      0.57      0.29        14\n",
            "         1.0       0.00      0.00      0.00        18\n",
            "         2.0       0.92      0.68      0.79       249\n",
            "         3.0       0.55      0.76      0.64        63\n",
            "         4.0       0.58      0.89      0.70        57\n",
            "\n",
            "   micro avg       0.69      0.69      0.69       401\n",
            "   macro avg       0.45      0.58      0.48       401\n",
            "weighted avg       0.75      0.69      0.70       401\n",
            "\n",
            "[[  8   0   1   0   5]\n",
            " [  6   0   6   0   6]\n",
            " [ 13   0 170  40  26]\n",
            " [ 11   0   4  48   0]\n",
            " [  3   0   3   0  51]]\n",
            "[[112   0   0   0]\n",
            " [  0 137   3   7]\n",
            " [  0   0  48   0]\n",
            " [  0   0   0  94]]\n",
            "Epoch 1/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.2508 - acc: 0.9533\n",
            "Epoch 2/250\n",
            "514/514 [==============================] - 0s 45us/step - loss: 0.2082 - acc: 0.9669\n",
            "Epoch 3/250\n",
            "514/514 [==============================] - 0s 44us/step - loss: 0.2005 - acc: 0.9728\n",
            "Epoch 4/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.2327 - acc: 0.9533\n",
            "Epoch 5/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.2080 - acc: 0.9611\n",
            "Epoch 6/250\n",
            "514/514 [==============================] - 0s 46us/step - loss: 0.2296 - acc: 0.9650\n",
            "Epoch 7/250\n",
            "514/514 [==============================] - 0s 46us/step - loss: 0.1862 - acc: 0.9630\n",
            "Epoch 8/250\n",
            "514/514 [==============================] - 0s 47us/step - loss: 0.2282 - acc: 0.9591\n",
            "Epoch 9/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.1838 - acc: 0.9669\n",
            "Epoch 10/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.2321 - acc: 0.9553\n",
            "Epoch 11/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.2235 - acc: 0.9650\n",
            "Epoch 12/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1673 - acc: 0.9747\n",
            "Epoch 13/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.2295 - acc: 0.9553\n",
            "Epoch 14/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.2107 - acc: 0.9572\n",
            "Epoch 15/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.2050 - acc: 0.9611\n",
            "Epoch 16/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.2727 - acc: 0.9455\n",
            "Epoch 17/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1426 - acc: 0.9669\n",
            "Epoch 18/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1864 - acc: 0.9669\n",
            "Epoch 19/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1571 - acc: 0.9669\n",
            "Epoch 20/250\n",
            "514/514 [==============================] - 0s 75us/step - loss: 0.2210 - acc: 0.9591\n",
            "Epoch 21/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1967 - acc: 0.9553\n",
            "Epoch 22/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1611 - acc: 0.9669\n",
            "Epoch 23/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.2172 - acc: 0.9514\n",
            "Epoch 24/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1256 - acc: 0.9689\n",
            "Epoch 25/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1774 - acc: 0.9689\n",
            "Epoch 26/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.2127 - acc: 0.9630\n",
            "Epoch 27/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1418 - acc: 0.9728\n",
            "Epoch 28/250\n",
            "514/514 [==============================] - 0s 65us/step - loss: 0.1959 - acc: 0.9669\n",
            "Epoch 29/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1773 - acc: 0.9689\n",
            "Epoch 30/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.2154 - acc: 0.9630\n",
            "Epoch 31/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1739 - acc: 0.9767\n",
            "Epoch 32/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.2124 - acc: 0.9650\n",
            "Epoch 33/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1695 - acc: 0.9630\n",
            "Epoch 34/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1984 - acc: 0.9669\n",
            "Epoch 35/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.2132 - acc: 0.9553\n",
            "Epoch 36/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1775 - acc: 0.9650\n",
            "Epoch 37/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1561 - acc: 0.9689\n",
            "Epoch 38/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1672 - acc: 0.9728\n",
            "Epoch 39/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1578 - acc: 0.9669\n",
            "Epoch 40/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1425 - acc: 0.9669\n",
            "Epoch 41/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1870 - acc: 0.9689\n",
            "Epoch 42/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1662 - acc: 0.9630\n",
            "Epoch 43/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1699 - acc: 0.9669\n",
            "Epoch 44/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1434 - acc: 0.9708\n",
            "Epoch 45/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1783 - acc: 0.9669\n",
            "Epoch 46/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1689 - acc: 0.9591\n",
            "Epoch 47/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1525 - acc: 0.9767\n",
            "Epoch 48/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1473 - acc: 0.9708\n",
            "Epoch 49/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1594 - acc: 0.9747\n",
            "Epoch 50/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1694 - acc: 0.9650\n",
            "Epoch 51/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1327 - acc: 0.9708\n",
            "Epoch 52/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.1575 - acc: 0.9669\n",
            "Epoch 53/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1666 - acc: 0.9650\n",
            "Epoch 54/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1404 - acc: 0.9728\n",
            "Epoch 55/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1556 - acc: 0.9669\n",
            "Epoch 56/250\n",
            "514/514 [==============================] - 0s 70us/step - loss: 0.1291 - acc: 0.9689\n",
            "Epoch 57/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.1572 - acc: 0.9708\n",
            "Epoch 58/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1593 - acc: 0.9669\n",
            "Epoch 59/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.1986 - acc: 0.9669\n",
            "Epoch 60/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1920 - acc: 0.9591\n",
            "Epoch 61/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1473 - acc: 0.9669\n",
            "Epoch 62/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1512 - acc: 0.9689\n",
            "Epoch 63/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.1435 - acc: 0.9669\n",
            "Epoch 64/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1937 - acc: 0.9533\n",
            "Epoch 65/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1584 - acc: 0.9669\n",
            "Epoch 66/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1685 - acc: 0.9533\n",
            "Epoch 67/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1660 - acc: 0.9553\n",
            "Epoch 68/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1742 - acc: 0.9533\n",
            "Epoch 69/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1575 - acc: 0.9553\n",
            "Epoch 70/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1355 - acc: 0.9650\n",
            "Epoch 71/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1484 - acc: 0.9611\n",
            "Epoch 72/250\n",
            "514/514 [==============================] - 0s 59us/step - loss: 0.1552 - acc: 0.9650\n",
            "Epoch 73/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1539 - acc: 0.9708\n",
            "Epoch 74/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1316 - acc: 0.9689\n",
            "Epoch 75/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1831 - acc: 0.9591\n",
            "Epoch 76/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1081 - acc: 0.9767\n",
            "Epoch 77/250\n",
            "514/514 [==============================] - 0s 62us/step - loss: 0.1740 - acc: 0.9630\n",
            "Epoch 78/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1373 - acc: 0.9708\n",
            "Epoch 79/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1171 - acc: 0.9786\n",
            "Epoch 80/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1500 - acc: 0.9611\n",
            "Epoch 81/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1528 - acc: 0.9728\n",
            "Epoch 82/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1518 - acc: 0.9611\n",
            "Epoch 83/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1094 - acc: 0.9805\n",
            "Epoch 84/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1309 - acc: 0.9728\n",
            "Epoch 85/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1358 - acc: 0.9767\n",
            "Epoch 86/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1468 - acc: 0.9689\n",
            "Epoch 87/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1035 - acc: 0.9786\n",
            "Epoch 88/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1479 - acc: 0.9728\n",
            "Epoch 89/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1423 - acc: 0.9689\n",
            "Epoch 90/250\n",
            "514/514 [==============================] - 0s 67us/step - loss: 0.1374 - acc: 0.9825\n",
            "Epoch 91/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1438 - acc: 0.9650\n",
            "Epoch 92/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1231 - acc: 0.9767\n",
            "Epoch 93/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.0944 - acc: 0.9767\n",
            "Epoch 94/250\n",
            "514/514 [==============================] - 0s 46us/step - loss: 0.1233 - acc: 0.9767\n",
            "Epoch 95/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.1224 - acc: 0.9767\n",
            "Epoch 96/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1165 - acc: 0.9767\n",
            "Epoch 97/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1314 - acc: 0.9767\n",
            "Epoch 98/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1480 - acc: 0.9669\n",
            "Epoch 99/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1605 - acc: 0.9650\n",
            "Epoch 100/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1332 - acc: 0.9689\n",
            "Epoch 101/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1444 - acc: 0.9669\n",
            "Epoch 102/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1171 - acc: 0.9728\n",
            "Epoch 103/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1201 - acc: 0.9767\n",
            "Epoch 104/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1308 - acc: 0.9747\n",
            "Epoch 105/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1213 - acc: 0.9689\n",
            "Epoch 106/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1994 - acc: 0.9572\n",
            "Epoch 107/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1014 - acc: 0.9767\n",
            "Epoch 108/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1232 - acc: 0.9825\n",
            "Epoch 109/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1613 - acc: 0.9650\n",
            "Epoch 110/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1131 - acc: 0.9844\n",
            "Epoch 111/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1205 - acc: 0.9708\n",
            "Epoch 112/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1149 - acc: 0.9669\n",
            "Epoch 113/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.1211 - acc: 0.9708\n",
            "Epoch 114/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1208 - acc: 0.9708\n",
            "Epoch 115/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1292 - acc: 0.9767\n",
            "Epoch 116/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1457 - acc: 0.9689\n",
            "Epoch 117/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1327 - acc: 0.9708\n",
            "Epoch 118/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1280 - acc: 0.9786\n",
            "Epoch 119/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1175 - acc: 0.9767\n",
            "Epoch 120/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1325 - acc: 0.9708\n",
            "Epoch 121/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1576 - acc: 0.9630\n",
            "Epoch 122/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1058 - acc: 0.9747\n",
            "Epoch 123/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1425 - acc: 0.9708\n",
            "Epoch 124/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1504 - acc: 0.9708\n",
            "Epoch 125/250\n",
            "514/514 [==============================] - 0s 73us/step - loss: 0.1104 - acc: 0.9728\n",
            "Epoch 126/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1037 - acc: 0.9767\n",
            "Epoch 127/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.0870 - acc: 0.9825\n",
            "Epoch 128/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1467 - acc: 0.9630\n",
            "Epoch 129/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1220 - acc: 0.9708\n",
            "Epoch 130/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1191 - acc: 0.9786\n",
            "Epoch 131/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1141 - acc: 0.9708\n",
            "Epoch 132/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1314 - acc: 0.9747\n",
            "Epoch 133/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1298 - acc: 0.9650\n",
            "Epoch 134/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1163 - acc: 0.9708\n",
            "Epoch 135/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1074 - acc: 0.9747\n",
            "Epoch 136/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1125 - acc: 0.9767\n",
            "Epoch 137/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1241 - acc: 0.9650\n",
            "Epoch 138/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1330 - acc: 0.9572\n",
            "Epoch 139/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1250 - acc: 0.9708\n",
            "Epoch 140/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1284 - acc: 0.9708\n",
            "Epoch 141/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1138 - acc: 0.9767\n",
            "Epoch 142/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1226 - acc: 0.9708\n",
            "Epoch 143/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1293 - acc: 0.9669\n",
            "Epoch 144/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1276 - acc: 0.9708\n",
            "Epoch 145/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1339 - acc: 0.9708\n",
            "Epoch 146/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1309 - acc: 0.9728\n",
            "Epoch 147/250\n",
            "514/514 [==============================] - 0s 62us/step - loss: 0.1307 - acc: 0.9689\n",
            "Epoch 148/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1274 - acc: 0.9689\n",
            "Epoch 149/250\n",
            "514/514 [==============================] - 0s 59us/step - loss: 0.1150 - acc: 0.9805\n",
            "Epoch 150/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1478 - acc: 0.9611\n",
            "Epoch 151/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1484 - acc: 0.9591\n",
            "Epoch 152/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1190 - acc: 0.9747\n",
            "Epoch 153/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1270 - acc: 0.9669\n",
            "Epoch 154/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1256 - acc: 0.9689\n",
            "Epoch 155/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1143 - acc: 0.9747\n",
            "Epoch 156/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1117 - acc: 0.9728\n",
            "Epoch 157/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1125 - acc: 0.9708\n",
            "Epoch 158/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1565 - acc: 0.9708\n",
            "Epoch 159/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1153 - acc: 0.9747\n",
            "Epoch 160/250\n",
            "514/514 [==============================] - 0s 73us/step - loss: 0.1223 - acc: 0.9728\n",
            "Epoch 161/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1325 - acc: 0.9591\n",
            "Epoch 162/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.0862 - acc: 0.9883\n",
            "Epoch 163/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.0924 - acc: 0.9825\n",
            "Epoch 164/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1281 - acc: 0.9708\n",
            "Epoch 165/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1147 - acc: 0.9767\n",
            "Epoch 166/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1249 - acc: 0.9708\n",
            "Epoch 167/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1192 - acc: 0.9708\n",
            "Epoch 168/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1171 - acc: 0.9728\n",
            "Epoch 169/250\n",
            "514/514 [==============================] - 0s 59us/step - loss: 0.1213 - acc: 0.9689\n",
            "Epoch 170/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.0914 - acc: 0.9767\n",
            "Epoch 171/250\n",
            "514/514 [==============================] - 0s 48us/step - loss: 0.1028 - acc: 0.9786\n",
            "Epoch 172/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1238 - acc: 0.9728\n",
            "Epoch 173/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1179 - acc: 0.9767\n",
            "Epoch 174/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1162 - acc: 0.9708\n",
            "Epoch 175/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1201 - acc: 0.9747\n",
            "Epoch 176/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.0940 - acc: 0.9825\n",
            "Epoch 177/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.1061 - acc: 0.9708\n",
            "Epoch 178/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1118 - acc: 0.9669\n",
            "Epoch 179/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1483 - acc: 0.9630\n",
            "Epoch 180/250\n",
            "514/514 [==============================] - 0s 65us/step - loss: 0.1128 - acc: 0.9689\n",
            "Epoch 181/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1305 - acc: 0.9708\n",
            "Epoch 182/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1660 - acc: 0.9669\n",
            "Epoch 183/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.0939 - acc: 0.9805\n",
            "Epoch 184/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.1537 - acc: 0.9708\n",
            "Epoch 185/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1350 - acc: 0.9689\n",
            "Epoch 186/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1149 - acc: 0.9747\n",
            "Epoch 187/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.0994 - acc: 0.9805\n",
            "Epoch 188/250\n",
            "514/514 [==============================] - 0s 49us/step - loss: 0.1156 - acc: 0.9747\n",
            "Epoch 189/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1083 - acc: 0.9708\n",
            "Epoch 190/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1344 - acc: 0.9728\n",
            "Epoch 191/250\n",
            "514/514 [==============================] - 0s 62us/step - loss: 0.1198 - acc: 0.9669\n",
            "Epoch 192/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1318 - acc: 0.9708\n",
            "Epoch 193/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1264 - acc: 0.9611\n",
            "Epoch 194/250\n",
            "514/514 [==============================] - 0s 74us/step - loss: 0.0954 - acc: 0.9786\n",
            "Epoch 195/250\n",
            "514/514 [==============================] - 0s 65us/step - loss: 0.0981 - acc: 0.9747\n",
            "Epoch 196/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.1410 - acc: 0.9747\n",
            "Epoch 197/250\n",
            "514/514 [==============================] - 0s 64us/step - loss: 0.0986 - acc: 0.9708\n",
            "Epoch 198/250\n",
            "514/514 [==============================] - 0s 64us/step - loss: 0.1310 - acc: 0.9747\n",
            "Epoch 199/250\n",
            "514/514 [==============================] - 0s 50us/step - loss: 0.1256 - acc: 0.9669\n",
            "Epoch 200/250\n",
            "514/514 [==============================] - 0s 64us/step - loss: 0.1048 - acc: 0.9728\n",
            "Epoch 201/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1350 - acc: 0.9669\n",
            "Epoch 202/250\n",
            "514/514 [==============================] - 0s 68us/step - loss: 0.1154 - acc: 0.9728\n",
            "Epoch 203/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.0866 - acc: 0.9805\n",
            "Epoch 204/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.1127 - acc: 0.9747\n",
            "Epoch 205/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1112 - acc: 0.9708\n",
            "Epoch 206/250\n",
            "514/514 [==============================] - 0s 62us/step - loss: 0.0989 - acc: 0.9708\n",
            "Epoch 207/250\n",
            "514/514 [==============================] - 0s 67us/step - loss: 0.1236 - acc: 0.9728\n",
            "Epoch 208/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1281 - acc: 0.9708\n",
            "Epoch 209/250\n",
            "514/514 [==============================] - 0s 67us/step - loss: 0.0914 - acc: 0.9825\n",
            "Epoch 210/250\n",
            "514/514 [==============================] - 0s 68us/step - loss: 0.1109 - acc: 0.9689\n",
            "Epoch 211/250\n",
            "514/514 [==============================] - 0s 83us/step - loss: 0.1049 - acc: 0.9805\n",
            "Epoch 212/250\n",
            "514/514 [==============================] - 0s 59us/step - loss: 0.1491 - acc: 0.9650\n",
            "Epoch 213/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1178 - acc: 0.9728\n",
            "Epoch 214/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.0982 - acc: 0.9805\n",
            "Epoch 215/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.0981 - acc: 0.9825\n",
            "Epoch 216/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.0989 - acc: 0.9786\n",
            "Epoch 217/250\n",
            "514/514 [==============================] - 0s 67us/step - loss: 0.0960 - acc: 0.9767\n",
            "Epoch 218/250\n",
            "514/514 [==============================] - 0s 64us/step - loss: 0.0945 - acc: 0.9786\n",
            "Epoch 219/250\n",
            "514/514 [==============================] - 0s 67us/step - loss: 0.1098 - acc: 0.9767\n",
            "Epoch 220/250\n",
            "514/514 [==============================] - 0s 64us/step - loss: 0.1145 - acc: 0.9747\n",
            "Epoch 221/250\n",
            "514/514 [==============================] - 0s 61us/step - loss: 0.1565 - acc: 0.9572\n",
            "Epoch 222/250\n",
            "514/514 [==============================] - 0s 93us/step - loss: 0.1107 - acc: 0.9728\n",
            "Epoch 223/250\n",
            "514/514 [==============================] - 0s 67us/step - loss: 0.1041 - acc: 0.9805\n",
            "Epoch 224/250\n",
            "514/514 [==============================] - 0s 92us/step - loss: 0.1105 - acc: 0.9767\n",
            "Epoch 225/250\n",
            "514/514 [==============================] - 0s 87us/step - loss: 0.1421 - acc: 0.9650\n",
            "Epoch 226/250\n",
            "514/514 [==============================] - 0s 56us/step - loss: 0.0992 - acc: 0.9825\n",
            "Epoch 227/250\n",
            "514/514 [==============================] - 0s 62us/step - loss: 0.1046 - acc: 0.9767\n",
            "Epoch 228/250\n",
            "514/514 [==============================] - 0s 59us/step - loss: 0.1350 - acc: 0.9689\n",
            "Epoch 229/250\n",
            "514/514 [==============================] - 0s 64us/step - loss: 0.1238 - acc: 0.9747\n",
            "Epoch 230/250\n",
            "514/514 [==============================] - 0s 105us/step - loss: 0.0765 - acc: 0.9825\n",
            "Epoch 231/250\n",
            "514/514 [==============================] - 0s 57us/step - loss: 0.0969 - acc: 0.9786\n",
            "Epoch 232/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.0848 - acc: 0.9864\n",
            "Epoch 233/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1044 - acc: 0.9708\n",
            "Epoch 234/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.0976 - acc: 0.9786\n",
            "Epoch 235/250\n",
            "514/514 [==============================] - 0s 60us/step - loss: 0.1205 - acc: 0.9669\n",
            "Epoch 236/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.0898 - acc: 0.9805\n",
            "Epoch 237/250\n",
            "514/514 [==============================] - 0s 51us/step - loss: 0.1148 - acc: 0.9767\n",
            "Epoch 238/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1091 - acc: 0.9669\n",
            "Epoch 239/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1071 - acc: 0.9767\n",
            "Epoch 240/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.1020 - acc: 0.9767\n",
            "Epoch 241/250\n",
            "514/514 [==============================] - 0s 59us/step - loss: 0.1118 - acc: 0.9747\n",
            "Epoch 242/250\n",
            "514/514 [==============================] - 0s 58us/step - loss: 0.1095 - acc: 0.9747\n",
            "Epoch 243/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.0865 - acc: 0.9825\n",
            "Epoch 244/250\n",
            "514/514 [==============================] - 0s 55us/step - loss: 0.0939 - acc: 0.9767\n",
            "Epoch 245/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1088 - acc: 0.9689\n",
            "Epoch 246/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1103 - acc: 0.9786\n",
            "Epoch 247/250\n",
            "514/514 [==============================] - 0s 54us/step - loss: 0.1207 - acc: 0.9728\n",
            "Epoch 248/250\n",
            "514/514 [==============================] - 0s 53us/step - loss: 0.1321 - acc: 0.9708\n",
            "Epoch 249/250\n",
            "514/514 [==============================] - 0s 63us/step - loss: 0.1005 - acc: 0.9786\n",
            "Epoch 250/250\n",
            "514/514 [==============================] - 0s 52us/step - loss: 0.1132 - acc: 0.9747\n",
            "514/514 [==============================] - 0s 29us/step\n",
            "Test loss: 1.6942190790722411\n",
            "Test accuracy: 0.7198443579766537\n",
            "Test loss: 1.6942190790722411\n",
            "Test accuracy: 0.7198443579766537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 7/22 [01:00<02:06,  8.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.6498054474708171 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.83      0.71        36\n",
            "         1.0       0.00      0.00      0.00        53\n",
            "         2.0       0.96      0.62      0.75       301\n",
            "         3.0       0.49      1.00      0.65        54\n",
            "         4.0       0.39      0.90      0.55        70\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       514\n",
            "   macro avg       0.49      0.67      0.53       514\n",
            "weighted avg       0.71      0.65      0.63       514\n",
            "\n",
            "[[ 30   0   0   0   6]\n",
            " [  6   0   2   0  45]\n",
            " [ 11   0 187  57  46]\n",
            " [  0   0   0  54   0]\n",
            " [  1   0   6   0  63]]\n",
            "[[119   0   0   0   0]\n",
            " [  0   1   2   0   1]\n",
            " [  0   3 188   1   4]\n",
            " [  0   0   0  93   0]\n",
            " [  0   0   1   0 101]]\n",
            "Epoch 1/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.6355 - acc: 0.8540\n",
            "Epoch 2/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.5572 - acc: 0.8657\n",
            "Epoch 3/250\n",
            "767/767 [==============================] - 0s 50us/step - loss: 0.5062 - acc: 0.8644\n",
            "Epoch 4/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.5022 - acc: 0.8683\n",
            "Epoch 5/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.4793 - acc: 0.8879\n",
            "Epoch 6/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.4521 - acc: 0.8735\n",
            "Epoch 7/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.4379 - acc: 0.8722\n",
            "Epoch 8/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.4563 - acc: 0.8761\n",
            "Epoch 9/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.4199 - acc: 0.8801\n",
            "Epoch 10/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.4212 - acc: 0.8853\n",
            "Epoch 11/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.3540 - acc: 0.8944\n",
            "Epoch 12/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.3611 - acc: 0.8879\n",
            "Epoch 13/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.3468 - acc: 0.8905\n",
            "Epoch 14/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.3941 - acc: 0.8814\n",
            "Epoch 15/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.3194 - acc: 0.8918\n",
            "Epoch 16/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.3679 - acc: 0.8853\n",
            "Epoch 17/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.3083 - acc: 0.9035\n",
            "Epoch 18/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.3513 - acc: 0.8918\n",
            "Epoch 19/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2809 - acc: 0.9009\n",
            "Epoch 20/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2837 - acc: 0.9009\n",
            "Epoch 21/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2513 - acc: 0.9140\n",
            "Epoch 22/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2885 - acc: 0.9035\n",
            "Epoch 23/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2398 - acc: 0.9218\n",
            "Epoch 24/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2609 - acc: 0.9100\n",
            "Epoch 25/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.2802 - acc: 0.9087\n",
            "Epoch 26/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2671 - acc: 0.9100\n",
            "Epoch 27/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2700 - acc: 0.9022\n",
            "Epoch 28/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2700 - acc: 0.9166\n",
            "Epoch 29/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2732 - acc: 0.9074\n",
            "Epoch 30/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.2563 - acc: 0.9061\n",
            "Epoch 31/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2460 - acc: 0.9179\n",
            "Epoch 32/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2560 - acc: 0.9140\n",
            "Epoch 33/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.2499 - acc: 0.9231\n",
            "Epoch 34/250\n",
            "767/767 [==============================] - 0s 47us/step - loss: 0.2570 - acc: 0.9100\n",
            "Epoch 35/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2135 - acc: 0.9231\n",
            "Epoch 36/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2364 - acc: 0.9192\n",
            "Epoch 37/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2417 - acc: 0.9244\n",
            "Epoch 38/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2674 - acc: 0.9035\n",
            "Epoch 39/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.2131 - acc: 0.9179\n",
            "Epoch 40/250\n",
            "767/767 [==============================] - 0s 48us/step - loss: 0.2286 - acc: 0.9179\n",
            "Epoch 41/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.2167 - acc: 0.9244\n",
            "Epoch 42/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2716 - acc: 0.9009\n",
            "Epoch 43/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2381 - acc: 0.9035\n",
            "Epoch 44/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2241 - acc: 0.9257\n",
            "Epoch 45/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2139 - acc: 0.9218\n",
            "Epoch 46/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2360 - acc: 0.9231\n",
            "Epoch 47/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.2508 - acc: 0.9179\n",
            "Epoch 48/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2455 - acc: 0.9140\n",
            "Epoch 49/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2235 - acc: 0.9218\n",
            "Epoch 50/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2456 - acc: 0.9061\n",
            "Epoch 51/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2441 - acc: 0.9166\n",
            "Epoch 52/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2051 - acc: 0.9205\n",
            "Epoch 53/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.2196 - acc: 0.9113\n",
            "Epoch 54/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.2305 - acc: 0.9205\n",
            "Epoch 55/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2267 - acc: 0.9166\n",
            "Epoch 56/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2213 - acc: 0.9140\n",
            "Epoch 57/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2057 - acc: 0.9205\n",
            "Epoch 58/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2218 - acc: 0.9100\n",
            "Epoch 59/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2310 - acc: 0.9205\n",
            "Epoch 60/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.2104 - acc: 0.9257\n",
            "Epoch 61/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.2086 - acc: 0.9322\n",
            "Epoch 62/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2214 - acc: 0.9218\n",
            "Epoch 63/250\n",
            "767/767 [==============================] - 0s 50us/step - loss: 0.2157 - acc: 0.9218\n",
            "Epoch 64/250\n",
            "767/767 [==============================] - 0s 48us/step - loss: 0.2076 - acc: 0.9205\n",
            "Epoch 65/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1951 - acc: 0.9270\n",
            "Epoch 66/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2026 - acc: 0.9283\n",
            "Epoch 67/250\n",
            "767/767 [==============================] - 0s 47us/step - loss: 0.2178 - acc: 0.9244\n",
            "Epoch 68/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2051 - acc: 0.9205\n",
            "Epoch 69/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2058 - acc: 0.9309\n",
            "Epoch 70/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1981 - acc: 0.9205\n",
            "Epoch 71/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2277 - acc: 0.9153\n",
            "Epoch 72/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2138 - acc: 0.9296\n",
            "Epoch 73/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1896 - acc: 0.9283\n",
            "Epoch 74/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.2172 - acc: 0.9192\n",
            "Epoch 75/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1798 - acc: 0.9374\n",
            "Epoch 76/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1882 - acc: 0.9348\n",
            "Epoch 77/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1981 - acc: 0.9296\n",
            "Epoch 78/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2273 - acc: 0.9270\n",
            "Epoch 79/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2031 - acc: 0.9335\n",
            "Epoch 80/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2243 - acc: 0.9113\n",
            "Epoch 81/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2445 - acc: 0.9140\n",
            "Epoch 82/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1977 - acc: 0.9283\n",
            "Epoch 83/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1810 - acc: 0.9361\n",
            "Epoch 84/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1852 - acc: 0.9361\n",
            "Epoch 85/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2039 - acc: 0.9309\n",
            "Epoch 86/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2111 - acc: 0.9244\n",
            "Epoch 87/250\n",
            "767/767 [==============================] - 0s 48us/step - loss: 0.2146 - acc: 0.9166\n",
            "Epoch 88/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2204 - acc: 0.9244\n",
            "Epoch 89/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1825 - acc: 0.9322\n",
            "Epoch 90/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2029 - acc: 0.9231\n",
            "Epoch 91/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2191 - acc: 0.9192\n",
            "Epoch 92/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1892 - acc: 0.9322\n",
            "Epoch 93/250\n",
            "767/767 [==============================] - 0s 56us/step - loss: 0.2097 - acc: 0.9205\n",
            "Epoch 94/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2052 - acc: 0.9244\n",
            "Epoch 95/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1910 - acc: 0.9322\n",
            "Epoch 96/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1951 - acc: 0.9296\n",
            "Epoch 97/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1862 - acc: 0.9335\n",
            "Epoch 98/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1888 - acc: 0.9283\n",
            "Epoch 99/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1847 - acc: 0.9335\n",
            "Epoch 100/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1906 - acc: 0.9179\n",
            "Epoch 101/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1610 - acc: 0.9361\n",
            "Epoch 102/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.2004 - acc: 0.9244\n",
            "Epoch 103/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1851 - acc: 0.9309\n",
            "Epoch 104/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.2005 - acc: 0.9218\n",
            "Epoch 105/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1840 - acc: 0.9374\n",
            "Epoch 106/250\n",
            "767/767 [==============================] - 0s 49us/step - loss: 0.1861 - acc: 0.9361\n",
            "Epoch 107/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1915 - acc: 0.9335\n",
            "Epoch 108/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1680 - acc: 0.9374\n",
            "Epoch 109/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2056 - acc: 0.9283\n",
            "Epoch 110/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1702 - acc: 0.9439\n",
            "Epoch 111/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.2109 - acc: 0.9231\n",
            "Epoch 112/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1532 - acc: 0.9439\n",
            "Epoch 113/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1952 - acc: 0.9270\n",
            "Epoch 114/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.1760 - acc: 0.9374\n",
            "Epoch 115/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2128 - acc: 0.9192\n",
            "Epoch 116/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1846 - acc: 0.9348\n",
            "Epoch 117/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1960 - acc: 0.9205\n",
            "Epoch 118/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.1926 - acc: 0.9335\n",
            "Epoch 119/250\n",
            "767/767 [==============================] - 0s 50us/step - loss: 0.1921 - acc: 0.9374\n",
            "Epoch 120/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1608 - acc: 0.9400\n",
            "Epoch 121/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1886 - acc: 0.9244\n",
            "Epoch 122/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1766 - acc: 0.9335\n",
            "Epoch 123/250\n",
            "767/767 [==============================] - 0s 57us/step - loss: 0.1829 - acc: 0.9335\n",
            "Epoch 124/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1970 - acc: 0.9257\n",
            "Epoch 125/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1826 - acc: 0.9296\n",
            "Epoch 126/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1896 - acc: 0.9335\n",
            "Epoch 127/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.2115 - acc: 0.9113\n",
            "Epoch 128/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1766 - acc: 0.9309\n",
            "Epoch 129/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1824 - acc: 0.9361\n",
            "Epoch 130/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.1587 - acc: 0.9465\n",
            "Epoch 131/250\n",
            "767/767 [==============================] - 0s 48us/step - loss: 0.2000 - acc: 0.9244\n",
            "Epoch 132/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1558 - acc: 0.9400\n",
            "Epoch 133/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1822 - acc: 0.9309\n",
            "Epoch 134/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1647 - acc: 0.9348\n",
            "Epoch 135/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1655 - acc: 0.9439\n",
            "Epoch 136/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1733 - acc: 0.9439\n",
            "Epoch 137/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.2005 - acc: 0.9296\n",
            "Epoch 138/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1846 - acc: 0.9348\n",
            "Epoch 139/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1701 - acc: 0.9426\n",
            "Epoch 140/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1529 - acc: 0.9413\n",
            "Epoch 141/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1830 - acc: 0.9309\n",
            "Epoch 142/250\n",
            "767/767 [==============================] - 0s 62us/step - loss: 0.1630 - acc: 0.9413\n",
            "Epoch 143/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1546 - acc: 0.9452\n",
            "Epoch 144/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.2069 - acc: 0.9309\n",
            "Epoch 145/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1692 - acc: 0.9335\n",
            "Epoch 146/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1757 - acc: 0.9348\n",
            "Epoch 147/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1658 - acc: 0.9270\n",
            "Epoch 148/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.1793 - acc: 0.9270\n",
            "Epoch 149/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1946 - acc: 0.9348\n",
            "Epoch 150/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1542 - acc: 0.9426\n",
            "Epoch 151/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1775 - acc: 0.9335\n",
            "Epoch 152/250\n",
            "767/767 [==============================] - 0s 51us/step - loss: 0.1899 - acc: 0.9348\n",
            "Epoch 153/250\n",
            "767/767 [==============================] - 0s 58us/step - loss: 0.1558 - acc: 0.9400\n",
            "Epoch 154/250\n",
            "767/767 [==============================] - 0s 47us/step - loss: 0.1990 - acc: 0.9140\n",
            "Epoch 155/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1768 - acc: 0.9309\n",
            "Epoch 156/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1665 - acc: 0.9374\n",
            "Epoch 157/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.1763 - acc: 0.9309\n",
            "Epoch 158/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1634 - acc: 0.9374\n",
            "Epoch 159/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1795 - acc: 0.9309\n",
            "Epoch 160/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1675 - acc: 0.9387\n",
            "Epoch 161/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1625 - acc: 0.9348\n",
            "Epoch 162/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1963 - acc: 0.9361\n",
            "Epoch 163/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1736 - acc: 0.9361\n",
            "Epoch 164/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1687 - acc: 0.9426\n",
            "Epoch 165/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1765 - acc: 0.9283\n",
            "Epoch 166/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1544 - acc: 0.9387\n",
            "Epoch 167/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1577 - acc: 0.9426\n",
            "Epoch 168/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1814 - acc: 0.9413\n",
            "Epoch 169/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1942 - acc: 0.9374\n",
            "Epoch 170/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1805 - acc: 0.9348\n",
            "Epoch 171/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1837 - acc: 0.9361\n",
            "Epoch 172/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1719 - acc: 0.9309\n",
            "Epoch 173/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1655 - acc: 0.9348\n",
            "Epoch 174/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1754 - acc: 0.9387\n",
            "Epoch 175/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1716 - acc: 0.9296\n",
            "Epoch 176/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1813 - acc: 0.9400\n",
            "Epoch 177/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1853 - acc: 0.9283\n",
            "Epoch 178/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1611 - acc: 0.9387\n",
            "Epoch 179/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1578 - acc: 0.9387\n",
            "Epoch 180/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1312 - acc: 0.9452\n",
            "Epoch 181/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1796 - acc: 0.9361\n",
            "Epoch 182/250\n",
            "767/767 [==============================] - 0s 55us/step - loss: 0.1916 - acc: 0.9322\n",
            "Epoch 183/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1794 - acc: 0.9309\n",
            "Epoch 184/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1611 - acc: 0.9413\n",
            "Epoch 185/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1523 - acc: 0.9374\n",
            "Epoch 186/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1584 - acc: 0.9426\n",
            "Epoch 187/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1625 - acc: 0.9361\n",
            "Epoch 188/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1761 - acc: 0.9361\n",
            "Epoch 189/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1589 - acc: 0.9413\n",
            "Epoch 190/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1651 - acc: 0.9361\n",
            "Epoch 191/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1506 - acc: 0.9426\n",
            "Epoch 192/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1742 - acc: 0.9309\n",
            "Epoch 193/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1650 - acc: 0.9387\n",
            "Epoch 194/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1325 - acc: 0.9439\n",
            "Epoch 195/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1450 - acc: 0.9531\n",
            "Epoch 196/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1585 - acc: 0.9426\n",
            "Epoch 197/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1558 - acc: 0.9439\n",
            "Epoch 198/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1843 - acc: 0.9296\n",
            "Epoch 199/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.1732 - acc: 0.9335\n",
            "Epoch 200/250\n",
            "767/767 [==============================] - 0s 47us/step - loss: 0.1513 - acc: 0.9413\n",
            "Epoch 201/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1510 - acc: 0.9400\n",
            "Epoch 202/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1746 - acc: 0.9374\n",
            "Epoch 203/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1492 - acc: 0.9478\n",
            "Epoch 204/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1601 - acc: 0.9426\n",
            "Epoch 205/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.1612 - acc: 0.9426\n",
            "Epoch 206/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1715 - acc: 0.9335\n",
            "Epoch 207/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1621 - acc: 0.9426\n",
            "Epoch 208/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1525 - acc: 0.9439\n",
            "Epoch 209/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1620 - acc: 0.9439\n",
            "Epoch 210/250\n",
            "767/767 [==============================] - 0s 45us/step - loss: 0.1424 - acc: 0.9570\n",
            "Epoch 211/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1443 - acc: 0.9478\n",
            "Epoch 212/250\n",
            "767/767 [==============================] - 0s 58us/step - loss: 0.1375 - acc: 0.9531\n",
            "Epoch 213/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1794 - acc: 0.9322\n",
            "Epoch 214/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1819 - acc: 0.9374\n",
            "Epoch 215/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1473 - acc: 0.9570\n",
            "Epoch 216/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1739 - acc: 0.9270\n",
            "Epoch 217/250\n",
            "767/767 [==============================] - 0s 38us/step - loss: 0.1638 - acc: 0.9400\n",
            "Epoch 218/250\n",
            "767/767 [==============================] - 0s 42us/step - loss: 0.1641 - acc: 0.9478\n",
            "Epoch 219/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1677 - acc: 0.9492\n",
            "Epoch 220/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.1533 - acc: 0.9426\n",
            "Epoch 221/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1524 - acc: 0.9426\n",
            "Epoch 222/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1675 - acc: 0.9400\n",
            "Epoch 223/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1761 - acc: 0.9322\n",
            "Epoch 224/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1834 - acc: 0.9309\n",
            "Epoch 225/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1530 - acc: 0.9400\n",
            "Epoch 226/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1508 - acc: 0.9531\n",
            "Epoch 227/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1461 - acc: 0.9531\n",
            "Epoch 228/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1726 - acc: 0.9439\n",
            "Epoch 229/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1620 - acc: 0.9387\n",
            "Epoch 230/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1479 - acc: 0.9478\n",
            "Epoch 231/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1380 - acc: 0.9518\n",
            "Epoch 232/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1594 - acc: 0.9426\n",
            "Epoch 233/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.1389 - acc: 0.9478\n",
            "Epoch 234/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1652 - acc: 0.9387\n",
            "Epoch 235/250\n",
            "767/767 [==============================] - 0s 43us/step - loss: 0.1589 - acc: 0.9413\n",
            "Epoch 236/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1574 - acc: 0.9400\n",
            "Epoch 237/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1711 - acc: 0.9374\n",
            "Epoch 238/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1966 - acc: 0.9374\n",
            "Epoch 239/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1547 - acc: 0.9478\n",
            "Epoch 240/250\n",
            "767/767 [==============================] - 0s 37us/step - loss: 0.1751 - acc: 0.9361\n",
            "Epoch 241/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1737 - acc: 0.9322\n",
            "Epoch 242/250\n",
            "767/767 [==============================] - 0s 52us/step - loss: 0.1499 - acc: 0.9426\n",
            "Epoch 243/250\n",
            "767/767 [==============================] - 0s 46us/step - loss: 0.1734 - acc: 0.9322\n",
            "Epoch 244/250\n",
            "767/767 [==============================] - 0s 40us/step - loss: 0.1769 - acc: 0.9335\n",
            "Epoch 245/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1450 - acc: 0.9413\n",
            "Epoch 246/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1685 - acc: 0.9531\n",
            "Epoch 247/250\n",
            "767/767 [==============================] - 0s 41us/step - loss: 0.1481 - acc: 0.9439\n",
            "Epoch 248/250\n",
            "767/767 [==============================] - 0s 44us/step - loss: 0.1423 - acc: 0.9465\n",
            "Epoch 249/250\n",
            "767/767 [==============================] - 0s 39us/step - loss: 0.1607 - acc: 0.9452\n",
            "Epoch 250/250\n",
            "767/767 [==============================] - 0s 48us/step - loss: 0.1473 - acc: 0.9439\n",
            "767/767 [==============================] - 0s 21us/step\n",
            "Test loss: 2.548672474347628\n",
            "Test accuracy: 0.6140808344975289\n",
            "Test loss: 2.548672474347628\n",
            "Test accuracy: 0.6140808344975289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 8/22 [01:11<02:08,  9.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.5554106910039114 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.35      0.92      0.51        49\n",
            "         1.0       0.00      0.00      0.00        99\n",
            "         2.0       0.68      0.73      0.70       371\n",
            "         3.0       0.00      0.00      0.00       109\n",
            "         4.0       0.46      0.81      0.59       139\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       767\n",
            "   macro avg       0.30      0.49      0.36       767\n",
            "weighted avg       0.43      0.56      0.48       767\n",
            "\n",
            "[[ 45   0   1   0   3]\n",
            " [ 44   0   9   0  46]\n",
            " [ 20   0 269   1  81]\n",
            " [  2   0 107   0   0]\n",
            " [ 16   0  11   0 112]]\n",
            "[[139   0   0   0   0]\n",
            " [ 10   1   0   0   8]\n",
            " [  2   1 458   0   0]\n",
            " [  0   0   1   0   0]\n",
            " [ 38   0   0   0 109]]\n",
            "Epoch 1/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.3335 - acc: 0.9137\n",
            "Epoch 2/250\n",
            "637/637 [==============================] - 0s 52us/step - loss: 0.2837 - acc: 0.9278\n",
            "Epoch 3/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.2499 - acc: 0.9403\n",
            "Epoch 4/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.2588 - acc: 0.9325\n",
            "Epoch 5/250\n",
            "637/637 [==============================] - 0s 38us/step - loss: 0.2112 - acc: 0.9403\n",
            "Epoch 6/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1848 - acc: 0.9419\n",
            "Epoch 7/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.2109 - acc: 0.9403\n",
            "Epoch 8/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.1810 - acc: 0.9356\n",
            "Epoch 9/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1460 - acc: 0.9419\n",
            "Epoch 10/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.1499 - acc: 0.9435\n",
            "Epoch 11/250\n",
            "637/637 [==============================] - 0s 54us/step - loss: 0.1557 - acc: 0.9435\n",
            "Epoch 12/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1758 - acc: 0.9419\n",
            "Epoch 13/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1089 - acc: 0.9592\n",
            "Epoch 14/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.1267 - acc: 0.9608\n",
            "Epoch 15/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.1317 - acc: 0.9592\n",
            "Epoch 16/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.1255 - acc: 0.9498\n",
            "Epoch 17/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.1123 - acc: 0.9623\n",
            "Epoch 18/250\n",
            "637/637 [==============================] - 0s 50us/step - loss: 0.0959 - acc: 0.9655\n",
            "Epoch 19/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1306 - acc: 0.9623\n",
            "Epoch 20/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.1254 - acc: 0.9545\n",
            "Epoch 21/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1650 - acc: 0.9435\n",
            "Epoch 22/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1631 - acc: 0.9435\n",
            "Epoch 23/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.1312 - acc: 0.9513\n",
            "Epoch 24/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.1169 - acc: 0.9576\n",
            "Epoch 25/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.1273 - acc: 0.9513\n",
            "Epoch 26/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1014 - acc: 0.9639\n",
            "Epoch 27/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1078 - acc: 0.9702\n",
            "Epoch 28/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0967 - acc: 0.9686\n",
            "Epoch 29/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1119 - acc: 0.9623\n",
            "Epoch 30/250\n",
            "637/637 [==============================] - 0s 54us/step - loss: 0.1180 - acc: 0.9529\n",
            "Epoch 31/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.1429 - acc: 0.9529\n",
            "Epoch 32/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0942 - acc: 0.9749\n",
            "Epoch 33/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1222 - acc: 0.9592\n",
            "Epoch 34/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.1036 - acc: 0.9686\n",
            "Epoch 35/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0983 - acc: 0.9686\n",
            "Epoch 36/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0948 - acc: 0.9670\n",
            "Epoch 37/250\n",
            "637/637 [==============================] - 0s 64us/step - loss: 0.1314 - acc: 0.9576\n",
            "Epoch 38/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0936 - acc: 0.9655\n",
            "Epoch 39/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0953 - acc: 0.9686\n",
            "Epoch 40/250\n",
            "637/637 [==============================] - 0s 38us/step - loss: 0.1004 - acc: 0.9608\n",
            "Epoch 41/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0877 - acc: 0.9670\n",
            "Epoch 42/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1005 - acc: 0.9686\n",
            "Epoch 43/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0830 - acc: 0.9733\n",
            "Epoch 44/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0702 - acc: 0.9874\n",
            "Epoch 45/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.1058 - acc: 0.9702\n",
            "Epoch 46/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1199 - acc: 0.9513\n",
            "Epoch 47/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0943 - acc: 0.9655\n",
            "Epoch 48/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.1219 - acc: 0.9513\n",
            "Epoch 49/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.1119 - acc: 0.9639\n",
            "Epoch 50/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.1009 - acc: 0.9670\n",
            "Epoch 51/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1097 - acc: 0.9592\n",
            "Epoch 52/250\n",
            "637/637 [==============================] - 0s 50us/step - loss: 0.0820 - acc: 0.9812\n",
            "Epoch 53/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0980 - acc: 0.9670\n",
            "Epoch 54/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0950 - acc: 0.9655\n",
            "Epoch 55/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0715 - acc: 0.9796\n",
            "Epoch 56/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.1031 - acc: 0.9749\n",
            "Epoch 57/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0992 - acc: 0.9655\n",
            "Epoch 58/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1055 - acc: 0.9670\n",
            "Epoch 59/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0719 - acc: 0.9796\n",
            "Epoch 60/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0992 - acc: 0.9702\n",
            "Epoch 61/250\n",
            "637/637 [==============================] - 0s 52us/step - loss: 0.0903 - acc: 0.9702\n",
            "Epoch 62/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.1023 - acc: 0.9702\n",
            "Epoch 63/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0921 - acc: 0.9670\n",
            "Epoch 64/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0990 - acc: 0.9733\n",
            "Epoch 65/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0760 - acc: 0.9812\n",
            "Epoch 66/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0935 - acc: 0.9655\n",
            "Epoch 67/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0936 - acc: 0.9717\n",
            "Epoch 68/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0959 - acc: 0.9686\n",
            "Epoch 69/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.1100 - acc: 0.9592\n",
            "Epoch 70/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0798 - acc: 0.9702\n",
            "Epoch 71/250\n",
            "637/637 [==============================] - 0s 50us/step - loss: 0.0981 - acc: 0.9702\n",
            "Epoch 72/250\n",
            "637/637 [==============================] - 0s 54us/step - loss: 0.0839 - acc: 0.9717\n",
            "Epoch 73/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0906 - acc: 0.9733\n",
            "Epoch 74/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.1039 - acc: 0.9655\n",
            "Epoch 75/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0820 - acc: 0.9780\n",
            "Epoch 76/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0934 - acc: 0.9670\n",
            "Epoch 77/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0832 - acc: 0.9749\n",
            "Epoch 78/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0857 - acc: 0.9686\n",
            "Epoch 79/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1150 - acc: 0.9733\n",
            "Epoch 80/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0906 - acc: 0.9733\n",
            "Epoch 81/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0901 - acc: 0.9765\n",
            "Epoch 82/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0652 - acc: 0.9765\n",
            "Epoch 83/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0682 - acc: 0.9780\n",
            "Epoch 84/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0682 - acc: 0.9780\n",
            "Epoch 85/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0789 - acc: 0.9733\n",
            "Epoch 86/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0867 - acc: 0.9686\n",
            "Epoch 87/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0620 - acc: 0.9765\n",
            "Epoch 88/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0716 - acc: 0.9765\n",
            "Epoch 89/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0692 - acc: 0.9780\n",
            "Epoch 90/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0721 - acc: 0.9733\n",
            "Epoch 91/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0752 - acc: 0.9717\n",
            "Epoch 92/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0678 - acc: 0.9780\n",
            "Epoch 93/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.1049 - acc: 0.9639\n",
            "Epoch 94/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1044 - acc: 0.9686\n",
            "Epoch 95/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0787 - acc: 0.9717\n",
            "Epoch 96/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.1194 - acc: 0.9592\n",
            "Epoch 97/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0655 - acc: 0.9780\n",
            "Epoch 98/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0836 - acc: 0.9702\n",
            "Epoch 99/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0751 - acc: 0.9749\n",
            "Epoch 100/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0694 - acc: 0.9827\n",
            "Epoch 101/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0930 - acc: 0.9780\n",
            "Epoch 102/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0864 - acc: 0.9686\n",
            "Epoch 103/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0607 - acc: 0.9796\n",
            "Epoch 104/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0954 - acc: 0.9702\n",
            "Epoch 105/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0908 - acc: 0.9780\n",
            "Epoch 106/250\n",
            "637/637 [==============================] - 0s 59us/step - loss: 0.0698 - acc: 0.9796\n",
            "Epoch 107/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.1007 - acc: 0.9670\n",
            "Epoch 108/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0937 - acc: 0.9655\n",
            "Epoch 109/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0851 - acc: 0.9749\n",
            "Epoch 110/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0694 - acc: 0.9796\n",
            "Epoch 111/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0810 - acc: 0.9717\n",
            "Epoch 112/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0494 - acc: 0.9827\n",
            "Epoch 113/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.1020 - acc: 0.9686\n",
            "Epoch 114/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0912 - acc: 0.9733\n",
            "Epoch 115/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0818 - acc: 0.9702\n",
            "Epoch 116/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0466 - acc: 0.9859\n",
            "Epoch 117/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0744 - acc: 0.9780\n",
            "Epoch 118/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0758 - acc: 0.9749\n",
            "Epoch 119/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0782 - acc: 0.9765\n",
            "Epoch 120/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0733 - acc: 0.9796\n",
            "Epoch 121/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0628 - acc: 0.9827\n",
            "Epoch 122/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0703 - acc: 0.9843\n",
            "Epoch 123/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0798 - acc: 0.9796\n",
            "Epoch 124/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0950 - acc: 0.9608\n",
            "Epoch 125/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0561 - acc: 0.9796\n",
            "Epoch 126/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0657 - acc: 0.9765\n",
            "Epoch 127/250\n",
            "637/637 [==============================] - 0s 49us/step - loss: 0.0681 - acc: 0.9765\n",
            "Epoch 128/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0659 - acc: 0.9765\n",
            "Epoch 129/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0695 - acc: 0.9827\n",
            "Epoch 130/250\n",
            "637/637 [==============================] - 0s 51us/step - loss: 0.0746 - acc: 0.9670\n",
            "Epoch 131/250\n",
            "637/637 [==============================] - 0s 53us/step - loss: 0.0571 - acc: 0.9780\n",
            "Epoch 132/250\n",
            "637/637 [==============================] - 0s 54us/step - loss: 0.0844 - acc: 0.9717\n",
            "Epoch 133/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0846 - acc: 0.9686\n",
            "Epoch 134/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0660 - acc: 0.9812\n",
            "Epoch 135/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0616 - acc: 0.9780\n",
            "Epoch 136/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0593 - acc: 0.9827\n",
            "Epoch 137/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0731 - acc: 0.9733\n",
            "Epoch 138/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0709 - acc: 0.9796\n",
            "Epoch 139/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0751 - acc: 0.9765\n",
            "Epoch 140/250\n",
            "637/637 [==============================] - 0s 59us/step - loss: 0.0683 - acc: 0.9780\n",
            "Epoch 141/250\n",
            "637/637 [==============================] - 0s 57us/step - loss: 0.0828 - acc: 0.9702\n",
            "Epoch 142/250\n",
            "637/637 [==============================] - 0s 51us/step - loss: 0.0610 - acc: 0.9812\n",
            "Epoch 143/250\n",
            "637/637 [==============================] - 0s 53us/step - loss: 0.0619 - acc: 0.9780\n",
            "Epoch 144/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0880 - acc: 0.9749\n",
            "Epoch 145/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0772 - acc: 0.9717\n",
            "Epoch 146/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0677 - acc: 0.9749\n",
            "Epoch 147/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0556 - acc: 0.9827\n",
            "Epoch 148/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0640 - acc: 0.9780\n",
            "Epoch 149/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0630 - acc: 0.9812\n",
            "Epoch 150/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0690 - acc: 0.9796\n",
            "Epoch 151/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0642 - acc: 0.9765\n",
            "Epoch 152/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0560 - acc: 0.9859\n",
            "Epoch 153/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0693 - acc: 0.9796\n",
            "Epoch 154/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0851 - acc: 0.9623\n",
            "Epoch 155/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0866 - acc: 0.9733\n",
            "Epoch 156/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0651 - acc: 0.9812\n",
            "Epoch 157/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0679 - acc: 0.9812\n",
            "Epoch 158/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0525 - acc: 0.9827\n",
            "Epoch 159/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0558 - acc: 0.9827\n",
            "Epoch 160/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0626 - acc: 0.9843\n",
            "Epoch 161/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0690 - acc: 0.9749\n",
            "Epoch 162/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0579 - acc: 0.9812\n",
            "Epoch 163/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0625 - acc: 0.9780\n",
            "Epoch 164/250\n",
            "637/637 [==============================] - 0s 60us/step - loss: 0.0630 - acc: 0.9812\n",
            "Epoch 165/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0588 - acc: 0.9780\n",
            "Epoch 166/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0736 - acc: 0.9733\n",
            "Epoch 167/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0676 - acc: 0.9717\n",
            "Epoch 168/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0580 - acc: 0.9749\n",
            "Epoch 169/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0577 - acc: 0.9827\n",
            "Epoch 170/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0483 - acc: 0.9812\n",
            "Epoch 171/250\n",
            "637/637 [==============================] - 0s 50us/step - loss: 0.0653 - acc: 0.9796\n",
            "Epoch 172/250\n",
            "637/637 [==============================] - 0s 51us/step - loss: 0.0586 - acc: 0.9827\n",
            "Epoch 173/250\n",
            "637/637 [==============================] - 0s 68us/step - loss: 0.0739 - acc: 0.9717\n",
            "Epoch 174/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0595 - acc: 0.9827\n",
            "Epoch 175/250\n",
            "637/637 [==============================] - 0s 49us/step - loss: 0.0561 - acc: 0.9827\n",
            "Epoch 176/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0695 - acc: 0.9733\n",
            "Epoch 177/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0664 - acc: 0.9780\n",
            "Epoch 178/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0639 - acc: 0.9796\n",
            "Epoch 179/250\n",
            "637/637 [==============================] - 0s 52us/step - loss: 0.0528 - acc: 0.9812\n",
            "Epoch 180/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0431 - acc: 0.9922\n",
            "Epoch 181/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0603 - acc: 0.9812\n",
            "Epoch 182/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0605 - acc: 0.9812\n",
            "Epoch 183/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0777 - acc: 0.9686\n",
            "Epoch 184/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0820 - acc: 0.9717\n",
            "Epoch 185/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0498 - acc: 0.9812\n",
            "Epoch 186/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0448 - acc: 0.9890\n",
            "Epoch 187/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0529 - acc: 0.9827\n",
            "Epoch 188/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0803 - acc: 0.9749\n",
            "Epoch 189/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0623 - acc: 0.9827\n",
            "Epoch 190/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0637 - acc: 0.9812\n",
            "Epoch 191/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0516 - acc: 0.9874\n",
            "Epoch 192/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0477 - acc: 0.9874\n",
            "Epoch 193/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0862 - acc: 0.9717\n",
            "Epoch 194/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0609 - acc: 0.9796\n",
            "Epoch 195/250\n",
            "637/637 [==============================] - 0s 38us/step - loss: 0.0661 - acc: 0.9765\n",
            "Epoch 196/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0579 - acc: 0.9796\n",
            "Epoch 197/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0649 - acc: 0.9780\n",
            "Epoch 198/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0480 - acc: 0.9827\n",
            "Epoch 199/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0470 - acc: 0.9906\n",
            "Epoch 200/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0843 - acc: 0.9702\n",
            "Epoch 201/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0740 - acc: 0.9749\n",
            "Epoch 202/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0630 - acc: 0.9749\n",
            "Epoch 203/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0642 - acc: 0.9780\n",
            "Epoch 204/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0504 - acc: 0.9812\n",
            "Epoch 205/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0489 - acc: 0.9812\n",
            "Epoch 206/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0622 - acc: 0.9812\n",
            "Epoch 207/250\n",
            "637/637 [==============================] - 0s 56us/step - loss: 0.0771 - acc: 0.9765\n",
            "Epoch 208/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0635 - acc: 0.9780\n",
            "Epoch 209/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0396 - acc: 0.9874\n",
            "Epoch 210/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0638 - acc: 0.9796\n",
            "Epoch 211/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0642 - acc: 0.9796\n",
            "Epoch 212/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0501 - acc: 0.9859\n",
            "Epoch 213/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0760 - acc: 0.9717\n",
            "Epoch 214/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0382 - acc: 0.9874\n",
            "Epoch 215/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0373 - acc: 0.9890\n",
            "Epoch 216/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0609 - acc: 0.9796\n",
            "Epoch 217/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0448 - acc: 0.9890\n",
            "Epoch 218/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0733 - acc: 0.9749\n",
            "Epoch 219/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0810 - acc: 0.9749\n",
            "Epoch 220/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0425 - acc: 0.9906\n",
            "Epoch 221/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0809 - acc: 0.9717\n",
            "Epoch 222/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0405 - acc: 0.9859\n",
            "Epoch 223/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0420 - acc: 0.9890\n",
            "Epoch 224/250\n",
            "637/637 [==============================] - 0s 43us/step - loss: 0.0382 - acc: 0.9906\n",
            "Epoch 225/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0647 - acc: 0.9812\n",
            "Epoch 226/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0541 - acc: 0.9843\n",
            "Epoch 227/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0529 - acc: 0.9843\n",
            "Epoch 228/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0481 - acc: 0.9843\n",
            "Epoch 229/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0406 - acc: 0.9922\n",
            "Epoch 230/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0722 - acc: 0.9780\n",
            "Epoch 231/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0292 - acc: 0.9984\n",
            "Epoch 232/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0498 - acc: 0.9796\n",
            "Epoch 233/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0529 - acc: 0.9922\n",
            "Epoch 234/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0519 - acc: 0.9874\n",
            "Epoch 235/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0525 - acc: 0.9827\n",
            "Epoch 236/250\n",
            "637/637 [==============================] - 0s 45us/step - loss: 0.0660 - acc: 0.9796\n",
            "Epoch 237/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0461 - acc: 0.9890\n",
            "Epoch 238/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0609 - acc: 0.9796\n",
            "Epoch 239/250\n",
            "637/637 [==============================] - 0s 41us/step - loss: 0.0615 - acc: 0.9843\n",
            "Epoch 240/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0640 - acc: 0.9812\n",
            "Epoch 241/250\n",
            "637/637 [==============================] - 0s 44us/step - loss: 0.0562 - acc: 0.9812\n",
            "Epoch 242/250\n",
            "637/637 [==============================] - 0s 63us/step - loss: 0.0726 - acc: 0.9749\n",
            "Epoch 243/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0640 - acc: 0.9827\n",
            "Epoch 244/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0657 - acc: 0.9812\n",
            "Epoch 245/250\n",
            "637/637 [==============================] - 0s 46us/step - loss: 0.0605 - acc: 0.9812\n",
            "Epoch 246/250\n",
            "637/637 [==============================] - 0s 48us/step - loss: 0.0716 - acc: 0.9749\n",
            "Epoch 247/250\n",
            "637/637 [==============================] - 0s 47us/step - loss: 0.0353 - acc: 0.9874\n",
            "Epoch 248/250\n",
            "637/637 [==============================] - 0s 49us/step - loss: 0.0457 - acc: 0.9874\n",
            "Epoch 249/250\n",
            "637/637 [==============================] - 0s 40us/step - loss: 0.0701 - acc: 0.9765\n",
            "Epoch 250/250\n",
            "637/637 [==============================] - 0s 42us/step - loss: 0.0471 - acc: 0.9827\n",
            "637/637 [==============================] - 0s 25us/step\n",
            "Test loss: 2.622378416300943\n",
            "Test accuracy: 0.7174254264711772\n",
            "Test loss: 2.622378416300943\n",
            "Test accuracy: 0.7174254264711772\n",
            "Seq Test accuracy score : 0.6452119309262166 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.35      0.93      0.51        70\n",
            "         1.0       0.00      0.00      0.00        48\n",
            "         2.0       0.72      0.86      0.79       213\n",
            "         3.0       0.00      0.00      0.00        60\n",
            "         4.0       0.82      0.66      0.73       246\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       637\n",
            "   macro avg       0.38      0.49      0.41       637\n",
            "weighted avg       0.60      0.65      0.60       637\n",
            "\n",
            "[[ 65   0   2   0   3]\n",
            " [ 27   0   1   0  20]\n",
            " [ 17   0 184   0  12]\n",
            " [  0   0  60   0   0]\n",
            " [ 77   0   7   0 162]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 41%|████      | 9/22 [01:20<01:59,  9.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[306   0   0   0]\n",
            " [ 24   0   0   0]\n",
            " [  0   0 221   0]\n",
            " [  3   0   0  83]]\n",
            "Epoch 1/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 2.9197 - acc: 0.6957\n",
            "Epoch 2/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 2.3346 - acc: 0.7500\n",
            "Epoch 3/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 2.0477 - acc: 0.7826\n",
            "Epoch 4/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 1.2608 - acc: 0.8326\n",
            "Epoch 5/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.9798 - acc: 0.8609\n",
            "Epoch 6/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.6222 - acc: 0.9109\n",
            "Epoch 7/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.4737 - acc: 0.9326\n",
            "Epoch 8/250\n",
            "460/460 [==============================] - 0s 71us/step - loss: 0.3898 - acc: 0.9196\n",
            "Epoch 9/250\n",
            "460/460 [==============================] - 0s 70us/step - loss: 0.3552 - acc: 0.9435\n",
            "Epoch 10/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.2732 - acc: 0.9565\n",
            "Epoch 11/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.2720 - acc: 0.9500\n",
            "Epoch 12/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.3550 - acc: 0.9457\n",
            "Epoch 13/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.3536 - acc: 0.9348\n",
            "Epoch 14/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.2946 - acc: 0.9413\n",
            "Epoch 15/250\n",
            "460/460 [==============================] - 0s 61us/step - loss: 0.2222 - acc: 0.9565\n",
            "Epoch 16/250\n",
            "460/460 [==============================] - 0s 72us/step - loss: 0.2360 - acc: 0.9565\n",
            "Epoch 17/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.2595 - acc: 0.9543\n",
            "Epoch 18/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.2149 - acc: 0.9739\n",
            "Epoch 19/250\n",
            "460/460 [==============================] - 0s 71us/step - loss: 0.2719 - acc: 0.9609\n",
            "Epoch 20/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.2385 - acc: 0.9587\n",
            "Epoch 21/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.2332 - acc: 0.9565\n",
            "Epoch 22/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.2054 - acc: 0.9609\n",
            "Epoch 23/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1792 - acc: 0.9761\n",
            "Epoch 24/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.2170 - acc: 0.9543\n",
            "Epoch 25/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1943 - acc: 0.9696\n",
            "Epoch 26/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.1617 - acc: 0.9674\n",
            "Epoch 27/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1835 - acc: 0.9674\n",
            "Epoch 28/250\n",
            "460/460 [==============================] - 0s 61us/step - loss: 0.2090 - acc: 0.9652\n",
            "Epoch 29/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.1822 - acc: 0.9609\n",
            "Epoch 30/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.1934 - acc: 0.9717\n",
            "Epoch 31/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1586 - acc: 0.9717\n",
            "Epoch 32/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1862 - acc: 0.9587\n",
            "Epoch 33/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.1856 - acc: 0.9696\n",
            "Epoch 34/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1090 - acc: 0.9804\n",
            "Epoch 35/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1010 - acc: 0.9761\n",
            "Epoch 36/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.1532 - acc: 0.9739\n",
            "Epoch 37/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1191 - acc: 0.9739\n",
            "Epoch 38/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.1265 - acc: 0.9761\n",
            "Epoch 39/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1871 - acc: 0.9565\n",
            "Epoch 40/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.1134 - acc: 0.9783\n",
            "Epoch 41/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1209 - acc: 0.9783\n",
            "Epoch 42/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.1291 - acc: 0.9761\n",
            "Epoch 43/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1476 - acc: 0.9783\n",
            "Epoch 44/250\n",
            "460/460 [==============================] - 0s 45us/step - loss: 0.1540 - acc: 0.9696\n",
            "Epoch 45/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1175 - acc: 0.9848\n",
            "Epoch 46/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0996 - acc: 0.9848\n",
            "Epoch 47/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1399 - acc: 0.9761\n",
            "Epoch 48/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.1985 - acc: 0.9609\n",
            "Epoch 49/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.1343 - acc: 0.9696\n",
            "Epoch 50/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1153 - acc: 0.9783\n",
            "Epoch 51/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1404 - acc: 0.9761\n",
            "Epoch 52/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0804 - acc: 0.9848\n",
            "Epoch 53/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.1131 - acc: 0.9783\n",
            "Epoch 54/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1453 - acc: 0.9783\n",
            "Epoch 55/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.1403 - acc: 0.9717\n",
            "Epoch 56/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1314 - acc: 0.9761\n",
            "Epoch 57/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1273 - acc: 0.9783\n",
            "Epoch 58/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1609 - acc: 0.9696\n",
            "Epoch 59/250\n",
            "460/460 [==============================] - 0s 65us/step - loss: 0.1116 - acc: 0.9761\n",
            "Epoch 60/250\n",
            "460/460 [==============================] - 0s 68us/step - loss: 0.1241 - acc: 0.9804\n",
            "Epoch 61/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.1641 - acc: 0.9739\n",
            "Epoch 62/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.1865 - acc: 0.9652\n",
            "Epoch 63/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1526 - acc: 0.9717\n",
            "Epoch 64/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1135 - acc: 0.9739\n",
            "Epoch 65/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.1299 - acc: 0.9739\n",
            "Epoch 66/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1023 - acc: 0.9783\n",
            "Epoch 67/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.1048 - acc: 0.9826\n",
            "Epoch 68/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0905 - acc: 0.9804\n",
            "Epoch 69/250\n",
            "460/460 [==============================] - 0s 62us/step - loss: 0.0986 - acc: 0.9870\n",
            "Epoch 70/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.1595 - acc: 0.9717\n",
            "Epoch 71/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0921 - acc: 0.9804\n",
            "Epoch 72/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.1363 - acc: 0.9761\n",
            "Epoch 73/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1237 - acc: 0.9739\n",
            "Epoch 74/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.1067 - acc: 0.9783\n",
            "Epoch 75/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1270 - acc: 0.9783\n",
            "Epoch 76/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1294 - acc: 0.9783\n",
            "Epoch 77/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0826 - acc: 0.9826\n",
            "Epoch 78/250\n",
            "460/460 [==============================] - 0s 62us/step - loss: 0.1045 - acc: 0.9783\n",
            "Epoch 79/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1324 - acc: 0.9761\n",
            "Epoch 80/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.1504 - acc: 0.9674\n",
            "Epoch 81/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1137 - acc: 0.9783\n",
            "Epoch 82/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.1382 - acc: 0.9717\n",
            "Epoch 83/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.1168 - acc: 0.9783\n",
            "Epoch 84/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.1387 - acc: 0.9761\n",
            "Epoch 85/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.1094 - acc: 0.9804\n",
            "Epoch 86/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.1583 - acc: 0.9717\n",
            "Epoch 87/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1314 - acc: 0.9826\n",
            "Epoch 88/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.1144 - acc: 0.9652\n",
            "Epoch 89/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1004 - acc: 0.9826\n",
            "Epoch 90/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.1004 - acc: 0.9761\n",
            "Epoch 91/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.1375 - acc: 0.9674\n",
            "Epoch 92/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0896 - acc: 0.9826\n",
            "Epoch 93/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1095 - acc: 0.9804\n",
            "Epoch 94/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0986 - acc: 0.9848\n",
            "Epoch 95/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.1042 - acc: 0.9826\n",
            "Epoch 96/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0697 - acc: 0.9826\n",
            "Epoch 97/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0864 - acc: 0.9891\n",
            "Epoch 98/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.1287 - acc: 0.9739\n",
            "Epoch 99/250\n",
            "460/460 [==============================] - 0s 67us/step - loss: 0.1295 - acc: 0.9761\n",
            "Epoch 100/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1105 - acc: 0.9739\n",
            "Epoch 101/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.0712 - acc: 0.9783\n",
            "Epoch 102/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.0937 - acc: 0.9826\n",
            "Epoch 103/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1317 - acc: 0.9761\n",
            "Epoch 104/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0991 - acc: 0.9804\n",
            "Epoch 105/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.1239 - acc: 0.9804\n",
            "Epoch 106/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.1070 - acc: 0.9761\n",
            "Epoch 107/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.1159 - acc: 0.9848\n",
            "Epoch 108/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1403 - acc: 0.9739\n",
            "Epoch 109/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0507 - acc: 0.9870\n",
            "Epoch 110/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0895 - acc: 0.9761\n",
            "Epoch 111/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.1265 - acc: 0.9717\n",
            "Epoch 112/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0845 - acc: 0.9739\n",
            "Epoch 113/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1143 - acc: 0.9717\n",
            "Epoch 114/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0780 - acc: 0.9848\n",
            "Epoch 115/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0726 - acc: 0.9804\n",
            "Epoch 116/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0534 - acc: 0.9848\n",
            "Epoch 117/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0742 - acc: 0.9848\n",
            "Epoch 118/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0841 - acc: 0.9717\n",
            "Epoch 119/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0715 - acc: 0.9804\n",
            "Epoch 120/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0698 - acc: 0.9826\n",
            "Epoch 121/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0341 - acc: 0.9935\n",
            "Epoch 122/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0844 - acc: 0.9739\n",
            "Epoch 123/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0811 - acc: 0.9870\n",
            "Epoch 124/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0704 - acc: 0.9870\n",
            "Epoch 125/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0720 - acc: 0.9935\n",
            "Epoch 126/250\n",
            "460/460 [==============================] - 0s 65us/step - loss: 0.0785 - acc: 0.9717\n",
            "Epoch 127/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.0998 - acc: 0.9826\n",
            "Epoch 128/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0711 - acc: 0.9804\n",
            "Epoch 129/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1084 - acc: 0.9783\n",
            "Epoch 130/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0739 - acc: 0.9848\n",
            "Epoch 131/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.1195 - acc: 0.9652\n",
            "Epoch 132/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0784 - acc: 0.9804\n",
            "Epoch 133/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.0527 - acc: 0.9891\n",
            "Epoch 134/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0664 - acc: 0.9804\n",
            "Epoch 135/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0449 - acc: 0.9913\n",
            "Epoch 136/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0645 - acc: 0.9826\n",
            "Epoch 137/250\n",
            "460/460 [==============================] - 0s 46us/step - loss: 0.0705 - acc: 0.9804\n",
            "Epoch 138/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0672 - acc: 0.9826\n",
            "Epoch 139/250\n",
            "460/460 [==============================] - 0s 86us/step - loss: 0.0709 - acc: 0.9804\n",
            "Epoch 140/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0734 - acc: 0.9739\n",
            "Epoch 141/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0828 - acc: 0.9783\n",
            "Epoch 142/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.1023 - acc: 0.9826\n",
            "Epoch 143/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0681 - acc: 0.9826\n",
            "Epoch 144/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0676 - acc: 0.9848\n",
            "Epoch 145/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0882 - acc: 0.9761\n",
            "Epoch 146/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0839 - acc: 0.9761\n",
            "Epoch 147/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0478 - acc: 0.9870\n",
            "Epoch 148/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0613 - acc: 0.9870\n",
            "Epoch 149/250\n",
            "460/460 [==============================] - 0s 64us/step - loss: 0.0842 - acc: 0.9761\n",
            "Epoch 150/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0856 - acc: 0.9761\n",
            "Epoch 151/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0612 - acc: 0.9848\n",
            "Epoch 152/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0717 - acc: 0.9848\n",
            "Epoch 153/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0743 - acc: 0.9761\n",
            "Epoch 154/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.1270 - acc: 0.9674\n",
            "Epoch 155/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0612 - acc: 0.9891\n",
            "Epoch 156/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0365 - acc: 0.9935\n",
            "Epoch 157/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0427 - acc: 0.9935\n",
            "Epoch 158/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0736 - acc: 0.9783\n",
            "Epoch 159/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0906 - acc: 0.9783\n",
            "Epoch 160/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0672 - acc: 0.9804\n",
            "Epoch 161/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0576 - acc: 0.9891\n",
            "Epoch 162/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0812 - acc: 0.9826\n",
            "Epoch 163/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0559 - acc: 0.9848\n",
            "Epoch 164/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0362 - acc: 0.9913\n",
            "Epoch 165/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0604 - acc: 0.9848\n",
            "Epoch 166/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0671 - acc: 0.9804\n",
            "Epoch 167/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0552 - acc: 0.9870\n",
            "Epoch 168/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0483 - acc: 0.9870\n",
            "Epoch 169/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0787 - acc: 0.9761\n",
            "Epoch 170/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.1008 - acc: 0.9783\n",
            "Epoch 171/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0453 - acc: 0.9891\n",
            "Epoch 172/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0495 - acc: 0.9913\n",
            "Epoch 173/250\n",
            "460/460 [==============================] - 0s 58us/step - loss: 0.0388 - acc: 0.9913\n",
            "Epoch 174/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0727 - acc: 0.9826\n",
            "Epoch 175/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0449 - acc: 0.9891\n",
            "Epoch 176/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0561 - acc: 0.9870\n",
            "Epoch 177/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0927 - acc: 0.9761\n",
            "Epoch 178/250\n",
            "460/460 [==============================] - 0s 61us/step - loss: 0.0569 - acc: 0.9913\n",
            "Epoch 179/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0771 - acc: 0.9783\n",
            "Epoch 180/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0417 - acc: 0.9891\n",
            "Epoch 181/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0962 - acc: 0.9804\n",
            "Epoch 182/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0567 - acc: 0.9826\n",
            "Epoch 183/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0707 - acc: 0.9826\n",
            "Epoch 184/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0505 - acc: 0.9870\n",
            "Epoch 185/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0499 - acc: 0.9848\n",
            "Epoch 186/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0808 - acc: 0.9717\n",
            "Epoch 187/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0547 - acc: 0.9870\n",
            "Epoch 188/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0504 - acc: 0.9870\n",
            "Epoch 189/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0786 - acc: 0.9826\n",
            "Epoch 190/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0703 - acc: 0.9826\n",
            "Epoch 191/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0648 - acc: 0.9804\n",
            "Epoch 192/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.1044 - acc: 0.9674\n",
            "Epoch 193/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0471 - acc: 0.9913\n",
            "Epoch 194/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0708 - acc: 0.9804\n",
            "Epoch 195/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0346 - acc: 0.9935\n",
            "Epoch 196/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.1044 - acc: 0.9739\n",
            "Epoch 197/250\n",
            "460/460 [==============================] - 0s 66us/step - loss: 0.0467 - acc: 0.9913\n",
            "Epoch 198/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0830 - acc: 0.9783\n",
            "Epoch 199/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0678 - acc: 0.9848\n",
            "Epoch 200/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0674 - acc: 0.9804\n",
            "Epoch 201/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0530 - acc: 0.9870\n",
            "Epoch 202/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0476 - acc: 0.9848\n",
            "Epoch 203/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0588 - acc: 0.9848\n",
            "Epoch 204/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0762 - acc: 0.9783\n",
            "Epoch 205/250\n",
            "460/460 [==============================] - 0s 47us/step - loss: 0.0510 - acc: 0.9870\n",
            "Epoch 206/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0985 - acc: 0.9761\n",
            "Epoch 207/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0774 - acc: 0.9804\n",
            "Epoch 208/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0433 - acc: 0.9913\n",
            "Epoch 209/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0829 - acc: 0.9804\n",
            "Epoch 210/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0737 - acc: 0.9826\n",
            "Epoch 211/250\n",
            "460/460 [==============================] - 0s 48us/step - loss: 0.0366 - acc: 0.9913\n",
            "Epoch 212/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0420 - acc: 0.9913\n",
            "Epoch 213/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0512 - acc: 0.9848\n",
            "Epoch 214/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0677 - acc: 0.9826\n",
            "Epoch 215/250\n",
            "460/460 [==============================] - 0s 64us/step - loss: 0.0355 - acc: 0.9935\n",
            "Epoch 216/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0694 - acc: 0.9848\n",
            "Epoch 217/250\n",
            "460/460 [==============================] - 0s 51us/step - loss: 0.0733 - acc: 0.9826\n",
            "Epoch 218/250\n",
            "460/460 [==============================] - 0s 77us/step - loss: 0.0461 - acc: 0.9870\n",
            "Epoch 219/250\n",
            "460/460 [==============================] - 0s 67us/step - loss: 0.0612 - acc: 0.9848\n",
            "Epoch 220/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0554 - acc: 0.9891\n",
            "Epoch 221/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0590 - acc: 0.9870\n",
            "Epoch 222/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0851 - acc: 0.9826\n",
            "Epoch 223/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0709 - acc: 0.9826\n",
            "Epoch 224/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0570 - acc: 0.9891\n",
            "Epoch 225/250\n",
            "460/460 [==============================] - 0s 61us/step - loss: 0.0537 - acc: 0.9870\n",
            "Epoch 226/250\n",
            "460/460 [==============================] - 0s 59us/step - loss: 0.0394 - acc: 0.9870\n",
            "Epoch 227/250\n",
            "460/460 [==============================] - 0s 60us/step - loss: 0.0395 - acc: 0.9935\n",
            "Epoch 228/250\n",
            "460/460 [==============================] - 0s 54us/step - loss: 0.0548 - acc: 0.9891\n",
            "Epoch 229/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0260 - acc: 0.9978\n",
            "Epoch 230/250\n",
            "460/460 [==============================] - 0s 66us/step - loss: 0.0741 - acc: 0.9826\n",
            "Epoch 231/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0446 - acc: 0.9870\n",
            "Epoch 232/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0353 - acc: 0.9935\n",
            "Epoch 233/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0504 - acc: 0.9891\n",
            "Epoch 234/250\n",
            "460/460 [==============================] - 0s 61us/step - loss: 0.0472 - acc: 0.9913\n",
            "Epoch 235/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0409 - acc: 0.9935\n",
            "Epoch 236/250\n",
            "460/460 [==============================] - 0s 63us/step - loss: 0.0534 - acc: 0.9848\n",
            "Epoch 237/250\n",
            "460/460 [==============================] - 0s 65us/step - loss: 0.0587 - acc: 0.9891\n",
            "Epoch 238/250\n",
            "460/460 [==============================] - 0s 74us/step - loss: 0.0923 - acc: 0.9696\n",
            "Epoch 239/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0419 - acc: 0.9935\n",
            "Epoch 240/250\n",
            "460/460 [==============================] - 0s 56us/step - loss: 0.0318 - acc: 0.9935\n",
            "Epoch 241/250\n",
            "460/460 [==============================] - 0s 55us/step - loss: 0.0522 - acc: 0.9870\n",
            "Epoch 242/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0748 - acc: 0.9783\n",
            "Epoch 243/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.0714 - acc: 0.9826\n",
            "Epoch 244/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0217 - acc: 0.9957\n",
            "Epoch 245/250\n",
            "460/460 [==============================] - 0s 53us/step - loss: 0.0492 - acc: 0.9870\n",
            "Epoch 246/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0597 - acc: 0.9870\n",
            "Epoch 247/250\n",
            "460/460 [==============================] - 0s 49us/step - loss: 0.0621 - acc: 0.9848\n",
            "Epoch 248/250\n",
            "460/460 [==============================] - 0s 57us/step - loss: 0.1087 - acc: 0.9739\n",
            "Epoch 249/250\n",
            "460/460 [==============================] - 0s 52us/step - loss: 0.0617 - acc: 0.9848\n",
            "Epoch 250/250\n",
            "460/460 [==============================] - 0s 50us/step - loss: 0.0428 - acc: 0.9891\n",
            "460/460 [==============================] - 0s 29us/step\n",
            "Test loss: 2.187409797181254\n",
            "Test accuracy: 0.7239130439965621\n",
            "Test loss: 2.187409797181254\n",
            "Test accuracy: 0.7239130439965621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 10/22 [01:28<01:45,  8.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.691304347826087 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.32      0.48        62\n",
            "         1.0       0.00      0.00      0.00        40\n",
            "         2.0       0.65      0.88      0.75       128\n",
            "         3.0       0.96      0.82      0.88       139\n",
            "         4.0       0.49      0.78      0.60        91\n",
            "\n",
            "   micro avg       0.69      0.69      0.69       460\n",
            "   macro avg       0.60      0.56      0.54       460\n",
            "weighted avg       0.69      0.69      0.66       460\n",
            "\n",
            "[[ 20   0   9   1  32]\n",
            " [  0   0   8   2  30]\n",
            " [  1   0 113   2  12]\n",
            " [  1   0  24 114   0]\n",
            " [  0   0  20   0  71]]\n",
            "[[  1   0   1   0   5]\n",
            " [  0   0   1   0   3]\n",
            " [  0   0 284   0   0]\n",
            " [  0   0  24  56   0]\n",
            " [  0   0   0   0  85]]\n",
            "Epoch 1/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.1745 - acc: 0.9555\n",
            "Epoch 2/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.2079 - acc: 0.9576\n",
            "Epoch 3/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.2403 - acc: 0.9343\n",
            "Epoch 4/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.1491 - acc: 0.9576\n",
            "Epoch 5/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1612 - acc: 0.9661\n",
            "Epoch 6/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1841 - acc: 0.9492\n",
            "Epoch 7/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1925 - acc: 0.9619\n",
            "Epoch 8/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1457 - acc: 0.9703\n",
            "Epoch 9/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.1372 - acc: 0.9661\n",
            "Epoch 10/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.1065 - acc: 0.9746\n",
            "Epoch 11/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1279 - acc: 0.9597\n",
            "Epoch 12/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1407 - acc: 0.9640\n",
            "Epoch 13/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1151 - acc: 0.9703\n",
            "Epoch 14/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.1475 - acc: 0.9661\n",
            "Epoch 15/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.1532 - acc: 0.9682\n",
            "Epoch 16/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.1641 - acc: 0.9597\n",
            "Epoch 17/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1185 - acc: 0.9788\n",
            "Epoch 18/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.1515 - acc: 0.9576\n",
            "Epoch 19/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.1208 - acc: 0.9703\n",
            "Epoch 20/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.1115 - acc: 0.9831\n",
            "Epoch 21/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1185 - acc: 0.9682\n",
            "Epoch 22/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.1510 - acc: 0.9703\n",
            "Epoch 23/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1101 - acc: 0.9852\n",
            "Epoch 24/250\n",
            "472/472 [==============================] - 0s 62us/step - loss: 0.1461 - acc: 0.9619\n",
            "Epoch 25/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1007 - acc: 0.9767\n",
            "Epoch 26/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.1173 - acc: 0.9725\n",
            "Epoch 27/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.1205 - acc: 0.9703\n",
            "Epoch 28/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0998 - acc: 0.9767\n",
            "Epoch 29/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0934 - acc: 0.9831\n",
            "Epoch 30/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.0912 - acc: 0.9809\n",
            "Epoch 31/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0832 - acc: 0.9831\n",
            "Epoch 32/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1080 - acc: 0.9788\n",
            "Epoch 33/250\n",
            "472/472 [==============================] - 0s 74us/step - loss: 0.1407 - acc: 0.9725\n",
            "Epoch 34/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.1272 - acc: 0.9703\n",
            "Epoch 35/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.1458 - acc: 0.9725\n",
            "Epoch 36/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.0997 - acc: 0.9746\n",
            "Epoch 37/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.0932 - acc: 0.9767\n",
            "Epoch 38/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.1145 - acc: 0.9725\n",
            "Epoch 39/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1268 - acc: 0.9703\n",
            "Epoch 40/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0761 - acc: 0.9873\n",
            "Epoch 41/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1268 - acc: 0.9725\n",
            "Epoch 42/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.1409 - acc: 0.9682\n",
            "Epoch 43/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1161 - acc: 0.9703\n",
            "Epoch 44/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.1488 - acc: 0.9682\n",
            "Epoch 45/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1009 - acc: 0.9788\n",
            "Epoch 46/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1207 - acc: 0.9746\n",
            "Epoch 47/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.1088 - acc: 0.9767\n",
            "Epoch 48/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.1579 - acc: 0.9597\n",
            "Epoch 49/250\n",
            "472/472 [==============================] - 0s 62us/step - loss: 0.1164 - acc: 0.9725\n",
            "Epoch 50/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0958 - acc: 0.9767\n",
            "Epoch 51/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1239 - acc: 0.9725\n",
            "Epoch 52/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.1299 - acc: 0.9703\n",
            "Epoch 53/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.0907 - acc: 0.9809\n",
            "Epoch 54/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1027 - acc: 0.9746\n",
            "Epoch 55/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.1261 - acc: 0.9661\n",
            "Epoch 56/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1213 - acc: 0.9809\n",
            "Epoch 57/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0862 - acc: 0.9873\n",
            "Epoch 58/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0758 - acc: 0.9831\n",
            "Epoch 59/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.0914 - acc: 0.9788\n",
            "Epoch 60/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0920 - acc: 0.9725\n",
            "Epoch 61/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.1049 - acc: 0.9746\n",
            "Epoch 62/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1098 - acc: 0.9767\n",
            "Epoch 63/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1105 - acc: 0.9703\n",
            "Epoch 64/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.1053 - acc: 0.9703\n",
            "Epoch 65/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.1132 - acc: 0.9661\n",
            "Epoch 66/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1143 - acc: 0.9788\n",
            "Epoch 67/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0993 - acc: 0.9767\n",
            "Epoch 68/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0980 - acc: 0.9746\n",
            "Epoch 69/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0697 - acc: 0.9809\n",
            "Epoch 70/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1290 - acc: 0.9746\n",
            "Epoch 71/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0879 - acc: 0.9831\n",
            "Epoch 72/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0543 - acc: 0.9873\n",
            "Epoch 73/250\n",
            "472/472 [==============================] - 0s 72us/step - loss: 0.1324 - acc: 0.9619\n",
            "Epoch 74/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0975 - acc: 0.9746\n",
            "Epoch 75/250\n",
            "472/472 [==============================] - 0s 62us/step - loss: 0.0850 - acc: 0.9809\n",
            "Epoch 76/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0687 - acc: 0.9936\n",
            "Epoch 77/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.0968 - acc: 0.9788\n",
            "Epoch 78/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0826 - acc: 0.9831\n",
            "Epoch 79/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1013 - acc: 0.9725\n",
            "Epoch 80/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0724 - acc: 0.9873\n",
            "Epoch 81/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.1089 - acc: 0.9831\n",
            "Epoch 82/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0683 - acc: 0.9852\n",
            "Epoch 83/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0877 - acc: 0.9788\n",
            "Epoch 84/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0835 - acc: 0.9788\n",
            "Epoch 85/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1011 - acc: 0.9767\n",
            "Epoch 86/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.1154 - acc: 0.9661\n",
            "Epoch 87/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0672 - acc: 0.9852\n",
            "Epoch 88/250\n",
            "472/472 [==============================] - 0s 65us/step - loss: 0.0828 - acc: 0.9767\n",
            "Epoch 89/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0912 - acc: 0.9767\n",
            "Epoch 90/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0604 - acc: 0.9809\n",
            "Epoch 91/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0653 - acc: 0.9852\n",
            "Epoch 92/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.0768 - acc: 0.9809\n",
            "Epoch 93/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1079 - acc: 0.9788\n",
            "Epoch 94/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.1145 - acc: 0.9746\n",
            "Epoch 95/250\n",
            "472/472 [==============================] - 0s 65us/step - loss: 0.0865 - acc: 0.9831\n",
            "Epoch 96/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0852 - acc: 0.9831\n",
            "Epoch 97/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0857 - acc: 0.9852\n",
            "Epoch 98/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.0670 - acc: 0.9788\n",
            "Epoch 99/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0703 - acc: 0.9873\n",
            "Epoch 100/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0741 - acc: 0.9809\n",
            "Epoch 101/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0944 - acc: 0.9831\n",
            "Epoch 102/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0846 - acc: 0.9831\n",
            "Epoch 103/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0587 - acc: 0.9873\n",
            "Epoch 104/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0998 - acc: 0.9788\n",
            "Epoch 105/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0740 - acc: 0.9894\n",
            "Epoch 106/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.0621 - acc: 0.9873\n",
            "Epoch 107/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0903 - acc: 0.9831\n",
            "Epoch 108/250\n",
            "472/472 [==============================] - 0s 62us/step - loss: 0.0679 - acc: 0.9852\n",
            "Epoch 109/250\n",
            "472/472 [==============================] - 0s 70us/step - loss: 0.0856 - acc: 0.9894\n",
            "Epoch 110/250\n",
            "472/472 [==============================] - 0s 67us/step - loss: 0.0822 - acc: 0.9809\n",
            "Epoch 111/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0807 - acc: 0.9831\n",
            "Epoch 112/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0801 - acc: 0.9788\n",
            "Epoch 113/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.0617 - acc: 0.9915\n",
            "Epoch 114/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0727 - acc: 0.9831\n",
            "Epoch 115/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0693 - acc: 0.9831\n",
            "Epoch 116/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.1070 - acc: 0.9682\n",
            "Epoch 117/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0993 - acc: 0.9746\n",
            "Epoch 118/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0550 - acc: 0.9873\n",
            "Epoch 119/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0776 - acc: 0.9809\n",
            "Epoch 120/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0927 - acc: 0.9809\n",
            "Epoch 121/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0783 - acc: 0.9831\n",
            "Epoch 122/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0936 - acc: 0.9788\n",
            "Epoch 123/250\n",
            "472/472 [==============================] - 0s 65us/step - loss: 0.0946 - acc: 0.9809\n",
            "Epoch 124/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.1450 - acc: 0.9640\n",
            "Epoch 125/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0806 - acc: 0.9809\n",
            "Epoch 126/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0581 - acc: 0.9915\n",
            "Epoch 127/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0922 - acc: 0.9831\n",
            "Epoch 128/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0741 - acc: 0.9809\n",
            "Epoch 129/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.0738 - acc: 0.9831\n",
            "Epoch 130/250\n",
            "472/472 [==============================] - 0s 66us/step - loss: 0.0790 - acc: 0.9809\n",
            "Epoch 131/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.0760 - acc: 0.9894\n",
            "Epoch 132/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0726 - acc: 0.9852\n",
            "Epoch 133/250\n",
            "472/472 [==============================] - 0s 65us/step - loss: 0.0641 - acc: 0.9873\n",
            "Epoch 134/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.1085 - acc: 0.9767\n",
            "Epoch 135/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1185 - acc: 0.9746\n",
            "Epoch 136/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0689 - acc: 0.9894\n",
            "Epoch 137/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0626 - acc: 0.9852\n",
            "Epoch 138/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0463 - acc: 0.9915\n",
            "Epoch 139/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0746 - acc: 0.9831\n",
            "Epoch 140/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.0787 - acc: 0.9873\n",
            "Epoch 141/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0755 - acc: 0.9873\n",
            "Epoch 142/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0763 - acc: 0.9788\n",
            "Epoch 143/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.0670 - acc: 0.9873\n",
            "Epoch 144/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.0989 - acc: 0.9682\n",
            "Epoch 145/250\n",
            "472/472 [==============================] - 0s 84us/step - loss: 0.0575 - acc: 0.9894\n",
            "Epoch 146/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.0926 - acc: 0.9746\n",
            "Epoch 147/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0729 - acc: 0.9915\n",
            "Epoch 148/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.0784 - acc: 0.9831\n",
            "Epoch 149/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0643 - acc: 0.9852\n",
            "Epoch 150/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0744 - acc: 0.9852\n",
            "Epoch 151/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0829 - acc: 0.9852\n",
            "Epoch 152/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0828 - acc: 0.9831\n",
            "Epoch 153/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0795 - acc: 0.9809\n",
            "Epoch 154/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.0705 - acc: 0.9809\n",
            "Epoch 155/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0836 - acc: 0.9831\n",
            "Epoch 156/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0758 - acc: 0.9831\n",
            "Epoch 157/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0861 - acc: 0.9746\n",
            "Epoch 158/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0931 - acc: 0.9767\n",
            "Epoch 159/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0828 - acc: 0.9831\n",
            "Epoch 160/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0628 - acc: 0.9831\n",
            "Epoch 161/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.1040 - acc: 0.9746\n",
            "Epoch 162/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0914 - acc: 0.9788\n",
            "Epoch 163/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.0894 - acc: 0.9809\n",
            "Epoch 164/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0753 - acc: 0.9767\n",
            "Epoch 165/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.0695 - acc: 0.9873\n",
            "Epoch 166/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0567 - acc: 0.9852\n",
            "Epoch 167/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.1137 - acc: 0.9831\n",
            "Epoch 168/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.0780 - acc: 0.9873\n",
            "Epoch 169/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0548 - acc: 0.9915\n",
            "Epoch 170/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0667 - acc: 0.9873\n",
            "Epoch 171/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0886 - acc: 0.9852\n",
            "Epoch 172/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.0861 - acc: 0.9831\n",
            "Epoch 173/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.0789 - acc: 0.9809\n",
            "Epoch 174/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.0490 - acc: 0.9894\n",
            "Epoch 175/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.0710 - acc: 0.9809\n",
            "Epoch 176/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0477 - acc: 0.9894\n",
            "Epoch 177/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.0839 - acc: 0.9788\n",
            "Epoch 178/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.0764 - acc: 0.9809\n",
            "Epoch 179/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0859 - acc: 0.9788\n",
            "Epoch 180/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.0849 - acc: 0.9809\n",
            "Epoch 181/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0909 - acc: 0.9767\n",
            "Epoch 182/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0855 - acc: 0.9831\n",
            "Epoch 183/250\n",
            "472/472 [==============================] - 0s 85us/step - loss: 0.0536 - acc: 0.9915\n",
            "Epoch 184/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0808 - acc: 0.9788\n",
            "Epoch 185/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0709 - acc: 0.9873\n",
            "Epoch 186/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0597 - acc: 0.9894\n",
            "Epoch 187/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0553 - acc: 0.9936\n",
            "Epoch 188/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0711 - acc: 0.9767\n",
            "Epoch 189/250\n",
            "472/472 [==============================] - 0s 71us/step - loss: 0.0632 - acc: 0.9894\n",
            "Epoch 190/250\n",
            "472/472 [==============================] - 0s 64us/step - loss: 0.0568 - acc: 0.9915\n",
            "Epoch 191/250\n",
            "472/472 [==============================] - 0s 77us/step - loss: 0.0492 - acc: 0.9936\n",
            "Epoch 192/250\n",
            "472/472 [==============================] - 0s 69us/step - loss: 0.0607 - acc: 0.9873\n",
            "Epoch 193/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.1041 - acc: 0.9767\n",
            "Epoch 194/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0579 - acc: 0.9915\n",
            "Epoch 195/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0724 - acc: 0.9852\n",
            "Epoch 196/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0728 - acc: 0.9809\n",
            "Epoch 197/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.1139 - acc: 0.9767\n",
            "Epoch 198/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0837 - acc: 0.9831\n",
            "Epoch 199/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0848 - acc: 0.9831\n",
            "Epoch 200/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0630 - acc: 0.9873\n",
            "Epoch 201/250\n",
            "472/472 [==============================] - 0s 64us/step - loss: 0.0712 - acc: 0.9852\n",
            "Epoch 202/250\n",
            "472/472 [==============================] - 0s 73us/step - loss: 0.0831 - acc: 0.9831\n",
            "Epoch 203/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0486 - acc: 0.9915\n",
            "Epoch 204/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.0621 - acc: 0.9831\n",
            "Epoch 205/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0926 - acc: 0.9767\n",
            "Epoch 206/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.0584 - acc: 0.9894\n",
            "Epoch 207/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0603 - acc: 0.9852\n",
            "Epoch 208/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0850 - acc: 0.9746\n",
            "Epoch 209/250\n",
            "472/472 [==============================] - 0s 70us/step - loss: 0.0949 - acc: 0.9788\n",
            "Epoch 210/250\n",
            "472/472 [==============================] - 0s 62us/step - loss: 0.0857 - acc: 0.9809\n",
            "Epoch 211/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0949 - acc: 0.9746\n",
            "Epoch 212/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0756 - acc: 0.9831\n",
            "Epoch 213/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0510 - acc: 0.9873\n",
            "Epoch 214/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.0768 - acc: 0.9788\n",
            "Epoch 215/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0703 - acc: 0.9831\n",
            "Epoch 216/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.0595 - acc: 0.9852\n",
            "Epoch 217/250\n",
            "472/472 [==============================] - 0s 99us/step - loss: 0.0655 - acc: 0.9894\n",
            "Epoch 218/250\n",
            "472/472 [==============================] - 0s 66us/step - loss: 0.0497 - acc: 0.9894\n",
            "Epoch 219/250\n",
            "472/472 [==============================] - 0s 66us/step - loss: 0.0664 - acc: 0.9852\n",
            "Epoch 220/250\n",
            "472/472 [==============================] - 0s 73us/step - loss: 0.1006 - acc: 0.9767\n",
            "Epoch 221/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.0668 - acc: 0.9852\n",
            "Epoch 222/250\n",
            "472/472 [==============================] - 0s 64us/step - loss: 0.0843 - acc: 0.9767\n",
            "Epoch 223/250\n",
            "472/472 [==============================] - 0s 64us/step - loss: 0.0963 - acc: 0.9831\n",
            "Epoch 224/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0926 - acc: 0.9788\n",
            "Epoch 225/250\n",
            "472/472 [==============================] - 0s 68us/step - loss: 0.0739 - acc: 0.9831\n",
            "Epoch 226/250\n",
            "472/472 [==============================] - 0s 64us/step - loss: 0.0806 - acc: 0.9788\n",
            "Epoch 227/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.0909 - acc: 0.9788\n",
            "Epoch 228/250\n",
            "472/472 [==============================] - 0s 72us/step - loss: 0.0569 - acc: 0.9915\n",
            "Epoch 229/250\n",
            "472/472 [==============================] - 0s 71us/step - loss: 0.0792 - acc: 0.9852\n",
            "Epoch 230/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.0650 - acc: 0.9873\n",
            "Epoch 231/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0631 - acc: 0.9873\n",
            "Epoch 232/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.0868 - acc: 0.9809\n",
            "Epoch 233/250\n",
            "472/472 [==============================] - 0s 70us/step - loss: 0.0656 - acc: 0.9831\n",
            "Epoch 234/250\n",
            "472/472 [==============================] - 0s 70us/step - loss: 0.0573 - acc: 0.9894\n",
            "Epoch 235/250\n",
            "472/472 [==============================] - 0s 77us/step - loss: 0.0635 - acc: 0.9894\n",
            "Epoch 236/250\n",
            "472/472 [==============================] - 0s 77us/step - loss: 0.0495 - acc: 0.9936\n",
            "Epoch 237/250\n",
            "472/472 [==============================] - 0s 71us/step - loss: 0.1031 - acc: 0.9788\n",
            "Epoch 238/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.0691 - acc: 0.9831\n",
            "Epoch 239/250\n",
            "472/472 [==============================] - 0s 64us/step - loss: 0.0757 - acc: 0.9831\n",
            "Epoch 240/250\n",
            "472/472 [==============================] - 0s 74us/step - loss: 0.0778 - acc: 0.9873\n",
            "Epoch 241/250\n",
            "472/472 [==============================] - 0s 71us/step - loss: 0.0591 - acc: 0.9852\n",
            "Epoch 242/250\n",
            "472/472 [==============================] - 0s 77us/step - loss: 0.0654 - acc: 0.9894\n",
            "Epoch 243/250\n",
            "472/472 [==============================] - 0s 66us/step - loss: 0.0636 - acc: 0.9894\n",
            "Epoch 244/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.0776 - acc: 0.9852\n",
            "Epoch 245/250\n",
            "472/472 [==============================] - 0s 67us/step - loss: 0.0511 - acc: 0.9873\n",
            "Epoch 246/250\n",
            "472/472 [==============================] - 0s 67us/step - loss: 0.0800 - acc: 0.9873\n",
            "Epoch 247/250\n",
            "472/472 [==============================] - 0s 73us/step - loss: 0.0629 - acc: 0.9894\n",
            "Epoch 248/250\n",
            "472/472 [==============================] - 0s 78us/step - loss: 0.0559 - acc: 0.9936\n",
            "Epoch 249/250\n",
            "472/472 [==============================] - 0s 67us/step - loss: 0.1012 - acc: 0.9746\n",
            "Epoch 250/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.0647 - acc: 0.9852\n",
            "472/472 [==============================] - 0s 28us/step\n",
            "Test loss: 2.4827123607619335\n",
            "Test accuracy: 0.6673728843866769\n",
            "Test loss: 2.4827123607619335\n",
            "Test accuracy: 0.6673728843866769\n",
            "Seq Test accuracy score : 0.5021186440677966 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 11/22 [01:36<01:35,  8.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.35      0.48        62\n",
            "         1.0       0.00      0.00      0.00        66\n",
            "         2.0       0.46      0.81      0.59       130\n",
            "         3.0       0.88      0.43      0.58       172\n",
            "         4.0       0.27      0.86      0.41        42\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       472\n",
            "   macro avg       0.47      0.49      0.41       472\n",
            "weighted avg       0.57      0.50      0.47       472\n",
            "\n",
            "[[ 22   0   6   5  29]\n",
            " [  4   0  11   1  50]\n",
            " [  4   0 105   4  17]\n",
            " [  0   0  98  74   0]\n",
            " [  0   0   6   0  36]]\n",
            "[[ 10   0   1   1   0]\n",
            " [  0   1   1   0   6]\n",
            " [  0   0 223   0   0]\n",
            " [  0   0  50  62   0]\n",
            " [  0   0   0   0 117]]\n",
            "Epoch 1/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.3202 - acc: 0.9312\n",
            "Epoch 2/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.3660 - acc: 0.9194\n",
            "Epoch 3/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.3277 - acc: 0.9391\n",
            "Epoch 4/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.2959 - acc: 0.9391\n",
            "Epoch 5/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.2990 - acc: 0.9293\n",
            "Epoch 6/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.2700 - acc: 0.9450\n",
            "Epoch 7/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.2718 - acc: 0.9371\n",
            "Epoch 8/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.2440 - acc: 0.9273\n",
            "Epoch 9/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.2088 - acc: 0.9411\n",
            "Epoch 10/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1520 - acc: 0.9509\n",
            "Epoch 11/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.1547 - acc: 0.9509\n",
            "Epoch 12/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1776 - acc: 0.9607\n",
            "Epoch 13/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1776 - acc: 0.9470\n",
            "Epoch 14/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1719 - acc: 0.9548\n",
            "Epoch 15/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1980 - acc: 0.9371\n",
            "Epoch 16/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.2041 - acc: 0.9293\n",
            "Epoch 17/250\n",
            "509/509 [==============================] - 0s 68us/step - loss: 0.1986 - acc: 0.9371\n",
            "Epoch 18/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.1468 - acc: 0.9627\n",
            "Epoch 19/250\n",
            "509/509 [==============================] - 0s 46us/step - loss: 0.1742 - acc: 0.9607\n",
            "Epoch 20/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1820 - acc: 0.9470\n",
            "Epoch 21/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.1610 - acc: 0.9509\n",
            "Epoch 22/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1609 - acc: 0.9646\n",
            "Epoch 23/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1736 - acc: 0.9548\n",
            "Epoch 24/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1556 - acc: 0.9509\n",
            "Epoch 25/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1605 - acc: 0.9509\n",
            "Epoch 26/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1515 - acc: 0.9568\n",
            "Epoch 27/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1696 - acc: 0.9548\n",
            "Epoch 28/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.1540 - acc: 0.9528\n",
            "Epoch 29/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1806 - acc: 0.9470\n",
            "Epoch 30/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1317 - acc: 0.9627\n",
            "Epoch 31/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.1714 - acc: 0.9391\n",
            "Epoch 32/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.1538 - acc: 0.9528\n",
            "Epoch 33/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1456 - acc: 0.9528\n",
            "Epoch 34/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.1568 - acc: 0.9489\n",
            "Epoch 35/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.1431 - acc: 0.9568\n",
            "Epoch 36/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1428 - acc: 0.9646\n",
            "Epoch 37/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.1730 - acc: 0.9470\n",
            "Epoch 38/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1584 - acc: 0.9627\n",
            "Epoch 39/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1581 - acc: 0.9548\n",
            "Epoch 40/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.1089 - acc: 0.9627\n",
            "Epoch 41/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1206 - acc: 0.9705\n",
            "Epoch 42/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1166 - acc: 0.9646\n",
            "Epoch 43/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.1473 - acc: 0.9509\n",
            "Epoch 44/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1455 - acc: 0.9587\n",
            "Epoch 45/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.1532 - acc: 0.9528\n",
            "Epoch 46/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.1291 - acc: 0.9568\n",
            "Epoch 47/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.1165 - acc: 0.9705\n",
            "Epoch 48/250\n",
            "509/509 [==============================] - 0s 61us/step - loss: 0.1343 - acc: 0.9528\n",
            "Epoch 49/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1506 - acc: 0.9646\n",
            "Epoch 50/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.1406 - acc: 0.9548\n",
            "Epoch 51/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.1160 - acc: 0.9568\n",
            "Epoch 52/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.1288 - acc: 0.9607\n",
            "Epoch 53/250\n",
            "509/509 [==============================] - 0s 65us/step - loss: 0.1059 - acc: 0.9725\n",
            "Epoch 54/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1448 - acc: 0.9607\n",
            "Epoch 55/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.1242 - acc: 0.9568\n",
            "Epoch 56/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.1176 - acc: 0.9725\n",
            "Epoch 57/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.1120 - acc: 0.9686\n",
            "Epoch 58/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1297 - acc: 0.9587\n",
            "Epoch 59/250\n",
            "509/509 [==============================] - 0s 82us/step - loss: 0.0765 - acc: 0.9764\n",
            "Epoch 60/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1358 - acc: 0.9646\n",
            "Epoch 61/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1540 - acc: 0.9528\n",
            "Epoch 62/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1068 - acc: 0.9705\n",
            "Epoch 63/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1427 - acc: 0.9587\n",
            "Epoch 64/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1438 - acc: 0.9666\n",
            "Epoch 65/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.1178 - acc: 0.9607\n",
            "Epoch 66/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0786 - acc: 0.9862\n",
            "Epoch 67/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1015 - acc: 0.9666\n",
            "Epoch 68/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1138 - acc: 0.9646\n",
            "Epoch 69/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0992 - acc: 0.9725\n",
            "Epoch 70/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0960 - acc: 0.9745\n",
            "Epoch 71/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0885 - acc: 0.9784\n",
            "Epoch 72/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.0802 - acc: 0.9784\n",
            "Epoch 73/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1088 - acc: 0.9666\n",
            "Epoch 74/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.0901 - acc: 0.9686\n",
            "Epoch 75/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0856 - acc: 0.9686\n",
            "Epoch 76/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0957 - acc: 0.9666\n",
            "Epoch 77/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1249 - acc: 0.9587\n",
            "Epoch 78/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1208 - acc: 0.9548\n",
            "Epoch 79/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.1344 - acc: 0.9548\n",
            "Epoch 80/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1100 - acc: 0.9646\n",
            "Epoch 81/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0894 - acc: 0.9784\n",
            "Epoch 82/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0818 - acc: 0.9784\n",
            "Epoch 83/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1073 - acc: 0.9627\n",
            "Epoch 84/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1153 - acc: 0.9725\n",
            "Epoch 85/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0870 - acc: 0.9745\n",
            "Epoch 86/250\n",
            "509/509 [==============================] - 0s 62us/step - loss: 0.1293 - acc: 0.9607\n",
            "Epoch 87/250\n",
            "509/509 [==============================] - 0s 74us/step - loss: 0.0823 - acc: 0.9745\n",
            "Epoch 88/250\n",
            "509/509 [==============================] - 0s 70us/step - loss: 0.0757 - acc: 0.9804\n",
            "Epoch 89/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0758 - acc: 0.9745\n",
            "Epoch 90/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0780 - acc: 0.9784\n",
            "Epoch 91/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.1319 - acc: 0.9627\n",
            "Epoch 92/250\n",
            "509/509 [==============================] - 0s 42us/step - loss: 0.1234 - acc: 0.9705\n",
            "Epoch 93/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0706 - acc: 0.9804\n",
            "Epoch 94/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1177 - acc: 0.9666\n",
            "Epoch 95/250\n",
            "509/509 [==============================] - 0s 65us/step - loss: 0.0805 - acc: 0.9725\n",
            "Epoch 96/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0806 - acc: 0.9784\n",
            "Epoch 97/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1089 - acc: 0.9646\n",
            "Epoch 98/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0998 - acc: 0.9686\n",
            "Epoch 99/250\n",
            "509/509 [==============================] - 0s 44us/step - loss: 0.0778 - acc: 0.9646\n",
            "Epoch 100/250\n",
            "509/509 [==============================] - 0s 47us/step - loss: 0.1128 - acc: 0.9666\n",
            "Epoch 101/250\n",
            "509/509 [==============================] - 0s 44us/step - loss: 0.1071 - acc: 0.9607\n",
            "Epoch 102/250\n",
            "509/509 [==============================] - 0s 65us/step - loss: 0.0925 - acc: 0.9705\n",
            "Epoch 103/250\n",
            "509/509 [==============================] - 0s 62us/step - loss: 0.0921 - acc: 0.9705\n",
            "Epoch 104/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.1050 - acc: 0.9627\n",
            "Epoch 105/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.1088 - acc: 0.9725\n",
            "Epoch 106/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.1162 - acc: 0.9686\n",
            "Epoch 107/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1312 - acc: 0.9646\n",
            "Epoch 108/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0708 - acc: 0.9804\n",
            "Epoch 109/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1269 - acc: 0.9627\n",
            "Epoch 110/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1255 - acc: 0.9568\n",
            "Epoch 111/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0614 - acc: 0.9862\n",
            "Epoch 112/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1050 - acc: 0.9646\n",
            "Epoch 113/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1160 - acc: 0.9646\n",
            "Epoch 114/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.0930 - acc: 0.9705\n",
            "Epoch 115/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0737 - acc: 0.9745\n",
            "Epoch 116/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0976 - acc: 0.9666\n",
            "Epoch 117/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0670 - acc: 0.9823\n",
            "Epoch 118/250\n",
            "509/509 [==============================] - 0s 64us/step - loss: 0.1100 - acc: 0.9666\n",
            "Epoch 119/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0751 - acc: 0.9745\n",
            "Epoch 120/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1204 - acc: 0.9627\n",
            "Epoch 121/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1004 - acc: 0.9607\n",
            "Epoch 122/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.1158 - acc: 0.9666\n",
            "Epoch 123/250\n",
            "509/509 [==============================] - 0s 81us/step - loss: 0.0791 - acc: 0.9804\n",
            "Epoch 124/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0789 - acc: 0.9804\n",
            "Epoch 125/250\n",
            "509/509 [==============================] - 0s 47us/step - loss: 0.0886 - acc: 0.9686\n",
            "Epoch 126/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0731 - acc: 0.9745\n",
            "Epoch 127/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.1072 - acc: 0.9666\n",
            "Epoch 128/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0822 - acc: 0.9804\n",
            "Epoch 129/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0887 - acc: 0.9646\n",
            "Epoch 130/250\n",
            "509/509 [==============================] - 0s 44us/step - loss: 0.0491 - acc: 0.9843\n",
            "Epoch 131/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0692 - acc: 0.9764\n",
            "Epoch 132/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0715 - acc: 0.9745\n",
            "Epoch 133/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0787 - acc: 0.9784\n",
            "Epoch 134/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0700 - acc: 0.9764\n",
            "Epoch 135/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0711 - acc: 0.9804\n",
            "Epoch 136/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.1098 - acc: 0.9646\n",
            "Epoch 137/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0641 - acc: 0.9784\n",
            "Epoch 138/250\n",
            "509/509 [==============================] - 0s 47us/step - loss: 0.0771 - acc: 0.9745\n",
            "Epoch 139/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0772 - acc: 0.9745\n",
            "Epoch 140/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0858 - acc: 0.9686\n",
            "Epoch 141/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0940 - acc: 0.9705\n",
            "Epoch 142/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0745 - acc: 0.9784\n",
            "Epoch 143/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0741 - acc: 0.9764\n",
            "Epoch 144/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0705 - acc: 0.9764\n",
            "Epoch 145/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.1022 - acc: 0.9686\n",
            "Epoch 146/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0643 - acc: 0.9862\n",
            "Epoch 147/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.1232 - acc: 0.9646\n",
            "Epoch 148/250\n",
            "509/509 [==============================] - 0s 44us/step - loss: 0.0828 - acc: 0.9823\n",
            "Epoch 149/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0656 - acc: 0.9862\n",
            "Epoch 150/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0651 - acc: 0.9784\n",
            "Epoch 151/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0628 - acc: 0.9823\n",
            "Epoch 152/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0694 - acc: 0.9804\n",
            "Epoch 153/250\n",
            "509/509 [==============================] - 0s 46us/step - loss: 0.0695 - acc: 0.9784\n",
            "Epoch 154/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0536 - acc: 0.9843\n",
            "Epoch 155/250\n",
            "509/509 [==============================] - 0s 46us/step - loss: 0.0799 - acc: 0.9764\n",
            "Epoch 156/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.1073 - acc: 0.9705\n",
            "Epoch 157/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0604 - acc: 0.9804\n",
            "Epoch 158/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0817 - acc: 0.9764\n",
            "Epoch 159/250\n",
            "509/509 [==============================] - 0s 65us/step - loss: 0.0746 - acc: 0.9745\n",
            "Epoch 160/250\n",
            "509/509 [==============================] - 0s 68us/step - loss: 0.0848 - acc: 0.9666\n",
            "Epoch 161/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0793 - acc: 0.9725\n",
            "Epoch 162/250\n",
            "509/509 [==============================] - 0s 46us/step - loss: 0.0787 - acc: 0.9725\n",
            "Epoch 163/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0624 - acc: 0.9764\n",
            "Epoch 164/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0827 - acc: 0.9745\n",
            "Epoch 165/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0800 - acc: 0.9764\n",
            "Epoch 166/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0802 - acc: 0.9745\n",
            "Epoch 167/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0614 - acc: 0.9843\n",
            "Epoch 168/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0691 - acc: 0.9804\n",
            "Epoch 169/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1307 - acc: 0.9666\n",
            "Epoch 170/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0887 - acc: 0.9705\n",
            "Epoch 171/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0812 - acc: 0.9725\n",
            "Epoch 172/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0657 - acc: 0.9823\n",
            "Epoch 173/250\n",
            "509/509 [==============================] - 0s 53us/step - loss: 0.1047 - acc: 0.9725\n",
            "Epoch 174/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0554 - acc: 0.9823\n",
            "Epoch 175/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0985 - acc: 0.9705\n",
            "Epoch 176/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.1117 - acc: 0.9646\n",
            "Epoch 177/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0639 - acc: 0.9823\n",
            "Epoch 178/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0536 - acc: 0.9882\n",
            "Epoch 179/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0809 - acc: 0.9764\n",
            "Epoch 180/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0773 - acc: 0.9764\n",
            "Epoch 181/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0893 - acc: 0.9725\n",
            "Epoch 182/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0739 - acc: 0.9764\n",
            "Epoch 183/250\n",
            "509/509 [==============================] - 0s 46us/step - loss: 0.0673 - acc: 0.9764\n",
            "Epoch 184/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0785 - acc: 0.9705\n",
            "Epoch 185/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.1197 - acc: 0.9686\n",
            "Epoch 186/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0798 - acc: 0.9745\n",
            "Epoch 187/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0875 - acc: 0.9725\n",
            "Epoch 188/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0692 - acc: 0.9725\n",
            "Epoch 189/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.0342 - acc: 0.9941\n",
            "Epoch 190/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0673 - acc: 0.9784\n",
            "Epoch 191/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0703 - acc: 0.9804\n",
            "Epoch 192/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0722 - acc: 0.9804\n",
            "Epoch 193/250\n",
            "509/509 [==============================] - 0s 65us/step - loss: 0.0878 - acc: 0.9705\n",
            "Epoch 194/250\n",
            "509/509 [==============================] - 0s 70us/step - loss: 0.0867 - acc: 0.9745\n",
            "Epoch 195/250\n",
            "509/509 [==============================] - 0s 67us/step - loss: 0.0543 - acc: 0.9804\n",
            "Epoch 196/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0649 - acc: 0.9804\n",
            "Epoch 197/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0986 - acc: 0.9686\n",
            "Epoch 198/250\n",
            "509/509 [==============================] - 0s 47us/step - loss: 0.0835 - acc: 0.9705\n",
            "Epoch 199/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0623 - acc: 0.9843\n",
            "Epoch 200/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0595 - acc: 0.9862\n",
            "Epoch 201/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0891 - acc: 0.9646\n",
            "Epoch 202/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0551 - acc: 0.9843\n",
            "Epoch 203/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0669 - acc: 0.9823\n",
            "Epoch 204/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0731 - acc: 0.9705\n",
            "Epoch 205/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0918 - acc: 0.9725\n",
            "Epoch 206/250\n",
            "509/509 [==============================] - 0s 45us/step - loss: 0.0777 - acc: 0.9745\n",
            "Epoch 207/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0534 - acc: 0.9862\n",
            "Epoch 208/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.0813 - acc: 0.9745\n",
            "Epoch 209/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0709 - acc: 0.9764\n",
            "Epoch 210/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0881 - acc: 0.9705\n",
            "Epoch 211/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.1019 - acc: 0.9686\n",
            "Epoch 212/250\n",
            "509/509 [==============================] - 0s 61us/step - loss: 0.0841 - acc: 0.9745\n",
            "Epoch 213/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0707 - acc: 0.9784\n",
            "Epoch 214/250\n",
            "509/509 [==============================] - 0s 48us/step - loss: 0.0518 - acc: 0.9843\n",
            "Epoch 215/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.0825 - acc: 0.9745\n",
            "Epoch 216/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0841 - acc: 0.9725\n",
            "Epoch 217/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0699 - acc: 0.9862\n",
            "Epoch 218/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0502 - acc: 0.9862\n",
            "Epoch 219/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.0659 - acc: 0.9784\n",
            "Epoch 220/250\n",
            "509/509 [==============================] - 0s 47us/step - loss: 0.0515 - acc: 0.9882\n",
            "Epoch 221/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0540 - acc: 0.9784\n",
            "Epoch 222/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0843 - acc: 0.9705\n",
            "Epoch 223/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0567 - acc: 0.9843\n",
            "Epoch 224/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0748 - acc: 0.9804\n",
            "Epoch 225/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0439 - acc: 0.9862\n",
            "Epoch 226/250\n",
            "509/509 [==============================] - 0s 58us/step - loss: 0.0641 - acc: 0.9764\n",
            "Epoch 227/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0789 - acc: 0.9725\n",
            "Epoch 228/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0573 - acc: 0.9843\n",
            "Epoch 229/250\n",
            "509/509 [==============================] - 0s 66us/step - loss: 0.0804 - acc: 0.9823\n",
            "Epoch 230/250\n",
            "509/509 [==============================] - 0s 70us/step - loss: 0.0935 - acc: 0.9666\n",
            "Epoch 231/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0542 - acc: 0.9882\n",
            "Epoch 232/250\n",
            "509/509 [==============================] - 0s 55us/step - loss: 0.0779 - acc: 0.9804\n",
            "Epoch 233/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0540 - acc: 0.9823\n",
            "Epoch 234/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0587 - acc: 0.9823\n",
            "Epoch 235/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0553 - acc: 0.9862\n",
            "Epoch 236/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0829 - acc: 0.9745\n",
            "Epoch 237/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0610 - acc: 0.9823\n",
            "Epoch 238/250\n",
            "509/509 [==============================] - 0s 57us/step - loss: 0.0954 - acc: 0.9686\n",
            "Epoch 239/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.0705 - acc: 0.9725\n",
            "Epoch 240/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0705 - acc: 0.9843\n",
            "Epoch 241/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0766 - acc: 0.9764\n",
            "Epoch 242/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0616 - acc: 0.9804\n",
            "Epoch 243/250\n",
            "509/509 [==============================] - 0s 56us/step - loss: 0.0851 - acc: 0.9745\n",
            "Epoch 244/250\n",
            "509/509 [==============================] - 0s 52us/step - loss: 0.0342 - acc: 0.9902\n",
            "Epoch 245/250\n",
            "509/509 [==============================] - 0s 59us/step - loss: 0.0546 - acc: 0.9843\n",
            "Epoch 246/250\n",
            "509/509 [==============================] - 0s 60us/step - loss: 0.0572 - acc: 0.9804\n",
            "Epoch 247/250\n",
            "509/509 [==============================] - 0s 51us/step - loss: 0.0755 - acc: 0.9804\n",
            "Epoch 248/250\n",
            "509/509 [==============================] - 0s 49us/step - loss: 0.0778 - acc: 0.9705\n",
            "Epoch 249/250\n",
            "509/509 [==============================] - 0s 50us/step - loss: 0.0584 - acc: 0.9823\n",
            "Epoch 250/250\n",
            "509/509 [==============================] - 0s 54us/step - loss: 0.0632 - acc: 0.9764\n",
            "509/509 [==============================] - 0s 26us/step\n",
            "Test loss: 1.8522267669497867\n",
            "Test accuracy: 0.6561886040541418\n",
            "Test loss: 1.8522267669497867\n",
            "Test accuracy: 0.6561886040541418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 55%|█████▍    | 12/22 [01:45<01:27,  8.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.618860510805501 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.66      0.62        38\n",
            "         1.0       0.00      0.00      0.00        75\n",
            "         2.0       0.47      0.88      0.62       119\n",
            "         3.0       1.00      0.68      0.81       222\n",
            "         4.0       0.35      0.60      0.45        55\n",
            "\n",
            "   micro avg       0.62      0.62      0.62       509\n",
            "   macro avg       0.48      0.56      0.50       509\n",
            "weighted avg       0.63      0.62      0.59       509\n",
            "\n",
            "[[ 25   0   5   0   8]\n",
            " [ 15   0  20   0  40]\n",
            " [  2   0 105   0  12]\n",
            " [  0   0  70 152   0]\n",
            " [  0   0  22   0  33]]\n",
            "[[ 36   0   0   0   0]\n",
            " [  0   0   9   0  19]\n",
            " [  0   0 245   0   4]\n",
            " [  0   0  11  15   0]\n",
            " [  0   0   7   0 163]]\n",
            "Epoch 1/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.3923 - acc: 0.8936\n",
            "Epoch 2/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.3629 - acc: 0.8994\n",
            "Epoch 3/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.2959 - acc: 0.9188\n",
            "Epoch 4/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.2466 - acc: 0.9284\n",
            "Epoch 5/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.2421 - acc: 0.9381\n",
            "Epoch 6/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.2462 - acc: 0.9362\n",
            "Epoch 7/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.2188 - acc: 0.9439\n",
            "Epoch 8/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.2123 - acc: 0.9497\n",
            "Epoch 9/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1790 - acc: 0.9439\n",
            "Epoch 10/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.2043 - acc: 0.9439\n",
            "Epoch 11/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.2050 - acc: 0.9439\n",
            "Epoch 12/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1981 - acc: 0.9478\n",
            "Epoch 13/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1716 - acc: 0.9574\n",
            "Epoch 14/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1624 - acc: 0.9536\n",
            "Epoch 15/250\n",
            "517/517 [==============================] - 0s 51us/step - loss: 0.2149 - acc: 0.9381\n",
            "Epoch 16/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1932 - acc: 0.9458\n",
            "Epoch 17/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1924 - acc: 0.9458\n",
            "Epoch 18/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1909 - acc: 0.9497\n",
            "Epoch 19/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.2018 - acc: 0.9342\n",
            "Epoch 20/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1498 - acc: 0.9516\n",
            "Epoch 21/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1888 - acc: 0.9362\n",
            "Epoch 22/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1860 - acc: 0.9574\n",
            "Epoch 23/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1607 - acc: 0.9555\n",
            "Epoch 24/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.2187 - acc: 0.9304\n",
            "Epoch 25/250\n",
            "517/517 [==============================] - 0s 70us/step - loss: 0.1811 - acc: 0.9516\n",
            "Epoch 26/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.2056 - acc: 0.9381\n",
            "Epoch 27/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1796 - acc: 0.9458\n",
            "Epoch 28/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1594 - acc: 0.9555\n",
            "Epoch 29/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.2015 - acc: 0.9381\n",
            "Epoch 30/250\n",
            "517/517 [==============================] - 0s 93us/step - loss: 0.1700 - acc: 0.9536\n",
            "Epoch 31/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1857 - acc: 0.9574\n",
            "Epoch 32/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1718 - acc: 0.9555\n",
            "Epoch 33/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1528 - acc: 0.9574\n",
            "Epoch 34/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1397 - acc: 0.9691\n",
            "Epoch 35/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1720 - acc: 0.9497\n",
            "Epoch 36/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1611 - acc: 0.9574\n",
            "Epoch 37/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1833 - acc: 0.9497\n",
            "Epoch 38/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1836 - acc: 0.9613\n",
            "Epoch 39/250\n",
            "517/517 [==============================] - 0s 67us/step - loss: 0.1540 - acc: 0.9613\n",
            "Epoch 40/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1676 - acc: 0.9497\n",
            "Epoch 41/250\n",
            "517/517 [==============================] - 0s 75us/step - loss: 0.1780 - acc: 0.9613\n",
            "Epoch 42/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1567 - acc: 0.9632\n",
            "Epoch 43/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1540 - acc: 0.9478\n",
            "Epoch 44/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1614 - acc: 0.9478\n",
            "Epoch 45/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1621 - acc: 0.9478\n",
            "Epoch 46/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1800 - acc: 0.9574\n",
            "Epoch 47/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.1314 - acc: 0.9632\n",
            "Epoch 48/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1467 - acc: 0.9613\n",
            "Epoch 49/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1427 - acc: 0.9478\n",
            "Epoch 50/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1533 - acc: 0.9632\n",
            "Epoch 51/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1452 - acc: 0.9613\n",
            "Epoch 52/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1275 - acc: 0.9671\n",
            "Epoch 53/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1411 - acc: 0.9574\n",
            "Epoch 54/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.1436 - acc: 0.9555\n",
            "Epoch 55/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1523 - acc: 0.9613\n",
            "Epoch 56/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1362 - acc: 0.9671\n",
            "Epoch 57/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1193 - acc: 0.9652\n",
            "Epoch 58/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1675 - acc: 0.9458\n",
            "Epoch 59/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.1636 - acc: 0.9632\n",
            "Epoch 60/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1878 - acc: 0.9497\n",
            "Epoch 61/250\n",
            "517/517 [==============================] - 0s 91us/step - loss: 0.1930 - acc: 0.9439\n",
            "Epoch 62/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1296 - acc: 0.9671\n",
            "Epoch 63/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1180 - acc: 0.9574\n",
            "Epoch 64/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1824 - acc: 0.9497\n",
            "Epoch 65/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1185 - acc: 0.9632\n",
            "Epoch 66/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1323 - acc: 0.9555\n",
            "Epoch 67/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1378 - acc: 0.9710\n",
            "Epoch 68/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.1819 - acc: 0.9458\n",
            "Epoch 69/250\n",
            "517/517 [==============================] - 0s 67us/step - loss: 0.1232 - acc: 0.9632\n",
            "Epoch 70/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1350 - acc: 0.9613\n",
            "Epoch 71/250\n",
            "517/517 [==============================] - 0s 68us/step - loss: 0.1545 - acc: 0.9458\n",
            "Epoch 72/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1119 - acc: 0.9652\n",
            "Epoch 73/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1579 - acc: 0.9613\n",
            "Epoch 74/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.1429 - acc: 0.9671\n",
            "Epoch 75/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.1462 - acc: 0.9574\n",
            "Epoch 76/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1374 - acc: 0.9613\n",
            "Epoch 77/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1357 - acc: 0.9613\n",
            "Epoch 78/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.1436 - acc: 0.9594\n",
            "Epoch 79/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1182 - acc: 0.9710\n",
            "Epoch 80/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1623 - acc: 0.9536\n",
            "Epoch 81/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1244 - acc: 0.9768\n",
            "Epoch 82/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.1361 - acc: 0.9632\n",
            "Epoch 83/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1351 - acc: 0.9652\n",
            "Epoch 84/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1098 - acc: 0.9671\n",
            "Epoch 85/250\n",
            "517/517 [==============================] - 0s 68us/step - loss: 0.1314 - acc: 0.9671\n",
            "Epoch 86/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1007 - acc: 0.9729\n",
            "Epoch 87/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1341 - acc: 0.9691\n",
            "Epoch 88/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1615 - acc: 0.9594\n",
            "Epoch 89/250\n",
            "517/517 [==============================] - 0s 67us/step - loss: 0.1164 - acc: 0.9710\n",
            "Epoch 90/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.1287 - acc: 0.9671\n",
            "Epoch 91/250\n",
            "517/517 [==============================] - 0s 82us/step - loss: 0.1062 - acc: 0.9613\n",
            "Epoch 92/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1165 - acc: 0.9768\n",
            "Epoch 93/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.0984 - acc: 0.9729\n",
            "Epoch 94/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1445 - acc: 0.9555\n",
            "Epoch 95/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1362 - acc: 0.9671\n",
            "Epoch 96/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1118 - acc: 0.9691\n",
            "Epoch 97/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1462 - acc: 0.9632\n",
            "Epoch 98/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1352 - acc: 0.9632\n",
            "Epoch 99/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.1185 - acc: 0.9594\n",
            "Epoch 100/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1380 - acc: 0.9613\n",
            "Epoch 101/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1665 - acc: 0.9478\n",
            "Epoch 102/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1284 - acc: 0.9652\n",
            "Epoch 103/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1271 - acc: 0.9613\n",
            "Epoch 104/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1056 - acc: 0.9729\n",
            "Epoch 105/250\n",
            "517/517 [==============================] - 0s 67us/step - loss: 0.1247 - acc: 0.9613\n",
            "Epoch 106/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1181 - acc: 0.9632\n",
            "Epoch 107/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1113 - acc: 0.9632\n",
            "Epoch 108/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1194 - acc: 0.9671\n",
            "Epoch 109/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1673 - acc: 0.9478\n",
            "Epoch 110/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1232 - acc: 0.9632\n",
            "Epoch 111/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1445 - acc: 0.9574\n",
            "Epoch 112/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1345 - acc: 0.9729\n",
            "Epoch 113/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1215 - acc: 0.9555\n",
            "Epoch 114/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1132 - acc: 0.9691\n",
            "Epoch 115/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1172 - acc: 0.9691\n",
            "Epoch 116/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1091 - acc: 0.9671\n",
            "Epoch 117/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1228 - acc: 0.9710\n",
            "Epoch 118/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1057 - acc: 0.9632\n",
            "Epoch 119/250\n",
            "517/517 [==============================] - 0s 70us/step - loss: 0.1314 - acc: 0.9671\n",
            "Epoch 120/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1136 - acc: 0.9671\n",
            "Epoch 121/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1007 - acc: 0.9729\n",
            "Epoch 122/250\n",
            "517/517 [==============================] - 0s 84us/step - loss: 0.1192 - acc: 0.9632\n",
            "Epoch 123/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.0952 - acc: 0.9691\n",
            "Epoch 124/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1078 - acc: 0.9671\n",
            "Epoch 125/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.0969 - acc: 0.9691\n",
            "Epoch 126/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1581 - acc: 0.9574\n",
            "Epoch 127/250\n",
            "517/517 [==============================] - 0s 68us/step - loss: 0.0967 - acc: 0.9613\n",
            "Epoch 128/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.0963 - acc: 0.9729\n",
            "Epoch 129/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.0865 - acc: 0.9729\n",
            "Epoch 130/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1386 - acc: 0.9613\n",
            "Epoch 131/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1044 - acc: 0.9652\n",
            "Epoch 132/250\n",
            "517/517 [==============================] - 0s 69us/step - loss: 0.1787 - acc: 0.9458\n",
            "Epoch 133/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1271 - acc: 0.9594\n",
            "Epoch 134/250\n",
            "517/517 [==============================] - 0s 90us/step - loss: 0.1244 - acc: 0.9613\n",
            "Epoch 135/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.1185 - acc: 0.9632\n",
            "Epoch 136/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1206 - acc: 0.9613\n",
            "Epoch 137/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1104 - acc: 0.9594\n",
            "Epoch 138/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1007 - acc: 0.9594\n",
            "Epoch 139/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.0955 - acc: 0.9729\n",
            "Epoch 140/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.0817 - acc: 0.9729\n",
            "Epoch 141/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.0873 - acc: 0.9729\n",
            "Epoch 142/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.0978 - acc: 0.9729\n",
            "Epoch 143/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.0938 - acc: 0.9671\n",
            "Epoch 144/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1221 - acc: 0.9594\n",
            "Epoch 145/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.1130 - acc: 0.9632\n",
            "Epoch 146/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1119 - acc: 0.9729\n",
            "Epoch 147/250\n",
            "517/517 [==============================] - 0s 71us/step - loss: 0.0875 - acc: 0.9710\n",
            "Epoch 148/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0957 - acc: 0.9691\n",
            "Epoch 149/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1007 - acc: 0.9729\n",
            "Epoch 150/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1417 - acc: 0.9574\n",
            "Epoch 151/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1325 - acc: 0.9613\n",
            "Epoch 152/250\n",
            "517/517 [==============================] - 0s 77us/step - loss: 0.1258 - acc: 0.9652\n",
            "Epoch 153/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.0861 - acc: 0.9749\n",
            "Epoch 154/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.0975 - acc: 0.9691\n",
            "Epoch 155/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.1074 - acc: 0.9652\n",
            "Epoch 156/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1051 - acc: 0.9652\n",
            "Epoch 157/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1019 - acc: 0.9652\n",
            "Epoch 158/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1120 - acc: 0.9574\n",
            "Epoch 159/250\n",
            "517/517 [==============================] - 0s 69us/step - loss: 0.1026 - acc: 0.9691\n",
            "Epoch 160/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1007 - acc: 0.9691\n",
            "Epoch 161/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.0998 - acc: 0.9729\n",
            "Epoch 162/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1076 - acc: 0.9632\n",
            "Epoch 163/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.1163 - acc: 0.9691\n",
            "Epoch 164/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1102 - acc: 0.9691\n",
            "Epoch 165/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1018 - acc: 0.9691\n",
            "Epoch 166/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.0970 - acc: 0.9749\n",
            "Epoch 167/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1082 - acc: 0.9632\n",
            "Epoch 168/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1028 - acc: 0.9691\n",
            "Epoch 169/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.0942 - acc: 0.9691\n",
            "Epoch 170/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.0862 - acc: 0.9787\n",
            "Epoch 171/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1075 - acc: 0.9671\n",
            "Epoch 172/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1474 - acc: 0.9536\n",
            "Epoch 173/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1195 - acc: 0.9632\n",
            "Epoch 174/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1007 - acc: 0.9691\n",
            "Epoch 175/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1220 - acc: 0.9574\n",
            "Epoch 176/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0696 - acc: 0.9826\n",
            "Epoch 177/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0851 - acc: 0.9749\n",
            "Epoch 178/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.0835 - acc: 0.9729\n",
            "Epoch 179/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.0815 - acc: 0.9749\n",
            "Epoch 180/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.1130 - acc: 0.9671\n",
            "Epoch 181/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1165 - acc: 0.9497\n",
            "Epoch 182/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.1066 - acc: 0.9652\n",
            "Epoch 183/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.0793 - acc: 0.9768\n",
            "Epoch 184/250\n",
            "517/517 [==============================] - 0s 83us/step - loss: 0.1057 - acc: 0.9652\n",
            "Epoch 185/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1195 - acc: 0.9574\n",
            "Epoch 186/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.0943 - acc: 0.9671\n",
            "Epoch 187/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1220 - acc: 0.9632\n",
            "Epoch 188/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.1052 - acc: 0.9594\n",
            "Epoch 189/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.1174 - acc: 0.9691\n",
            "Epoch 190/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0925 - acc: 0.9555\n",
            "Epoch 191/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.0702 - acc: 0.9807\n",
            "Epoch 192/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.1032 - acc: 0.9710\n",
            "Epoch 193/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1307 - acc: 0.9613\n",
            "Epoch 194/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.0977 - acc: 0.9652\n",
            "Epoch 195/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.0860 - acc: 0.9768\n",
            "Epoch 196/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1109 - acc: 0.9691\n",
            "Epoch 197/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1115 - acc: 0.9652\n",
            "Epoch 198/250\n",
            "517/517 [==============================] - 0s 64us/step - loss: 0.0813 - acc: 0.9749\n",
            "Epoch 199/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.0721 - acc: 0.9826\n",
            "Epoch 200/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1328 - acc: 0.9594\n",
            "Epoch 201/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.0902 - acc: 0.9729\n",
            "Epoch 202/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.1160 - acc: 0.9574\n",
            "Epoch 203/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.1066 - acc: 0.9691\n",
            "Epoch 204/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1017 - acc: 0.9710\n",
            "Epoch 205/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1011 - acc: 0.9671\n",
            "Epoch 206/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1311 - acc: 0.9536\n",
            "Epoch 207/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1179 - acc: 0.9613\n",
            "Epoch 208/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1231 - acc: 0.9594\n",
            "Epoch 209/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.0849 - acc: 0.9691\n",
            "Epoch 210/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.1179 - acc: 0.9652\n",
            "Epoch 211/250\n",
            "517/517 [==============================] - 0s 65us/step - loss: 0.0870 - acc: 0.9710\n",
            "Epoch 212/250\n",
            "517/517 [==============================] - 0s 67us/step - loss: 0.1327 - acc: 0.9478\n",
            "Epoch 213/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.1107 - acc: 0.9613\n",
            "Epoch 214/250\n",
            "517/517 [==============================] - 0s 75us/step - loss: 0.0989 - acc: 0.9729\n",
            "Epoch 215/250\n",
            "517/517 [==============================] - 0s 66us/step - loss: 0.0735 - acc: 0.9845\n",
            "Epoch 216/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.0925 - acc: 0.9671\n",
            "Epoch 217/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1147 - acc: 0.9594\n",
            "Epoch 218/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.0851 - acc: 0.9787\n",
            "Epoch 219/250\n",
            "517/517 [==============================] - 0s 49us/step - loss: 0.0769 - acc: 0.9749\n",
            "Epoch 220/250\n",
            "517/517 [==============================] - 0s 62us/step - loss: 0.1129 - acc: 0.9574\n",
            "Epoch 221/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.1023 - acc: 0.9652\n",
            "Epoch 222/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.0922 - acc: 0.9729\n",
            "Epoch 223/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.0906 - acc: 0.9632\n",
            "Epoch 224/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.0851 - acc: 0.9710\n",
            "Epoch 225/250\n",
            "517/517 [==============================] - 0s 61us/step - loss: 0.0896 - acc: 0.9652\n",
            "Epoch 226/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.0641 - acc: 0.9865\n",
            "Epoch 227/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1029 - acc: 0.9632\n",
            "Epoch 228/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.0698 - acc: 0.9787\n",
            "Epoch 229/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1420 - acc: 0.9613\n",
            "Epoch 230/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0919 - acc: 0.9768\n",
            "Epoch 231/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.1208 - acc: 0.9710\n",
            "Epoch 232/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.1017 - acc: 0.9671\n",
            "Epoch 233/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.0582 - acc: 0.9865\n",
            "Epoch 234/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.1377 - acc: 0.9555\n",
            "Epoch 235/250\n",
            "517/517 [==============================] - 0s 59us/step - loss: 0.0826 - acc: 0.9787\n",
            "Epoch 236/250\n",
            "517/517 [==============================] - 0s 53us/step - loss: 0.0752 - acc: 0.9826\n",
            "Epoch 237/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1182 - acc: 0.9671\n",
            "Epoch 238/250\n",
            "517/517 [==============================] - 0s 55us/step - loss: 0.0806 - acc: 0.9787\n",
            "Epoch 239/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.1135 - acc: 0.9632\n",
            "Epoch 240/250\n",
            "517/517 [==============================] - 0s 52us/step - loss: 0.0678 - acc: 0.9845\n",
            "Epoch 241/250\n",
            "517/517 [==============================] - 0s 63us/step - loss: 0.0876 - acc: 0.9807\n",
            "Epoch 242/250\n",
            "517/517 [==============================] - 0s 54us/step - loss: 0.0770 - acc: 0.9710\n",
            "Epoch 243/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.1034 - acc: 0.9671\n",
            "Epoch 244/250\n",
            "517/517 [==============================] - 0s 57us/step - loss: 0.0727 - acc: 0.9884\n",
            "Epoch 245/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0771 - acc: 0.9787\n",
            "Epoch 246/250\n",
            "517/517 [==============================] - 0s 56us/step - loss: 0.0652 - acc: 0.9826\n",
            "Epoch 247/250\n",
            "517/517 [==============================] - 0s 68us/step - loss: 0.0729 - acc: 0.9768\n",
            "Epoch 248/250\n",
            "517/517 [==============================] - 0s 73us/step - loss: 0.1002 - acc: 0.9729\n",
            "Epoch 249/250\n",
            "517/517 [==============================] - 0s 60us/step - loss: 0.0698 - acc: 0.9807\n",
            "Epoch 250/250\n",
            "517/517 [==============================] - 0s 58us/step - loss: 0.0668 - acc: 0.9826\n",
            "517/517 [==============================] - 0s 29us/step\n",
            "Test loss: 2.5606124570568016\n",
            "Test accuracy: 0.6228239846414017\n",
            "Test loss: 2.5606124570568016\n",
            "Test accuracy: 0.6228239846414017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 59%|█████▉    | 13/22 [01:55<01:21,  9.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.6228239845261122 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.84      0.81        75\n",
            "         1.0       0.00      0.00      0.00       108\n",
            "         2.0       0.76      0.81      0.78       176\n",
            "         3.0       0.94      0.71      0.81       112\n",
            "         4.0       0.23      0.83      0.36        46\n",
            "\n",
            "   micro avg       0.62      0.62      0.62       517\n",
            "   macro avg       0.54      0.64      0.55       517\n",
            "weighted avg       0.60      0.62      0.59       517\n",
            "\n",
            "[[ 63   0   2   0  10]\n",
            " [ 17   0   3   0  88]\n",
            " [  1   0 142   4  29]\n",
            " [  0   0  33  79   0]\n",
            " [  0   0   7   1  38]]\n",
            "[[ 31   0   0   0   5]\n",
            " [  3   0   1   0   9]\n",
            " [  0   0 216   0   4]\n",
            " [  0   0   1  67   0]\n",
            " [  0   0   1   0 179]]\n",
            "Epoch 1/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 1.0700 - acc: 0.8224\n",
            "Epoch 2/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.9333 - acc: 0.8499\n",
            "Epoch 3/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.7307 - acc: 0.8647\n",
            "Epoch 4/250\n",
            "473/473 [==============================] - 0s 46us/step - loss: 0.6251 - acc: 0.8816\n",
            "Epoch 5/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.6020 - acc: 0.8943\n",
            "Epoch 6/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.4951 - acc: 0.8985\n",
            "Epoch 7/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.4762 - acc: 0.9049\n",
            "Epoch 8/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.4656 - acc: 0.9006\n",
            "Epoch 9/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.3918 - acc: 0.9133\n",
            "Epoch 10/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.3836 - acc: 0.9197\n",
            "Epoch 11/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.3686 - acc: 0.9260\n",
            "Epoch 12/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.3585 - acc: 0.9260\n",
            "Epoch 13/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.3339 - acc: 0.9239\n",
            "Epoch 14/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.2152 - acc: 0.9471\n",
            "Epoch 15/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.2601 - acc: 0.9387\n",
            "Epoch 16/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.2452 - acc: 0.9429\n",
            "Epoch 17/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.2721 - acc: 0.9239\n",
            "Epoch 18/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.2638 - acc: 0.9323\n",
            "Epoch 19/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.2273 - acc: 0.9302\n",
            "Epoch 20/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.2911 - acc: 0.9429\n",
            "Epoch 21/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.2793 - acc: 0.9323\n",
            "Epoch 22/250\n",
            "473/473 [==============================] - 0s 61us/step - loss: 0.2499 - acc: 0.9450\n",
            "Epoch 23/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.2296 - acc: 0.9366\n",
            "Epoch 24/250\n",
            "473/473 [==============================] - 0s 79us/step - loss: 0.2280 - acc: 0.9345\n",
            "Epoch 25/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.2578 - acc: 0.9366\n",
            "Epoch 26/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.2296 - acc: 0.9366\n",
            "Epoch 27/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.2354 - acc: 0.9345\n",
            "Epoch 28/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.2100 - acc: 0.9450\n",
            "Epoch 29/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1812 - acc: 0.9577\n",
            "Epoch 30/250\n",
            "473/473 [==============================] - 0s 61us/step - loss: 0.2077 - acc: 0.9514\n",
            "Epoch 31/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.1864 - acc: 0.9429\n",
            "Epoch 32/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1535 - acc: 0.9535\n",
            "Epoch 33/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.1877 - acc: 0.9429\n",
            "Epoch 34/250\n",
            "473/473 [==============================] - 0s 46us/step - loss: 0.2036 - acc: 0.9408\n",
            "Epoch 35/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1169 - acc: 0.9577\n",
            "Epoch 36/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1157 - acc: 0.9641\n",
            "Epoch 37/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1756 - acc: 0.9408\n",
            "Epoch 38/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.2186 - acc: 0.9387\n",
            "Epoch 39/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1779 - acc: 0.9514\n",
            "Epoch 40/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1580 - acc: 0.9577\n",
            "Epoch 41/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.1342 - acc: 0.9598\n",
            "Epoch 42/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1318 - acc: 0.9619\n",
            "Epoch 43/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.2097 - acc: 0.9429\n",
            "Epoch 44/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1698 - acc: 0.9577\n",
            "Epoch 45/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1585 - acc: 0.9577\n",
            "Epoch 46/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1598 - acc: 0.9514\n",
            "Epoch 47/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.1300 - acc: 0.9619\n",
            "Epoch 48/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1458 - acc: 0.9577\n",
            "Epoch 49/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.1496 - acc: 0.9514\n",
            "Epoch 50/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1362 - acc: 0.9556\n",
            "Epoch 51/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1579 - acc: 0.9535\n",
            "Epoch 52/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1293 - acc: 0.9704\n",
            "Epoch 53/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.1455 - acc: 0.9514\n",
            "Epoch 54/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1711 - acc: 0.9514\n",
            "Epoch 55/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.1294 - acc: 0.9641\n",
            "Epoch 56/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1315 - acc: 0.9662\n",
            "Epoch 57/250\n",
            "473/473 [==============================] - 0s 45us/step - loss: 0.1437 - acc: 0.9598\n",
            "Epoch 58/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1273 - acc: 0.9641\n",
            "Epoch 59/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1357 - acc: 0.9619\n",
            "Epoch 60/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1743 - acc: 0.9514\n",
            "Epoch 61/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1193 - acc: 0.9641\n",
            "Epoch 62/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1483 - acc: 0.9535\n",
            "Epoch 63/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.1381 - acc: 0.9577\n",
            "Epoch 64/250\n",
            "473/473 [==============================] - 0s 82us/step - loss: 0.1671 - acc: 0.9450\n",
            "Epoch 65/250\n",
            "473/473 [==============================] - 0s 71us/step - loss: 0.1200 - acc: 0.9619\n",
            "Epoch 66/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1148 - acc: 0.9704\n",
            "Epoch 67/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1508 - acc: 0.9514\n",
            "Epoch 68/250\n",
            "473/473 [==============================] - 0s 79us/step - loss: 0.1374 - acc: 0.9577\n",
            "Epoch 69/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.1213 - acc: 0.9619\n",
            "Epoch 70/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.1489 - acc: 0.9598\n",
            "Epoch 71/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1132 - acc: 0.9683\n",
            "Epoch 72/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0979 - acc: 0.9725\n",
            "Epoch 73/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1227 - acc: 0.9683\n",
            "Epoch 74/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1141 - acc: 0.9619\n",
            "Epoch 75/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.1158 - acc: 0.9577\n",
            "Epoch 76/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.1122 - acc: 0.9641\n",
            "Epoch 77/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.1283 - acc: 0.9556\n",
            "Epoch 78/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1008 - acc: 0.9683\n",
            "Epoch 79/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1439 - acc: 0.9514\n",
            "Epoch 80/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1400 - acc: 0.9641\n",
            "Epoch 81/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1165 - acc: 0.9662\n",
            "Epoch 82/250\n",
            "473/473 [==============================] - 0s 46us/step - loss: 0.0978 - acc: 0.9746\n",
            "Epoch 83/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1499 - acc: 0.9535\n",
            "Epoch 84/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.1204 - acc: 0.9598\n",
            "Epoch 85/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0762 - acc: 0.9767\n",
            "Epoch 86/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0741 - acc: 0.9789\n",
            "Epoch 87/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0981 - acc: 0.9683\n",
            "Epoch 88/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1274 - acc: 0.9641\n",
            "Epoch 89/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1371 - acc: 0.9641\n",
            "Epoch 90/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.1302 - acc: 0.9619\n",
            "Epoch 91/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.0794 - acc: 0.9767\n",
            "Epoch 92/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.1528 - acc: 0.9556\n",
            "Epoch 93/250\n",
            "473/473 [==============================] - 0s 63us/step - loss: 0.0939 - acc: 0.9725\n",
            "Epoch 94/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.0985 - acc: 0.9683\n",
            "Epoch 95/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1033 - acc: 0.9662\n",
            "Epoch 96/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1037 - acc: 0.9704\n",
            "Epoch 97/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.0836 - acc: 0.9746\n",
            "Epoch 98/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.1341 - acc: 0.9641\n",
            "Epoch 99/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.1302 - acc: 0.9619\n",
            "Epoch 100/250\n",
            "473/473 [==============================] - 0s 63us/step - loss: 0.1202 - acc: 0.9641\n",
            "Epoch 101/250\n",
            "473/473 [==============================] - 0s 91us/step - loss: 0.0797 - acc: 0.9767\n",
            "Epoch 102/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.1026 - acc: 0.9704\n",
            "Epoch 103/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0901 - acc: 0.9641\n",
            "Epoch 104/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0882 - acc: 0.9767\n",
            "Epoch 105/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0921 - acc: 0.9704\n",
            "Epoch 106/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0743 - acc: 0.9789\n",
            "Epoch 107/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.1246 - acc: 0.9641\n",
            "Epoch 108/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1157 - acc: 0.9598\n",
            "Epoch 109/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.1032 - acc: 0.9662\n",
            "Epoch 110/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1059 - acc: 0.9704\n",
            "Epoch 111/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0947 - acc: 0.9746\n",
            "Epoch 112/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.0842 - acc: 0.9789\n",
            "Epoch 113/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0887 - acc: 0.9725\n",
            "Epoch 114/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.1258 - acc: 0.9662\n",
            "Epoch 115/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.1063 - acc: 0.9704\n",
            "Epoch 116/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.0766 - acc: 0.9725\n",
            "Epoch 117/250\n",
            "473/473 [==============================] - 0s 65us/step - loss: 0.1201 - acc: 0.9556\n",
            "Epoch 118/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.1165 - acc: 0.9619\n",
            "Epoch 119/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1517 - acc: 0.9556\n",
            "Epoch 120/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.1048 - acc: 0.9704\n",
            "Epoch 121/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.1397 - acc: 0.9619\n",
            "Epoch 122/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.0754 - acc: 0.9789\n",
            "Epoch 123/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.0798 - acc: 0.9746\n",
            "Epoch 124/250\n",
            "473/473 [==============================] - 0s 61us/step - loss: 0.0734 - acc: 0.9767\n",
            "Epoch 125/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0939 - acc: 0.9725\n",
            "Epoch 126/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.1182 - acc: 0.9641\n",
            "Epoch 127/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0932 - acc: 0.9725\n",
            "Epoch 128/250\n",
            "473/473 [==============================] - 0s 46us/step - loss: 0.0935 - acc: 0.9725\n",
            "Epoch 129/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1119 - acc: 0.9683\n",
            "Epoch 130/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0808 - acc: 0.9831\n",
            "Epoch 131/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0815 - acc: 0.9704\n",
            "Epoch 132/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1267 - acc: 0.9619\n",
            "Epoch 133/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.0903 - acc: 0.9704\n",
            "Epoch 134/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0799 - acc: 0.9789\n",
            "Epoch 135/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.0782 - acc: 0.9767\n",
            "Epoch 136/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0765 - acc: 0.9789\n",
            "Epoch 137/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.1096 - acc: 0.9704\n",
            "Epoch 138/250\n",
            "473/473 [==============================] - 0s 74us/step - loss: 0.0582 - acc: 0.9873\n",
            "Epoch 139/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.1194 - acc: 0.9683\n",
            "Epoch 140/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.0800 - acc: 0.9789\n",
            "Epoch 141/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1053 - acc: 0.9704\n",
            "Epoch 142/250\n",
            "473/473 [==============================] - 0s 66us/step - loss: 0.1183 - acc: 0.9641\n",
            "Epoch 143/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0796 - acc: 0.9810\n",
            "Epoch 144/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0781 - acc: 0.9725\n",
            "Epoch 145/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0896 - acc: 0.9810\n",
            "Epoch 146/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0923 - acc: 0.9725\n",
            "Epoch 147/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.1488 - acc: 0.9662\n",
            "Epoch 148/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.0879 - acc: 0.9831\n",
            "Epoch 149/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0820 - acc: 0.9746\n",
            "Epoch 150/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.0808 - acc: 0.9789\n",
            "Epoch 151/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0856 - acc: 0.9746\n",
            "Epoch 152/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.0719 - acc: 0.9831\n",
            "Epoch 153/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1103 - acc: 0.9683\n",
            "Epoch 154/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1175 - acc: 0.9598\n",
            "Epoch 155/250\n",
            "473/473 [==============================] - 0s 41us/step - loss: 0.0875 - acc: 0.9725\n",
            "Epoch 156/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0937 - acc: 0.9704\n",
            "Epoch 157/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0521 - acc: 0.9852\n",
            "Epoch 158/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.1147 - acc: 0.9683\n",
            "Epoch 159/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0699 - acc: 0.9789\n",
            "Epoch 160/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0661 - acc: 0.9810\n",
            "Epoch 161/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0987 - acc: 0.9662\n",
            "Epoch 162/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1435 - acc: 0.9514\n",
            "Epoch 163/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0779 - acc: 0.9767\n",
            "Epoch 164/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0903 - acc: 0.9746\n",
            "Epoch 165/250\n",
            "473/473 [==============================] - 0s 46us/step - loss: 0.0992 - acc: 0.9767\n",
            "Epoch 166/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0826 - acc: 0.9746\n",
            "Epoch 167/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0999 - acc: 0.9641\n",
            "Epoch 168/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1139 - acc: 0.9725\n",
            "Epoch 169/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.1212 - acc: 0.9619\n",
            "Epoch 170/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1061 - acc: 0.9683\n",
            "Epoch 171/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.1033 - acc: 0.9767\n",
            "Epoch 172/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1286 - acc: 0.9619\n",
            "Epoch 173/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0916 - acc: 0.9746\n",
            "Epoch 174/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0723 - acc: 0.9789\n",
            "Epoch 175/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0795 - acc: 0.9746\n",
            "Epoch 176/250\n",
            "473/473 [==============================] - 0s 65us/step - loss: 0.0781 - acc: 0.9789\n",
            "Epoch 177/250\n",
            "473/473 [==============================] - 0s 74us/step - loss: 0.0524 - acc: 0.9873\n",
            "Epoch 178/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.0703 - acc: 0.9852\n",
            "Epoch 179/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.0972 - acc: 0.9725\n",
            "Epoch 180/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.0578 - acc: 0.9810\n",
            "Epoch 181/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1043 - acc: 0.9746\n",
            "Epoch 182/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.0655 - acc: 0.9810\n",
            "Epoch 183/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0876 - acc: 0.9683\n",
            "Epoch 184/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.0917 - acc: 0.9704\n",
            "Epoch 185/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0818 - acc: 0.9767\n",
            "Epoch 186/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0641 - acc: 0.9810\n",
            "Epoch 187/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.0802 - acc: 0.9746\n",
            "Epoch 188/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0817 - acc: 0.9767\n",
            "Epoch 189/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0959 - acc: 0.9662\n",
            "Epoch 190/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.0864 - acc: 0.9767\n",
            "Epoch 191/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1038 - acc: 0.9662\n",
            "Epoch 192/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0589 - acc: 0.9810\n",
            "Epoch 193/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0715 - acc: 0.9789\n",
            "Epoch 194/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0882 - acc: 0.9767\n",
            "Epoch 195/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.0783 - acc: 0.9810\n",
            "Epoch 196/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0997 - acc: 0.9641\n",
            "Epoch 197/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0908 - acc: 0.9746\n",
            "Epoch 198/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0701 - acc: 0.9810\n",
            "Epoch 199/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0788 - acc: 0.9767\n",
            "Epoch 200/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0541 - acc: 0.9894\n",
            "Epoch 201/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0896 - acc: 0.9746\n",
            "Epoch 202/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.0868 - acc: 0.9704\n",
            "Epoch 203/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.1018 - acc: 0.9683\n",
            "Epoch 204/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0637 - acc: 0.9789\n",
            "Epoch 205/250\n",
            "473/473 [==============================] - 0s 49us/step - loss: 0.1019 - acc: 0.9725\n",
            "Epoch 206/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0769 - acc: 0.9767\n",
            "Epoch 207/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0849 - acc: 0.9767\n",
            "Epoch 208/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1027 - acc: 0.9767\n",
            "Epoch 209/250\n",
            "473/473 [==============================] - 0s 51us/step - loss: 0.0816 - acc: 0.9746\n",
            "Epoch 210/250\n",
            "473/473 [==============================] - 0s 47us/step - loss: 0.0779 - acc: 0.9725\n",
            "Epoch 211/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.0720 - acc: 0.9725\n",
            "Epoch 212/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.1034 - acc: 0.9725\n",
            "Epoch 213/250\n",
            "473/473 [==============================] - 0s 48us/step - loss: 0.0649 - acc: 0.9831\n",
            "Epoch 214/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0827 - acc: 0.9746\n",
            "Epoch 215/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.1005 - acc: 0.9725\n",
            "Epoch 216/250\n",
            "473/473 [==============================] - 0s 67us/step - loss: 0.0832 - acc: 0.9767\n",
            "Epoch 217/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0628 - acc: 0.9831\n",
            "Epoch 218/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.0579 - acc: 0.9852\n",
            "Epoch 219/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0443 - acc: 0.9915\n",
            "Epoch 220/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.0649 - acc: 0.9831\n",
            "Epoch 221/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.0743 - acc: 0.9767\n",
            "Epoch 222/250\n",
            "473/473 [==============================] - 0s 67us/step - loss: 0.0721 - acc: 0.9789\n",
            "Epoch 223/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.0875 - acc: 0.9746\n",
            "Epoch 224/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.0606 - acc: 0.9789\n",
            "Epoch 225/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.1180 - acc: 0.9704\n",
            "Epoch 226/250\n",
            "473/473 [==============================] - 0s 64us/step - loss: 0.0868 - acc: 0.9725\n",
            "Epoch 227/250\n",
            "473/473 [==============================] - 0s 50us/step - loss: 0.0759 - acc: 0.9725\n",
            "Epoch 228/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0490 - acc: 0.9873\n",
            "Epoch 229/250\n",
            "473/473 [==============================] - 0s 55us/step - loss: 0.0625 - acc: 0.9810\n",
            "Epoch 230/250\n",
            "473/473 [==============================] - 0s 61us/step - loss: 0.0851 - acc: 0.9746\n",
            "Epoch 231/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.0749 - acc: 0.9810\n",
            "Epoch 232/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0568 - acc: 0.9810\n",
            "Epoch 233/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.0731 - acc: 0.9746\n",
            "Epoch 234/250\n",
            "473/473 [==============================] - 0s 52us/step - loss: 0.1296 - acc: 0.9767\n",
            "Epoch 235/250\n",
            "473/473 [==============================] - 0s 57us/step - loss: 0.0967 - acc: 0.9746\n",
            "Epoch 236/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.1025 - acc: 0.9641\n",
            "Epoch 237/250\n",
            "473/473 [==============================] - 0s 62us/step - loss: 0.0790 - acc: 0.9725\n",
            "Epoch 238/250\n",
            "473/473 [==============================] - 0s 59us/step - loss: 0.0855 - acc: 0.9746\n",
            "Epoch 239/250\n",
            "473/473 [==============================] - 0s 68us/step - loss: 0.0805 - acc: 0.9746\n",
            "Epoch 240/250\n",
            "473/473 [==============================] - 0s 65us/step - loss: 0.0900 - acc: 0.9746\n",
            "Epoch 241/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0426 - acc: 0.9873\n",
            "Epoch 242/250\n",
            "473/473 [==============================] - 0s 66us/step - loss: 0.0838 - acc: 0.9831\n",
            "Epoch 243/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0749 - acc: 0.9767\n",
            "Epoch 244/250\n",
            "473/473 [==============================] - 0s 53us/step - loss: 0.0964 - acc: 0.9725\n",
            "Epoch 245/250\n",
            "473/473 [==============================] - 0s 58us/step - loss: 0.0963 - acc: 0.9704\n",
            "Epoch 246/250\n",
            "473/473 [==============================] - 0s 61us/step - loss: 0.0668 - acc: 0.9789\n",
            "Epoch 247/250\n",
            "473/473 [==============================] - 0s 60us/step - loss: 0.0764 - acc: 0.9810\n",
            "Epoch 248/250\n",
            "473/473 [==============================] - 0s 54us/step - loss: 0.0600 - acc: 0.9789\n",
            "Epoch 249/250\n",
            "473/473 [==============================] - 0s 56us/step - loss: 0.0868 - acc: 0.9746\n",
            "Epoch 250/250\n",
            "473/473 [==============================] - 0s 78us/step - loss: 0.0615 - acc: 0.9873\n",
            "473/473 [==============================] - 0s 39us/step\n",
            "Test loss: 1.7022697635735315\n",
            "Test accuracy: 0.7124735677721132\n",
            "Test loss: 1.7022697635735315\n",
            "Test accuracy: 0.7124735677721132\n",
            "Seq Test accuracy score : 0.5856236786469344 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▎   | 14/22 [02:03<01:10,  8.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.85      0.74        73\n",
            "         1.0       0.00      0.00      0.00        69\n",
            "         2.0       0.48      0.93      0.63       139\n",
            "         3.0       0.89      0.48      0.62       143\n",
            "         4.0       0.55      0.37      0.44        49\n",
            "\n",
            "   micro avg       0.59      0.59      0.59       473\n",
            "   macro avg       0.52      0.52      0.49       473\n",
            "weighted avg       0.57      0.59      0.53       473\n",
            "\n",
            "[[ 62   0   7   1   3]\n",
            " [ 27   0  29   3  10]\n",
            " [  4   0 129   4   2]\n",
            " [  0   0  75  68   0]\n",
            " [  1   0  30   0  18]]\n",
            "[[ 70   0   0   0   1]\n",
            " [  1   0   6   1   2]\n",
            " [  0   0 326   2   0]\n",
            " [  0   0   9  16   0]\n",
            " [  0   0  11   0  28]]\n",
            "Epoch 1/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.9206 - acc: 0.6043\n",
            "Epoch 2/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 2.0570 - acc: 0.5226\n",
            "Epoch 3/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.6785 - acc: 0.5419\n",
            "Epoch 4/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.4291 - acc: 0.5161\n",
            "Epoch 5/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.4590 - acc: 0.4108\n",
            "Epoch 6/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.3379 - acc: 0.4387\n",
            "Epoch 7/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.4291 - acc: 0.3871\n",
            "Epoch 8/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.3438 - acc: 0.4022\n",
            "Epoch 9/250\n",
            "465/465 [==============================] - 0s 63us/step - loss: 1.2856 - acc: 0.5140\n",
            "Epoch 10/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.2593 - acc: 0.5032\n",
            "Epoch 11/250\n",
            "465/465 [==============================] - 0s 64us/step - loss: 1.2877 - acc: 0.5161\n",
            "Epoch 12/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.2011 - acc: 0.5355\n",
            "Epoch 13/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.2205 - acc: 0.5118\n",
            "Epoch 14/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.2116 - acc: 0.5204\n",
            "Epoch 15/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1915 - acc: 0.5054\n",
            "Epoch 16/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.2372 - acc: 0.5075\n",
            "Epoch 17/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.2385 - acc: 0.5075\n",
            "Epoch 18/250\n",
            "465/465 [==============================] - 0s 65us/step - loss: 1.2004 - acc: 0.5075\n",
            "Epoch 19/250\n",
            "465/465 [==============================] - 0s 68us/step - loss: 1.2362 - acc: 0.5011\n",
            "Epoch 20/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.2038 - acc: 0.4968\n",
            "Epoch 21/250\n",
            "465/465 [==============================] - 0s 58us/step - loss: 1.2400 - acc: 0.5118\n",
            "Epoch 22/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.2061 - acc: 0.5011\n",
            "Epoch 23/250\n",
            "465/465 [==============================] - 0s 61us/step - loss: 1.1759 - acc: 0.5226\n",
            "Epoch 24/250\n",
            "465/465 [==============================] - 0s 63us/step - loss: 1.2136 - acc: 0.4753\n",
            "Epoch 25/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.2093 - acc: 0.4753\n",
            "Epoch 26/250\n",
            "465/465 [==============================] - 0s 92us/step - loss: 1.2135 - acc: 0.4581\n",
            "Epoch 27/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.2139 - acc: 0.5161\n",
            "Epoch 28/250\n",
            "465/465 [==============================] - 0s 61us/step - loss: 1.2244 - acc: 0.5011\n",
            "Epoch 29/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1687 - acc: 0.5140\n",
            "Epoch 30/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1895 - acc: 0.4946\n",
            "Epoch 31/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1714 - acc: 0.4968\n",
            "Epoch 32/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.2161 - acc: 0.4925\n",
            "Epoch 33/250\n",
            "465/465 [==============================] - 0s 64us/step - loss: 1.2289 - acc: 0.4860\n",
            "Epoch 34/250\n",
            "465/465 [==============================] - 0s 67us/step - loss: 1.1752 - acc: 0.4946\n",
            "Epoch 35/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.2030 - acc: 0.5097\n",
            "Epoch 36/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.2392 - acc: 0.4946\n",
            "Epoch 37/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1679 - acc: 0.5097\n",
            "Epoch 38/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1887 - acc: 0.5140\n",
            "Epoch 39/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1759 - acc: 0.5075\n",
            "Epoch 40/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1673 - acc: 0.5247\n",
            "Epoch 41/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1866 - acc: 0.5097\n",
            "Epoch 42/250\n",
            "465/465 [==============================] - 0s 58us/step - loss: 1.1868 - acc: 0.4968\n",
            "Epoch 43/250\n",
            "465/465 [==============================] - 0s 61us/step - loss: 1.2075 - acc: 0.5032\n",
            "Epoch 44/250\n",
            "465/465 [==============================] - 0s 67us/step - loss: 1.1310 - acc: 0.5183\n",
            "Epoch 45/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1433 - acc: 0.5312\n",
            "Epoch 46/250\n",
            "465/465 [==============================] - 0s 61us/step - loss: 1.1957 - acc: 0.4796\n",
            "Epoch 47/250\n",
            "465/465 [==============================] - 0s 64us/step - loss: 1.2093 - acc: 0.5054\n",
            "Epoch 48/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1887 - acc: 0.4860\n",
            "Epoch 49/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1859 - acc: 0.4989\n",
            "Epoch 50/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.1526 - acc: 0.5140\n",
            "Epoch 51/250\n",
            "465/465 [==============================] - 0s 66us/step - loss: 1.2364 - acc: 0.4667\n",
            "Epoch 52/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1410 - acc: 0.5247\n",
            "Epoch 53/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.1711 - acc: 0.5075\n",
            "Epoch 54/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1597 - acc: 0.5054\n",
            "Epoch 55/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1728 - acc: 0.4839\n",
            "Epoch 56/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.1713 - acc: 0.4903\n",
            "Epoch 57/250\n",
            "465/465 [==============================] - 0s 63us/step - loss: 1.1867 - acc: 0.4645\n",
            "Epoch 58/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.1735 - acc: 0.4968\n",
            "Epoch 59/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1670 - acc: 0.4946\n",
            "Epoch 60/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.2361 - acc: 0.4796\n",
            "Epoch 61/250\n",
            "465/465 [==============================] - 0s 92us/step - loss: 1.2050 - acc: 0.4602\n",
            "Epoch 62/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1843 - acc: 0.5183\n",
            "Epoch 63/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.1933 - acc: 0.4817\n",
            "Epoch 64/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.2053 - acc: 0.4839\n",
            "Epoch 65/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1770 - acc: 0.4903\n",
            "Epoch 66/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1729 - acc: 0.5032\n",
            "Epoch 67/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1924 - acc: 0.5075\n",
            "Epoch 68/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1682 - acc: 0.5161\n",
            "Epoch 69/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1947 - acc: 0.5097\n",
            "Epoch 70/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1932 - acc: 0.5032\n",
            "Epoch 71/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1681 - acc: 0.5226\n",
            "Epoch 72/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1618 - acc: 0.4989\n",
            "Epoch 73/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1885 - acc: 0.5204\n",
            "Epoch 74/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.2033 - acc: 0.4796\n",
            "Epoch 75/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1609 - acc: 0.4753\n",
            "Epoch 76/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1769 - acc: 0.4882\n",
            "Epoch 77/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1703 - acc: 0.5118\n",
            "Epoch 78/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.1627 - acc: 0.5054\n",
            "Epoch 79/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1802 - acc: 0.4925\n",
            "Epoch 80/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1494 - acc: 0.4968\n",
            "Epoch 81/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1647 - acc: 0.5054\n",
            "Epoch 82/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1833 - acc: 0.4882\n",
            "Epoch 83/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1831 - acc: 0.4968\n",
            "Epoch 84/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1653 - acc: 0.5484\n",
            "Epoch 85/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1449 - acc: 0.5204\n",
            "Epoch 86/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.2108 - acc: 0.4817\n",
            "Epoch 87/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1711 - acc: 0.4989\n",
            "Epoch 88/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1746 - acc: 0.4968\n",
            "Epoch 89/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1633 - acc: 0.5118\n",
            "Epoch 90/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1471 - acc: 0.5140\n",
            "Epoch 91/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1913 - acc: 0.4860\n",
            "Epoch 92/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1808 - acc: 0.4817\n",
            "Epoch 93/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1784 - acc: 0.4860\n",
            "Epoch 94/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1228 - acc: 0.4989\n",
            "Epoch 95/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.2187 - acc: 0.4796\n",
            "Epoch 96/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1835 - acc: 0.4946\n",
            "Epoch 97/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1878 - acc: 0.4968\n",
            "Epoch 98/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1688 - acc: 0.4796\n",
            "Epoch 99/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1432 - acc: 0.5118\n",
            "Epoch 100/250\n",
            "465/465 [==============================] - 0s 68us/step - loss: 1.1601 - acc: 0.4774\n",
            "Epoch 101/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1798 - acc: 0.5118\n",
            "Epoch 102/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1528 - acc: 0.4968\n",
            "Epoch 103/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1544 - acc: 0.5118\n",
            "Epoch 104/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1526 - acc: 0.5054\n",
            "Epoch 105/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1742 - acc: 0.5118\n",
            "Epoch 106/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1846 - acc: 0.5118\n",
            "Epoch 107/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1476 - acc: 0.5269\n",
            "Epoch 108/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1646 - acc: 0.5183\n",
            "Epoch 109/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1297 - acc: 0.4903\n",
            "Epoch 110/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1419 - acc: 0.5183\n",
            "Epoch 111/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.1568 - acc: 0.4989\n",
            "Epoch 112/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1698 - acc: 0.5247\n",
            "Epoch 113/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1784 - acc: 0.4903\n",
            "Epoch 114/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1728 - acc: 0.5097\n",
            "Epoch 115/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1938 - acc: 0.4731\n",
            "Epoch 116/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1314 - acc: 0.5118\n",
            "Epoch 117/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1379 - acc: 0.5290\n",
            "Epoch 118/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1500 - acc: 0.5333\n",
            "Epoch 119/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1769 - acc: 0.5140\n",
            "Epoch 120/250\n",
            "465/465 [==============================] - 0s 45us/step - loss: 1.2046 - acc: 0.4903\n",
            "Epoch 121/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1752 - acc: 0.4839\n",
            "Epoch 122/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1373 - acc: 0.5140\n",
            "Epoch 123/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1461 - acc: 0.5161\n",
            "Epoch 124/250\n",
            "465/465 [==============================] - 0s 57us/step - loss: 1.1669 - acc: 0.4903\n",
            "Epoch 125/250\n",
            "465/465 [==============================] - 0s 45us/step - loss: 1.1594 - acc: 0.5118\n",
            "Epoch 126/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.2222 - acc: 0.4860\n",
            "Epoch 127/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1566 - acc: 0.5441\n",
            "Epoch 128/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1544 - acc: 0.4710\n",
            "Epoch 129/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1700 - acc: 0.4925\n",
            "Epoch 130/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1459 - acc: 0.5247\n",
            "Epoch 131/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1548 - acc: 0.4925\n",
            "Epoch 132/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1695 - acc: 0.4968\n",
            "Epoch 133/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1323 - acc: 0.4946\n",
            "Epoch 134/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1705 - acc: 0.5011\n",
            "Epoch 135/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1426 - acc: 0.5269\n",
            "Epoch 136/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1789 - acc: 0.4860\n",
            "Epoch 137/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1662 - acc: 0.5333\n",
            "Epoch 138/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1390 - acc: 0.5075\n",
            "Epoch 139/250\n",
            "465/465 [==============================] - 0s 71us/step - loss: 1.2010 - acc: 0.4989\n",
            "Epoch 140/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.1626 - acc: 0.5054\n",
            "Epoch 141/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1633 - acc: 0.5118\n",
            "Epoch 142/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1538 - acc: 0.5161\n",
            "Epoch 143/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1763 - acc: 0.4860\n",
            "Epoch 144/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1658 - acc: 0.5183\n",
            "Epoch 145/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1602 - acc: 0.5204\n",
            "Epoch 146/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1609 - acc: 0.5161\n",
            "Epoch 147/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1696 - acc: 0.5011\n",
            "Epoch 148/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1577 - acc: 0.5054\n",
            "Epoch 149/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1864 - acc: 0.4925\n",
            "Epoch 150/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1658 - acc: 0.5032\n",
            "Epoch 151/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1500 - acc: 0.5032\n",
            "Epoch 152/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1382 - acc: 0.5226\n",
            "Epoch 153/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1411 - acc: 0.5204\n",
            "Epoch 154/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1446 - acc: 0.5075\n",
            "Epoch 155/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1522 - acc: 0.4796\n",
            "Epoch 156/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1422 - acc: 0.5183\n",
            "Epoch 157/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1736 - acc: 0.5183\n",
            "Epoch 158/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1741 - acc: 0.5075\n",
            "Epoch 159/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1585 - acc: 0.5118\n",
            "Epoch 160/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1443 - acc: 0.4989\n",
            "Epoch 161/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1722 - acc: 0.5097\n",
            "Epoch 162/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1371 - acc: 0.5355\n",
            "Epoch 163/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1689 - acc: 0.4882\n",
            "Epoch 164/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1644 - acc: 0.5226\n",
            "Epoch 165/250\n",
            "465/465 [==============================] - 0s 61us/step - loss: 1.1288 - acc: 0.5032\n",
            "Epoch 166/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1442 - acc: 0.5204\n",
            "Epoch 167/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1421 - acc: 0.5419\n",
            "Epoch 168/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1276 - acc: 0.5204\n",
            "Epoch 169/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1613 - acc: 0.4946\n",
            "Epoch 170/250\n",
            "465/465 [==============================] - 0s 77us/step - loss: 1.1465 - acc: 0.5118\n",
            "Epoch 171/250\n",
            "465/465 [==============================] - 0s 58us/step - loss: 1.1669 - acc: 0.5011\n",
            "Epoch 172/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1477 - acc: 0.4903\n",
            "Epoch 173/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1817 - acc: 0.4968\n",
            "Epoch 174/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1474 - acc: 0.5054\n",
            "Epoch 175/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1523 - acc: 0.5032\n",
            "Epoch 176/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1495 - acc: 0.4882\n",
            "Epoch 177/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1339 - acc: 0.5183\n",
            "Epoch 178/250\n",
            "465/465 [==============================] - 0s 62us/step - loss: 1.1566 - acc: 0.5140\n",
            "Epoch 179/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1343 - acc: 0.5097\n",
            "Epoch 180/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1202 - acc: 0.5075\n",
            "Epoch 181/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1574 - acc: 0.5140\n",
            "Epoch 182/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1890 - acc: 0.4946\n",
            "Epoch 183/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1548 - acc: 0.5032\n",
            "Epoch 184/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1344 - acc: 0.5032\n",
            "Epoch 185/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1476 - acc: 0.5075\n",
            "Epoch 186/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1574 - acc: 0.5075\n",
            "Epoch 187/250\n",
            "465/465 [==============================] - 0s 58us/step - loss: 1.1811 - acc: 0.5118\n",
            "Epoch 188/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1218 - acc: 0.5376\n",
            "Epoch 189/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1805 - acc: 0.5183\n",
            "Epoch 190/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1441 - acc: 0.4796\n",
            "Epoch 191/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1457 - acc: 0.5226\n",
            "Epoch 192/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1522 - acc: 0.5140\n",
            "Epoch 193/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1497 - acc: 0.4968\n",
            "Epoch 194/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1689 - acc: 0.5161\n",
            "Epoch 195/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.1328 - acc: 0.5376\n",
            "Epoch 196/250\n",
            "465/465 [==============================] - 0s 44us/step - loss: 1.1269 - acc: 0.5376\n",
            "Epoch 197/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1427 - acc: 0.5290\n",
            "Epoch 198/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1370 - acc: 0.4989\n",
            "Epoch 199/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1503 - acc: 0.5290\n",
            "Epoch 200/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1508 - acc: 0.5075\n",
            "Epoch 201/250\n",
            "465/465 [==============================] - 0s 58us/step - loss: 1.1663 - acc: 0.5269\n",
            "Epoch 202/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1677 - acc: 0.5247\n",
            "Epoch 203/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1675 - acc: 0.5011\n",
            "Epoch 204/250\n",
            "465/465 [==============================] - 0s 45us/step - loss: 1.1192 - acc: 0.5290\n",
            "Epoch 205/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1522 - acc: 0.5290\n",
            "Epoch 206/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1553 - acc: 0.5290\n",
            "Epoch 207/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1545 - acc: 0.5161\n",
            "Epoch 208/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1625 - acc: 0.5484\n",
            "Epoch 209/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1127 - acc: 0.5333\n",
            "Epoch 210/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1281 - acc: 0.5355\n",
            "Epoch 211/250\n",
            "465/465 [==============================] - 0s 54us/step - loss: 1.1201 - acc: 0.5376\n",
            "Epoch 212/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1562 - acc: 0.5226\n",
            "Epoch 213/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1742 - acc: 0.5204\n",
            "Epoch 214/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1508 - acc: 0.5032\n",
            "Epoch 215/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1352 - acc: 0.5054\n",
            "Epoch 216/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1656 - acc: 0.5161\n",
            "Epoch 217/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1700 - acc: 0.5183\n",
            "Epoch 218/250\n",
            "465/465 [==============================] - 0s 64us/step - loss: 1.1373 - acc: 0.5226\n",
            "Epoch 219/250\n",
            "465/465 [==============================] - 0s 71us/step - loss: 1.1171 - acc: 0.5484\n",
            "Epoch 220/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1546 - acc: 0.5333\n",
            "Epoch 221/250\n",
            "465/465 [==============================] - 0s 48us/step - loss: 1.1567 - acc: 0.5355\n",
            "Epoch 222/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1626 - acc: 0.5226\n",
            "Epoch 223/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1321 - acc: 0.5355\n",
            "Epoch 224/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1490 - acc: 0.5398\n",
            "Epoch 225/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1345 - acc: 0.5398\n",
            "Epoch 226/250\n",
            "465/465 [==============================] - 0s 51us/step - loss: 1.1572 - acc: 0.5376\n",
            "Epoch 227/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1343 - acc: 0.5376\n",
            "Epoch 228/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1291 - acc: 0.5226\n",
            "Epoch 229/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1572 - acc: 0.5312\n",
            "Epoch 230/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1393 - acc: 0.5290\n",
            "Epoch 231/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1541 - acc: 0.5419\n",
            "Epoch 232/250\n",
            "465/465 [==============================] - 0s 50us/step - loss: 1.1426 - acc: 0.5376\n",
            "Epoch 233/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1295 - acc: 0.5355\n",
            "Epoch 234/250\n",
            "465/465 [==============================] - 0s 59us/step - loss: 1.1496 - acc: 0.5419\n",
            "Epoch 235/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1276 - acc: 0.5355\n",
            "Epoch 236/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1508 - acc: 0.5355\n",
            "Epoch 237/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1399 - acc: 0.5333\n",
            "Epoch 238/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1155 - acc: 0.5484\n",
            "Epoch 239/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1616 - acc: 0.5269\n",
            "Epoch 240/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1320 - acc: 0.5355\n",
            "Epoch 241/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1184 - acc: 0.5290\n",
            "Epoch 242/250\n",
            "465/465 [==============================] - 0s 52us/step - loss: 1.1295 - acc: 0.5118\n",
            "Epoch 243/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1284 - acc: 0.5097\n",
            "Epoch 244/250\n",
            "465/465 [==============================] - 0s 60us/step - loss: 1.1532 - acc: 0.4925\n",
            "Epoch 245/250\n",
            "465/465 [==============================] - 0s 53us/step - loss: 1.1554 - acc: 0.5376\n",
            "Epoch 246/250\n",
            "465/465 [==============================] - 0s 47us/step - loss: 1.1267 - acc: 0.5333\n",
            "Epoch 247/250\n",
            "465/465 [==============================] - 0s 49us/step - loss: 1.1428 - acc: 0.5398\n",
            "Epoch 248/250\n",
            "465/465 [==============================] - 0s 46us/step - loss: 1.1592 - acc: 0.5355\n",
            "Epoch 249/250\n",
            "465/465 [==============================] - 0s 56us/step - loss: 1.1342 - acc: 0.5333\n",
            "Epoch 250/250\n",
            "465/465 [==============================] - 0s 55us/step - loss: 1.1764 - acc: 0.5333\n",
            "465/465 [==============================] - 0s 26us/step\n",
            "Test loss: 1.7484582413909255\n",
            "Test accuracy: 0.39569892409027263\n",
            "Test loss: 1.7484582413909255\n",
            "Test accuracy: 0.39569892409027263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 15/22 [02:11<01:00,  8.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.4021505376344086 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        98\n",
            "         1.0       0.00      0.00      0.00       122\n",
            "         2.0       0.40      1.00      0.57       187\n",
            "         3.0       0.00      0.00      0.00        24\n",
            "         4.0       0.00      0.00      0.00        34\n",
            "\n",
            "   micro avg       0.40      0.40      0.40       465\n",
            "   macro avg       0.08      0.20      0.11       465\n",
            "weighted avg       0.16      0.40      0.23       465\n",
            "\n",
            "[[  0   0  98   0   0]\n",
            " [  0   0 122   0   0]\n",
            " [  0   0 187   0   0]\n",
            " [  0   0  24   0   0]\n",
            " [  0   0  34   0   0]]\n",
            "[[ 32   0   0   0  24]\n",
            " [  0   0   4   0  45]\n",
            " [  0   0 250  12   0]\n",
            " [  0   0   2  19   0]\n",
            " [  0   0   1   0  76]]\n",
            "Epoch 1/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 1.0124 - acc: 0.6412\n",
            "Epoch 2/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.9987 - acc: 0.6582\n",
            "Epoch 3/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.9821 - acc: 0.6561\n",
            "Epoch 4/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.9846 - acc: 0.6603\n",
            "Epoch 5/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.9669 - acc: 0.6603\n",
            "Epoch 6/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9595 - acc: 0.6539\n",
            "Epoch 7/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9762 - acc: 0.6645\n",
            "Epoch 8/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.9646 - acc: 0.6603\n",
            "Epoch 9/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.9195 - acc: 0.6688\n",
            "Epoch 10/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.9466 - acc: 0.6645\n",
            "Epoch 11/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9403 - acc: 0.6667\n",
            "Epoch 12/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.9351 - acc: 0.6624\n",
            "Epoch 13/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9466 - acc: 0.6624\n",
            "Epoch 14/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.9508 - acc: 0.6688\n",
            "Epoch 15/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9534 - acc: 0.6582\n",
            "Epoch 16/250\n",
            "471/471 [==============================] - 0s 45us/step - loss: 0.9392 - acc: 0.6603\n",
            "Epoch 17/250\n",
            "471/471 [==============================] - 0s 59us/step - loss: 0.9348 - acc: 0.6645\n",
            "Epoch 18/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9524 - acc: 0.6624\n",
            "Epoch 19/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9094 - acc: 0.6582\n",
            "Epoch 20/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.9414 - acc: 0.6645\n",
            "Epoch 21/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.9280 - acc: 0.6709\n",
            "Epoch 22/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.9196 - acc: 0.6645\n",
            "Epoch 23/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.9146 - acc: 0.6730\n",
            "Epoch 24/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.9343 - acc: 0.6730\n",
            "Epoch 25/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9249 - acc: 0.6709\n",
            "Epoch 26/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9247 - acc: 0.6667\n",
            "Epoch 27/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.9137 - acc: 0.6645\n",
            "Epoch 28/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9184 - acc: 0.6688\n",
            "Epoch 29/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.9309 - acc: 0.6688\n",
            "Epoch 30/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9144 - acc: 0.6667\n",
            "Epoch 31/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.9285 - acc: 0.6645\n",
            "Epoch 32/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9019 - acc: 0.6709\n",
            "Epoch 33/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9051 - acc: 0.6730\n",
            "Epoch 34/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.9232 - acc: 0.6645\n",
            "Epoch 35/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9053 - acc: 0.6667\n",
            "Epoch 36/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.9124 - acc: 0.6497\n",
            "Epoch 37/250\n",
            "471/471 [==============================] - 0s 66us/step - loss: 0.9027 - acc: 0.6624\n",
            "Epoch 38/250\n",
            "471/471 [==============================] - 0s 66us/step - loss: 0.9099 - acc: 0.6709\n",
            "Epoch 39/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.9148 - acc: 0.6688\n",
            "Epoch 40/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9201 - acc: 0.6688\n",
            "Epoch 41/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.9316 - acc: 0.6667\n",
            "Epoch 42/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8830 - acc: 0.6752\n",
            "Epoch 43/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9016 - acc: 0.6688\n",
            "Epoch 44/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9050 - acc: 0.6603\n",
            "Epoch 45/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8808 - acc: 0.6730\n",
            "Epoch 46/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9094 - acc: 0.6603\n",
            "Epoch 47/250\n",
            "471/471 [==============================] - 0s 72us/step - loss: 0.9273 - acc: 0.6709\n",
            "Epoch 48/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.9321 - acc: 0.6730\n",
            "Epoch 49/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8841 - acc: 0.6730\n",
            "Epoch 50/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.9097 - acc: 0.6688\n",
            "Epoch 51/250\n",
            "471/471 [==============================] - 0s 59us/step - loss: 0.8767 - acc: 0.6709\n",
            "Epoch 52/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8857 - acc: 0.6730\n",
            "Epoch 53/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8864 - acc: 0.6709\n",
            "Epoch 54/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8988 - acc: 0.6709\n",
            "Epoch 55/250\n",
            "471/471 [==============================] - 0s 63us/step - loss: 0.8892 - acc: 0.6752\n",
            "Epoch 56/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.9145 - acc: 0.6709\n",
            "Epoch 57/250\n",
            "471/471 [==============================] - 0s 59us/step - loss: 0.8929 - acc: 0.6709\n",
            "Epoch 58/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8820 - acc: 0.6730\n",
            "Epoch 59/250\n",
            "471/471 [==============================] - 0s 64us/step - loss: 0.9096 - acc: 0.6709\n",
            "Epoch 60/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.9290 - acc: 0.6645\n",
            "Epoch 61/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9034 - acc: 0.6709\n",
            "Epoch 62/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.8930 - acc: 0.6709\n",
            "Epoch 63/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.8878 - acc: 0.6688\n",
            "Epoch 64/250\n",
            "471/471 [==============================] - 0s 61us/step - loss: 0.8777 - acc: 0.6709\n",
            "Epoch 65/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8915 - acc: 0.6667\n",
            "Epoch 66/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8935 - acc: 0.6709\n",
            "Epoch 67/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.9122 - acc: 0.6688\n",
            "Epoch 68/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8844 - acc: 0.6752\n",
            "Epoch 69/250\n",
            "471/471 [==============================] - 0s 67us/step - loss: 0.8941 - acc: 0.6752\n",
            "Epoch 70/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.9122 - acc: 0.6709\n",
            "Epoch 71/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8949 - acc: 0.6688\n",
            "Epoch 72/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.8732 - acc: 0.6709\n",
            "Epoch 73/250\n",
            "471/471 [==============================] - 0s 65us/step - loss: 0.8973 - acc: 0.6730\n",
            "Epoch 74/250\n",
            "471/471 [==============================] - 0s 61us/step - loss: 0.8853 - acc: 0.6730\n",
            "Epoch 75/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.9084 - acc: 0.6709\n",
            "Epoch 76/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8751 - acc: 0.6752\n",
            "Epoch 77/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8815 - acc: 0.6709\n",
            "Epoch 78/250\n",
            "471/471 [==============================] - 0s 63us/step - loss: 0.8950 - acc: 0.6709\n",
            "Epoch 79/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8900 - acc: 0.6709\n",
            "Epoch 80/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8929 - acc: 0.6709\n",
            "Epoch 81/250\n",
            "471/471 [==============================] - 0s 64us/step - loss: 0.8688 - acc: 0.6730\n",
            "Epoch 82/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.9226 - acc: 0.6624\n",
            "Epoch 83/250\n",
            "471/471 [==============================] - 0s 62us/step - loss: 0.9112 - acc: 0.6709\n",
            "Epoch 84/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8902 - acc: 0.6688\n",
            "Epoch 85/250\n",
            "471/471 [==============================] - 0s 58us/step - loss: 0.8792 - acc: 0.6730\n",
            "Epoch 86/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8947 - acc: 0.6730\n",
            "Epoch 87/250\n",
            "471/471 [==============================] - 0s 59us/step - loss: 0.8712 - acc: 0.6709\n",
            "Epoch 88/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8835 - acc: 0.6730\n",
            "Epoch 89/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8803 - acc: 0.6688\n",
            "Epoch 90/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8661 - acc: 0.6730\n",
            "Epoch 91/250\n",
            "471/471 [==============================] - 0s 59us/step - loss: 0.8684 - acc: 0.6730\n",
            "Epoch 92/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8941 - acc: 0.6709\n",
            "Epoch 93/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8837 - acc: 0.6752\n",
            "Epoch 94/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8857 - acc: 0.6752\n",
            "Epoch 95/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8695 - acc: 0.6730\n",
            "Epoch 96/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8813 - acc: 0.6752\n",
            "Epoch 97/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8995 - acc: 0.6730\n",
            "Epoch 98/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9013 - acc: 0.6688\n",
            "Epoch 99/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8987 - acc: 0.6709\n",
            "Epoch 100/250\n",
            "471/471 [==============================] - 0s 45us/step - loss: 0.8872 - acc: 0.6730\n",
            "Epoch 101/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.9040 - acc: 0.6709\n",
            "Epoch 102/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8764 - acc: 0.6752\n",
            "Epoch 103/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8830 - acc: 0.6752\n",
            "Epoch 104/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8812 - acc: 0.6730\n",
            "Epoch 105/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8966 - acc: 0.6709\n",
            "Epoch 106/250\n",
            "471/471 [==============================] - 0s 42us/step - loss: 0.8684 - acc: 0.6730\n",
            "Epoch 107/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.9053 - acc: 0.6667\n",
            "Epoch 108/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8975 - acc: 0.6730\n",
            "Epoch 109/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.8890 - acc: 0.6752\n",
            "Epoch 110/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8766 - acc: 0.6730\n",
            "Epoch 111/250\n",
            "471/471 [==============================] - 0s 76us/step - loss: 0.8819 - acc: 0.6752\n",
            "Epoch 112/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8938 - acc: 0.6688\n",
            "Epoch 113/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8919 - acc: 0.6688\n",
            "Epoch 114/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8866 - acc: 0.6730\n",
            "Epoch 115/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8688 - acc: 0.6709\n",
            "Epoch 116/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.8774 - acc: 0.6752\n",
            "Epoch 117/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8836 - acc: 0.6752\n",
            "Epoch 118/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8950 - acc: 0.6730\n",
            "Epoch 119/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8888 - acc: 0.6752\n",
            "Epoch 120/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8875 - acc: 0.6752\n",
            "Epoch 121/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.8798 - acc: 0.6730\n",
            "Epoch 122/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8796 - acc: 0.6730\n",
            "Epoch 123/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.9066 - acc: 0.6709\n",
            "Epoch 124/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8786 - acc: 0.6730\n",
            "Epoch 125/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8869 - acc: 0.6730\n",
            "Epoch 126/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8987 - acc: 0.6709\n",
            "Epoch 127/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8802 - acc: 0.6730\n",
            "Epoch 128/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8864 - acc: 0.6688\n",
            "Epoch 129/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8722 - acc: 0.6752\n",
            "Epoch 130/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8785 - acc: 0.6709\n",
            "Epoch 131/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8715 - acc: 0.6752\n",
            "Epoch 132/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8778 - acc: 0.6752\n",
            "Epoch 133/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8594 - acc: 0.6709\n",
            "Epoch 134/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8658 - acc: 0.6709\n",
            "Epoch 135/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8718 - acc: 0.6709\n",
            "Epoch 136/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8900 - acc: 0.6709\n",
            "Epoch 137/250\n",
            "471/471 [==============================] - 0s 45us/step - loss: 0.8814 - acc: 0.6752\n",
            "Epoch 138/250\n",
            "471/471 [==============================] - 0s 45us/step - loss: 0.8888 - acc: 0.6709\n",
            "Epoch 139/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8613 - acc: 0.6752\n",
            "Epoch 140/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8848 - acc: 0.6688\n",
            "Epoch 141/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8704 - acc: 0.6752\n",
            "Epoch 142/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8542 - acc: 0.6730\n",
            "Epoch 143/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8671 - acc: 0.6752\n",
            "Epoch 144/250\n",
            "471/471 [==============================] - 0s 45us/step - loss: 0.8751 - acc: 0.6688\n",
            "Epoch 145/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8633 - acc: 0.6752\n",
            "Epoch 146/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8893 - acc: 0.6688\n",
            "Epoch 147/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8856 - acc: 0.6730\n",
            "Epoch 148/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8663 - acc: 0.6752\n",
            "Epoch 149/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8782 - acc: 0.6688\n",
            "Epoch 150/250\n",
            "471/471 [==============================] - 0s 71us/step - loss: 0.8730 - acc: 0.6688\n",
            "Epoch 151/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8602 - acc: 0.6709\n",
            "Epoch 152/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8689 - acc: 0.6752\n",
            "Epoch 153/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8673 - acc: 0.6709\n",
            "Epoch 154/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8669 - acc: 0.6709\n",
            "Epoch 155/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8661 - acc: 0.6752\n",
            "Epoch 156/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8745 - acc: 0.6752\n",
            "Epoch 157/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8782 - acc: 0.6709\n",
            "Epoch 158/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8631 - acc: 0.6730\n",
            "Epoch 159/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.8679 - acc: 0.6688\n",
            "Epoch 160/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8899 - acc: 0.6709\n",
            "Epoch 161/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8662 - acc: 0.6752\n",
            "Epoch 162/250\n",
            "471/471 [==============================] - 0s 43us/step - loss: 0.8780 - acc: 0.6730\n",
            "Epoch 163/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8719 - acc: 0.6730\n",
            "Epoch 164/250\n",
            "471/471 [==============================] - 0s 64us/step - loss: 0.8596 - acc: 0.6730\n",
            "Epoch 165/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.8651 - acc: 0.6752\n",
            "Epoch 166/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8985 - acc: 0.6730\n",
            "Epoch 167/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8661 - acc: 0.6730\n",
            "Epoch 168/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8682 - acc: 0.6709\n",
            "Epoch 169/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8842 - acc: 0.6730\n",
            "Epoch 170/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8796 - acc: 0.6730\n",
            "Epoch 171/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8697 - acc: 0.6730\n",
            "Epoch 172/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8678 - acc: 0.6688\n",
            "Epoch 173/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8859 - acc: 0.6730\n",
            "Epoch 174/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.8700 - acc: 0.6752\n",
            "Epoch 175/250\n",
            "471/471 [==============================] - 0s 58us/step - loss: 0.8852 - acc: 0.6752\n",
            "Epoch 176/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8666 - acc: 0.6752\n",
            "Epoch 177/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8710 - acc: 0.6752\n",
            "Epoch 178/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8793 - acc: 0.6752\n",
            "Epoch 179/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8738 - acc: 0.6709\n",
            "Epoch 180/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8848 - acc: 0.6730\n",
            "Epoch 181/250\n",
            "471/471 [==============================] - 0s 43us/step - loss: 0.8640 - acc: 0.6730\n",
            "Epoch 182/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8854 - acc: 0.6752\n",
            "Epoch 183/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8562 - acc: 0.6709\n",
            "Epoch 184/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8547 - acc: 0.6752\n",
            "Epoch 185/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8826 - acc: 0.6730\n",
            "Epoch 186/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8698 - acc: 0.6752\n",
            "Epoch 187/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8691 - acc: 0.6730\n",
            "Epoch 188/250\n",
            "471/471 [==============================] - 0s 43us/step - loss: 0.8710 - acc: 0.6752\n",
            "Epoch 189/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8732 - acc: 0.6752\n",
            "Epoch 190/250\n",
            "471/471 [==============================] - 0s 77us/step - loss: 0.8649 - acc: 0.6752\n",
            "Epoch 191/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8680 - acc: 0.6709\n",
            "Epoch 192/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8786 - acc: 0.6709\n",
            "Epoch 193/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8593 - acc: 0.6752\n",
            "Epoch 194/250\n",
            "471/471 [==============================] - 0s 41us/step - loss: 0.8721 - acc: 0.6709\n",
            "Epoch 195/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8569 - acc: 0.6730\n",
            "Epoch 196/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8739 - acc: 0.6752\n",
            "Epoch 197/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8655 - acc: 0.6730\n",
            "Epoch 198/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8747 - acc: 0.6752\n",
            "Epoch 199/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.8557 - acc: 0.6709\n",
            "Epoch 200/250\n",
            "471/471 [==============================] - 0s 58us/step - loss: 0.8651 - acc: 0.6730\n",
            "Epoch 201/250\n",
            "471/471 [==============================] - 0s 56us/step - loss: 0.8820 - acc: 0.6752\n",
            "Epoch 202/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8701 - acc: 0.6730\n",
            "Epoch 203/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8709 - acc: 0.6730\n",
            "Epoch 204/250\n",
            "471/471 [==============================] - 0s 45us/step - loss: 0.8528 - acc: 0.6730\n",
            "Epoch 205/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8741 - acc: 0.6752\n",
            "Epoch 206/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8657 - acc: 0.6709\n",
            "Epoch 207/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8617 - acc: 0.6752\n",
            "Epoch 208/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8602 - acc: 0.6709\n",
            "Epoch 209/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8709 - acc: 0.6752\n",
            "Epoch 210/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8555 - acc: 0.6752\n",
            "Epoch 211/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8615 - acc: 0.6730\n",
            "Epoch 212/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8744 - acc: 0.6730\n",
            "Epoch 213/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8621 - acc: 0.6730\n",
            "Epoch 214/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8660 - acc: 0.6752\n",
            "Epoch 215/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8576 - acc: 0.6752\n",
            "Epoch 216/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8409 - acc: 0.6752\n",
            "Epoch 217/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8659 - acc: 0.6752\n",
            "Epoch 218/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8536 - acc: 0.6752\n",
            "Epoch 219/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8430 - acc: 0.6752\n",
            "Epoch 220/250\n",
            "471/471 [==============================] - 0s 44us/step - loss: 0.8602 - acc: 0.6730\n",
            "Epoch 221/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8584 - acc: 0.6730\n",
            "Epoch 222/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8469 - acc: 0.6730\n",
            "Epoch 223/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8619 - acc: 0.6709\n",
            "Epoch 224/250\n",
            "471/471 [==============================] - 0s 58us/step - loss: 0.8589 - acc: 0.6730\n",
            "Epoch 225/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8660 - acc: 0.6709\n",
            "Epoch 226/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8572 - acc: 0.6730\n",
            "Epoch 227/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8779 - acc: 0.6752\n",
            "Epoch 228/250\n",
            "471/471 [==============================] - 0s 60us/step - loss: 0.8647 - acc: 0.6730\n",
            "Epoch 229/250\n",
            "471/471 [==============================] - 0s 71us/step - loss: 0.8548 - acc: 0.6730\n",
            "Epoch 230/250\n",
            "471/471 [==============================] - 0s 53us/step - loss: 0.8806 - acc: 0.6752\n",
            "Epoch 231/250\n",
            "471/471 [==============================] - 0s 59us/step - loss: 0.8739 - acc: 0.6730\n",
            "Epoch 232/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8493 - acc: 0.6730\n",
            "Epoch 233/250\n",
            "471/471 [==============================] - 0s 54us/step - loss: 0.8563 - acc: 0.6688\n",
            "Epoch 234/250\n",
            "471/471 [==============================] - 0s 49us/step - loss: 0.8570 - acc: 0.6752\n",
            "Epoch 235/250\n",
            "471/471 [==============================] - 0s 48us/step - loss: 0.8688 - acc: 0.6752\n",
            "Epoch 236/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8809 - acc: 0.6730\n",
            "Epoch 237/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8440 - acc: 0.6730\n",
            "Epoch 238/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8608 - acc: 0.6730\n",
            "Epoch 239/250\n",
            "471/471 [==============================] - 0s 52us/step - loss: 0.8783 - acc: 0.6709\n",
            "Epoch 240/250\n",
            "471/471 [==============================] - 0s 57us/step - loss: 0.8556 - acc: 0.6752\n",
            "Epoch 241/250\n",
            "471/471 [==============================] - 0s 46us/step - loss: 0.8772 - acc: 0.6730\n",
            "Epoch 242/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8635 - acc: 0.6752\n",
            "Epoch 243/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8611 - acc: 0.6752\n",
            "Epoch 244/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8545 - acc: 0.6752\n",
            "Epoch 245/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8550 - acc: 0.6709\n",
            "Epoch 246/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8606 - acc: 0.6730\n",
            "Epoch 247/250\n",
            "471/471 [==============================] - 0s 50us/step - loss: 0.8791 - acc: 0.6730\n",
            "Epoch 248/250\n",
            "471/471 [==============================] - 0s 55us/step - loss: 0.8506 - acc: 0.6730\n",
            "Epoch 249/250\n",
            "471/471 [==============================] - 0s 51us/step - loss: 0.8504 - acc: 0.6752\n",
            "Epoch 250/250\n",
            "471/471 [==============================] - 0s 47us/step - loss: 0.8663 - acc: 0.6730\n",
            "471/471 [==============================] - 0s 28us/step\n",
            "Test loss: 2.344037301221471\n",
            "Test accuracy: 0.4798301488730558\n",
            "Test loss: 2.344037301221471\n",
            "Test accuracy: 0.4798301488730558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 16/22 [02:19<00:50,  8.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.3481953290870488 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        96\n",
            "         1.0       0.00      0.00      0.00        82\n",
            "         2.0       0.35      1.00      0.52       164\n",
            "         3.0       0.00      0.00      0.00        54\n",
            "         4.0       0.00      0.00      0.00        75\n",
            "\n",
            "   micro avg       0.35      0.35      0.35       471\n",
            "   macro avg       0.07      0.20      0.10       471\n",
            "weighted avg       0.12      0.35      0.18       471\n",
            "\n",
            "[[  0   0  96   0   0]\n",
            " [  0   0  82   0   0]\n",
            " [  0   0 164   0   0]\n",
            " [  0   0  54   0   0]\n",
            " [  0   0  75   0   0]]\n",
            "[[ 44   0   0   0   0]\n",
            " [  3   0   6   0   0]\n",
            " [  0   0 300   0   0]\n",
            " [  0   0   8   0   0]\n",
            " [  0   0   4   0 106]]\n",
            "Epoch 1/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0997 - acc: 0.5358\n",
            "Epoch 2/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.1194 - acc: 0.5336\n",
            "Epoch 3/250\n",
            "461/461 [==============================] - 0s 43us/step - loss: 1.0882 - acc: 0.5293\n",
            "Epoch 4/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0731 - acc: 0.5358\n",
            "Epoch 5/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0776 - acc: 0.5315\n",
            "Epoch 6/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0507 - acc: 0.5315\n",
            "Epoch 7/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0634 - acc: 0.5315\n",
            "Epoch 8/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0491 - acc: 0.5249\n",
            "Epoch 9/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0477 - acc: 0.5293\n",
            "Epoch 10/250\n",
            "461/461 [==============================] - 0s 76us/step - loss: 1.0529 - acc: 0.5336\n",
            "Epoch 11/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0612 - acc: 0.5228\n",
            "Epoch 12/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0607 - acc: 0.5228\n",
            "Epoch 13/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0594 - acc: 0.5293\n",
            "Epoch 14/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0419 - acc: 0.5315\n",
            "Epoch 15/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0594 - acc: 0.5336\n",
            "Epoch 16/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0619 - acc: 0.5249\n",
            "Epoch 17/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0623 - acc: 0.5249\n",
            "Epoch 18/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0446 - acc: 0.5315\n",
            "Epoch 19/250\n",
            "461/461 [==============================] - 0s 62us/step - loss: 1.0572 - acc: 0.5336\n",
            "Epoch 20/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0654 - acc: 0.5249\n",
            "Epoch 21/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0529 - acc: 0.5336\n",
            "Epoch 22/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0286 - acc: 0.5315\n",
            "Epoch 23/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0419 - acc: 0.5293\n",
            "Epoch 24/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0529 - acc: 0.5315\n",
            "Epoch 25/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0317 - acc: 0.5293\n",
            "Epoch 26/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0416 - acc: 0.5336\n",
            "Epoch 27/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0562 - acc: 0.5336\n",
            "Epoch 28/250\n",
            "461/461 [==============================] - 0s 60us/step - loss: 1.0497 - acc: 0.5336\n",
            "Epoch 29/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0576 - acc: 0.5184\n",
            "Epoch 30/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0270 - acc: 0.5206\n",
            "Epoch 31/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0505 - acc: 0.5184\n",
            "Epoch 32/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0438 - acc: 0.5249\n",
            "Epoch 33/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0436 - acc: 0.5293\n",
            "Epoch 34/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0541 - acc: 0.5206\n",
            "Epoch 35/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0398 - acc: 0.5336\n",
            "Epoch 36/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0302 - acc: 0.5249\n",
            "Epoch 37/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0364 - acc: 0.5228\n",
            "Epoch 38/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0471 - acc: 0.5249\n",
            "Epoch 39/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0344 - acc: 0.5380\n",
            "Epoch 40/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0586 - acc: 0.5206\n",
            "Epoch 41/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0410 - acc: 0.5271\n",
            "Epoch 42/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0527 - acc: 0.5249\n",
            "Epoch 43/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0626 - acc: 0.5206\n",
            "Epoch 44/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0519 - acc: 0.5184\n",
            "Epoch 45/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0376 - acc: 0.5293\n",
            "Epoch 46/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0436 - acc: 0.5228\n",
            "Epoch 47/250\n",
            "461/461 [==============================] - 0s 58us/step - loss: 1.0554 - acc: 0.5293\n",
            "Epoch 48/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0497 - acc: 0.5315\n",
            "Epoch 49/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0593 - acc: 0.5206\n",
            "Epoch 50/250\n",
            "461/461 [==============================] - 0s 70us/step - loss: 1.0348 - acc: 0.5380\n",
            "Epoch 51/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0698 - acc: 0.5249\n",
            "Epoch 52/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0496 - acc: 0.5271\n",
            "Epoch 53/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0426 - acc: 0.5293\n",
            "Epoch 54/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0666 - acc: 0.5249\n",
            "Epoch 55/250\n",
            "461/461 [==============================] - 0s 44us/step - loss: 1.0432 - acc: 0.5293\n",
            "Epoch 56/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0427 - acc: 0.5293\n",
            "Epoch 57/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0560 - acc: 0.5271\n",
            "Epoch 58/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0253 - acc: 0.5358\n",
            "Epoch 59/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0500 - acc: 0.5249\n",
            "Epoch 60/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0404 - acc: 0.5249\n",
            "Epoch 61/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0645 - acc: 0.5206\n",
            "Epoch 62/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0476 - acc: 0.5336\n",
            "Epoch 63/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0541 - acc: 0.5358\n",
            "Epoch 64/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0331 - acc: 0.5358\n",
            "Epoch 65/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0294 - acc: 0.5315\n",
            "Epoch 66/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0235 - acc: 0.5336\n",
            "Epoch 67/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0515 - acc: 0.5271\n",
            "Epoch 68/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0423 - acc: 0.5315\n",
            "Epoch 69/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0374 - acc: 0.5163\n",
            "Epoch 70/250\n",
            "461/461 [==============================] - 0s 61us/step - loss: 1.0385 - acc: 0.5315\n",
            "Epoch 71/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0515 - acc: 0.5315\n",
            "Epoch 72/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0308 - acc: 0.5336\n",
            "Epoch 73/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0625 - acc: 0.5336\n",
            "Epoch 74/250\n",
            "461/461 [==============================] - 0s 43us/step - loss: 1.0529 - acc: 0.5293\n",
            "Epoch 75/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0203 - acc: 0.5336\n",
            "Epoch 76/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0472 - acc: 0.5315\n",
            "Epoch 77/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0374 - acc: 0.5315\n",
            "Epoch 78/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0486 - acc: 0.5315\n",
            "Epoch 79/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0530 - acc: 0.5336\n",
            "Epoch 80/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0508 - acc: 0.5336\n",
            "Epoch 81/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0588 - acc: 0.5293\n",
            "Epoch 82/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0465 - acc: 0.5336\n",
            "Epoch 83/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0328 - acc: 0.5315\n",
            "Epoch 84/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0624 - acc: 0.5336\n",
            "Epoch 85/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0304 - acc: 0.5315\n",
            "Epoch 86/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0495 - acc: 0.5315\n",
            "Epoch 87/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0364 - acc: 0.5358\n",
            "Epoch 88/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0386 - acc: 0.5336\n",
            "Epoch 89/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0529 - acc: 0.5315\n",
            "Epoch 90/250\n",
            "461/461 [==============================] - 0s 93us/step - loss: 1.0284 - acc: 0.5358\n",
            "Epoch 91/250\n",
            "461/461 [==============================] - 0s 58us/step - loss: 1.0395 - acc: 0.5358\n",
            "Epoch 92/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0458 - acc: 0.5358\n",
            "Epoch 93/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0306 - acc: 0.5358\n",
            "Epoch 94/250\n",
            "461/461 [==============================] - 0s 60us/step - loss: 1.0440 - acc: 0.5336\n",
            "Epoch 95/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0412 - acc: 0.5336\n",
            "Epoch 96/250\n",
            "461/461 [==============================] - 0s 58us/step - loss: 1.0384 - acc: 0.5358\n",
            "Epoch 97/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0451 - acc: 0.5358\n",
            "Epoch 98/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0485 - acc: 0.5315\n",
            "Epoch 99/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0561 - acc: 0.5293\n",
            "Epoch 100/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0434 - acc: 0.5336\n",
            "Epoch 101/250\n",
            "461/461 [==============================] - 0s 62us/step - loss: 1.0326 - acc: 0.5315\n",
            "Epoch 102/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0309 - acc: 0.5293\n",
            "Epoch 103/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0302 - acc: 0.5336\n",
            "Epoch 104/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0390 - acc: 0.5293\n",
            "Epoch 105/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0530 - acc: 0.5358\n",
            "Epoch 106/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0262 - acc: 0.5358\n",
            "Epoch 107/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0372 - acc: 0.5315\n",
            "Epoch 108/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0328 - acc: 0.5315\n",
            "Epoch 109/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0484 - acc: 0.5336\n",
            "Epoch 110/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0304 - acc: 0.5336\n",
            "Epoch 111/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0470 - acc: 0.5293\n",
            "Epoch 112/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0300 - acc: 0.5358\n",
            "Epoch 113/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0394 - acc: 0.5293\n",
            "Epoch 114/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0534 - acc: 0.5315\n",
            "Epoch 115/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0311 - acc: 0.5358\n",
            "Epoch 116/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0333 - acc: 0.5336\n",
            "Epoch 117/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0386 - acc: 0.5271\n",
            "Epoch 118/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0364 - acc: 0.5336\n",
            "Epoch 119/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0348 - acc: 0.5358\n",
            "Epoch 120/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0335 - acc: 0.5358\n",
            "Epoch 121/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0411 - acc: 0.5336\n",
            "Epoch 122/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0399 - acc: 0.5336\n",
            "Epoch 123/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0431 - acc: 0.5249\n",
            "Epoch 124/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0459 - acc: 0.5336\n",
            "Epoch 125/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0231 - acc: 0.5315\n",
            "Epoch 126/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0402 - acc: 0.5315\n",
            "Epoch 127/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0414 - acc: 0.5336\n",
            "Epoch 128/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0247 - acc: 0.5336\n",
            "Epoch 129/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0445 - acc: 0.5336\n",
            "Epoch 130/250\n",
            "461/461 [==============================] - 0s 71us/step - loss: 1.0591 - acc: 0.5358\n",
            "Epoch 131/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0419 - acc: 0.5358\n",
            "Epoch 132/250\n",
            "461/461 [==============================] - 0s 84us/step - loss: 1.0248 - acc: 0.5358\n",
            "Epoch 133/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0338 - acc: 0.5358\n",
            "Epoch 134/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0342 - acc: 0.5358\n",
            "Epoch 135/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0268 - acc: 0.5315\n",
            "Epoch 136/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0406 - acc: 0.5336\n",
            "Epoch 137/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0431 - acc: 0.5358\n",
            "Epoch 138/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0519 - acc: 0.5358\n",
            "Epoch 139/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0327 - acc: 0.5358\n",
            "Epoch 140/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0335 - acc: 0.5336\n",
            "Epoch 141/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0196 - acc: 0.5336\n",
            "Epoch 142/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0292 - acc: 0.5315\n",
            "Epoch 143/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0313 - acc: 0.5358\n",
            "Epoch 144/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0403 - acc: 0.5358\n",
            "Epoch 145/250\n",
            "461/461 [==============================] - 0s 65us/step - loss: 1.0408 - acc: 0.5358\n",
            "Epoch 146/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0401 - acc: 0.5293\n",
            "Epoch 147/250\n",
            "461/461 [==============================] - 0s 44us/step - loss: 1.0414 - acc: 0.5358\n",
            "Epoch 148/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0300 - acc: 0.5358\n",
            "Epoch 149/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0355 - acc: 0.5336\n",
            "Epoch 150/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0430 - acc: 0.5336\n",
            "Epoch 151/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0325 - acc: 0.5358\n",
            "Epoch 152/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0376 - acc: 0.5336\n",
            "Epoch 153/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0275 - acc: 0.5358\n",
            "Epoch 154/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0321 - acc: 0.5358\n",
            "Epoch 155/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0368 - acc: 0.5336\n",
            "Epoch 156/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0418 - acc: 0.5358\n",
            "Epoch 157/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0188 - acc: 0.5336\n",
            "Epoch 158/250\n",
            "461/461 [==============================] - 0s 57us/step - loss: 1.0414 - acc: 0.5358\n",
            "Epoch 159/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0393 - acc: 0.5358\n",
            "Epoch 160/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0566 - acc: 0.5336\n",
            "Epoch 161/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0391 - acc: 0.5358\n",
            "Epoch 162/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0355 - acc: 0.5358\n",
            "Epoch 163/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0422 - acc: 0.5358\n",
            "Epoch 164/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0330 - acc: 0.5336\n",
            "Epoch 165/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0228 - acc: 0.5358\n",
            "Epoch 166/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0223 - acc: 0.5358\n",
            "Epoch 167/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0221 - acc: 0.5358\n",
            "Epoch 168/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0424 - acc: 0.5358\n",
            "Epoch 169/250\n",
            "461/461 [==============================] - 0s 69us/step - loss: 1.0532 - acc: 0.5336\n",
            "Epoch 170/250\n",
            "461/461 [==============================] - 0s 62us/step - loss: 1.0323 - acc: 0.5358\n",
            "Epoch 171/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0437 - acc: 0.5358\n",
            "Epoch 172/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0353 - acc: 0.5336\n",
            "Epoch 173/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0362 - acc: 0.5358\n",
            "Epoch 174/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0418 - acc: 0.5358\n",
            "Epoch 175/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0243 - acc: 0.5358\n",
            "Epoch 176/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0430 - acc: 0.5358\n",
            "Epoch 177/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0313 - acc: 0.5336\n",
            "Epoch 178/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0367 - acc: 0.5358\n",
            "Epoch 179/250\n",
            "461/461 [==============================] - 0s 55us/step - loss: 1.0334 - acc: 0.5315\n",
            "Epoch 180/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0350 - acc: 0.5358\n",
            "Epoch 181/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0193 - acc: 0.5336\n",
            "Epoch 182/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0317 - acc: 0.5358\n",
            "Epoch 183/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0288 - acc: 0.5358\n",
            "Epoch 184/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0266 - acc: 0.5358\n",
            "Epoch 185/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0377 - acc: 0.5336\n",
            "Epoch 186/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0314 - acc: 0.5358\n",
            "Epoch 187/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0146 - acc: 0.5358\n",
            "Epoch 188/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0515 - acc: 0.5358\n",
            "Epoch 189/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0252 - acc: 0.5358\n",
            "Epoch 190/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0350 - acc: 0.5336\n",
            "Epoch 191/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0295 - acc: 0.5336\n",
            "Epoch 192/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0284 - acc: 0.5358\n",
            "Epoch 193/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0250 - acc: 0.5358\n",
            "Epoch 194/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0320 - acc: 0.5358\n",
            "Epoch 195/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0276 - acc: 0.5336\n",
            "Epoch 196/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0396 - acc: 0.5358\n",
            "Epoch 197/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0191 - acc: 0.5358\n",
            "Epoch 198/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0365 - acc: 0.5358\n",
            "Epoch 199/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0339 - acc: 0.5336\n",
            "Epoch 200/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0175 - acc: 0.5358\n",
            "Epoch 201/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0395 - acc: 0.5358\n",
            "Epoch 202/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0286 - acc: 0.5358\n",
            "Epoch 203/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0252 - acc: 0.5315\n",
            "Epoch 204/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0369 - acc: 0.5336\n",
            "Epoch 205/250\n",
            "461/461 [==============================] - 0s 45us/step - loss: 1.0318 - acc: 0.5358\n",
            "Epoch 206/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0282 - acc: 0.5358\n",
            "Epoch 207/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0389 - acc: 0.5358\n",
            "Epoch 208/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0264 - acc: 0.5336\n",
            "Epoch 209/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0254 - acc: 0.5358\n",
            "Epoch 210/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0339 - acc: 0.5358\n",
            "Epoch 211/250\n",
            "461/461 [==============================] - 0s 65us/step - loss: 1.0457 - acc: 0.5358\n",
            "Epoch 212/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0375 - acc: 0.5358\n",
            "Epoch 213/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0296 - acc: 0.5358\n",
            "Epoch 214/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0370 - acc: 0.5336\n",
            "Epoch 215/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0238 - acc: 0.5358\n",
            "Epoch 216/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0359 - acc: 0.5336\n",
            "Epoch 217/250\n",
            "461/461 [==============================] - 0s 58us/step - loss: 1.0310 - acc: 0.5336\n",
            "Epoch 218/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0321 - acc: 0.5358\n",
            "Epoch 219/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0274 - acc: 0.5358\n",
            "Epoch 220/250\n",
            "461/461 [==============================] - 0s 60us/step - loss: 1.0417 - acc: 0.5358\n",
            "Epoch 221/250\n",
            "461/461 [==============================] - 0s 54us/step - loss: 1.0371 - acc: 0.5358\n",
            "Epoch 222/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0330 - acc: 0.5358\n",
            "Epoch 223/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0404 - acc: 0.5315\n",
            "Epoch 224/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0183 - acc: 0.5358\n",
            "Epoch 225/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0377 - acc: 0.5336\n",
            "Epoch 226/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0326 - acc: 0.5358\n",
            "Epoch 227/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0307 - acc: 0.5358\n",
            "Epoch 228/250\n",
            "461/461 [==============================] - 0s 63us/step - loss: 1.0383 - acc: 0.5358\n",
            "Epoch 229/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0229 - acc: 0.5358\n",
            "Epoch 230/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0279 - acc: 0.5336\n",
            "Epoch 231/250\n",
            "461/461 [==============================] - 0s 48us/step - loss: 1.0258 - acc: 0.5358\n",
            "Epoch 232/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0332 - acc: 0.5336\n",
            "Epoch 233/250\n",
            "461/461 [==============================] - 0s 56us/step - loss: 1.0257 - acc: 0.5358\n",
            "Epoch 234/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0304 - acc: 0.5358\n",
            "Epoch 235/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0436 - acc: 0.5315\n",
            "Epoch 236/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0319 - acc: 0.5315\n",
            "Epoch 237/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0359 - acc: 0.5336\n",
            "Epoch 238/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0282 - acc: 0.5358\n",
            "Epoch 239/250\n",
            "461/461 [==============================] - 0s 49us/step - loss: 1.0430 - acc: 0.5358\n",
            "Epoch 240/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0344 - acc: 0.5358\n",
            "Epoch 241/250\n",
            "461/461 [==============================] - 0s 61us/step - loss: 1.0190 - acc: 0.5358\n",
            "Epoch 242/250\n",
            "461/461 [==============================] - 0s 47us/step - loss: 1.0290 - acc: 0.5358\n",
            "Epoch 243/250\n",
            "461/461 [==============================] - 0s 50us/step - loss: 1.0295 - acc: 0.5358\n",
            "Epoch 244/250\n",
            "461/461 [==============================] - 0s 59us/step - loss: 1.0189 - acc: 0.5358\n",
            "Epoch 245/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0377 - acc: 0.5358\n",
            "Epoch 246/250\n",
            "461/461 [==============================] - 0s 46us/step - loss: 1.0295 - acc: 0.5358\n",
            "Epoch 247/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0402 - acc: 0.5358\n",
            "Epoch 248/250\n",
            "461/461 [==============================] - 0s 52us/step - loss: 1.0434 - acc: 0.5358\n",
            "Epoch 249/250\n",
            "461/461 [==============================] - 0s 53us/step - loss: 1.0447 - acc: 0.5358\n",
            "Epoch 250/250\n",
            "461/461 [==============================] - 0s 51us/step - loss: 1.0262 - acc: 0.5315\n",
            "461/461 [==============================] - 0s 36us/step\n",
            "Test loss: 2.6909075067772523\n",
            "Test accuracy: 0.38828633599581275\n",
            "Test loss: 2.6909075067772523\n",
            "Test accuracy: 0.38828633599581275\n",
            "Seq Test accuracy score : 0.27331887201735355 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00       105\n",
            "         1.0       0.00      0.00      0.00       133\n",
            "         2.0       0.27      1.00      0.43       126\n",
            "         3.0       0.00      0.00      0.00        45\n",
            "         4.0       0.00      0.00      0.00        52\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       461\n",
            "   macro avg       0.05      0.20      0.09       461\n",
            "weighted avg       0.07      0.27      0.12       461\n",
            "\n",
            "[[  0   0 105   0   0]\n",
            " [  0   0 133   0   0]\n",
            " [  0   0 126   0   0]\n",
            " [  0   0  45   0   0]\n",
            " [  0   0  52   0   0]]\n",
            "[[ 95   0   0   0   0]\n",
            " [  3   0   8   0   9]\n",
            " [  0   0 227   0   0]\n",
            " [  0   0  11   0   0]\n",
            " [  0   0   1   0 107]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 17/22 [02:27<00:40,  8.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 2.5101 - acc: 0.3800\n",
            "Epoch 2/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 2.3375 - acc: 0.3820\n",
            "Epoch 3/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 2.1573 - acc: 0.3800\n",
            "Epoch 4/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 2.0206 - acc: 0.3800\n",
            "Epoch 5/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.8700 - acc: 0.3762\n",
            "Epoch 6/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.7801 - acc: 0.3762\n",
            "Epoch 7/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.6819 - acc: 0.3743\n",
            "Epoch 8/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.5745 - acc: 0.3800\n",
            "Epoch 9/250\n",
            "521/521 [==============================] - 0s 61us/step - loss: 1.5591 - acc: 0.3685\n",
            "Epoch 10/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.5031 - acc: 0.3724\n",
            "Epoch 11/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4780 - acc: 0.3628\n",
            "Epoch 12/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4956 - acc: 0.3589\n",
            "Epoch 13/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4336 - acc: 0.3800\n",
            "Epoch 14/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4504 - acc: 0.3608\n",
            "Epoch 15/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4587 - acc: 0.3436\n",
            "Epoch 16/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4069 - acc: 0.3743\n",
            "Epoch 17/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4292 - acc: 0.3589\n",
            "Epoch 18/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4591 - acc: 0.3512\n",
            "Epoch 19/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4458 - acc: 0.3378\n",
            "Epoch 20/250\n",
            "521/521 [==============================] - 0s 64us/step - loss: 1.4141 - acc: 0.3781\n",
            "Epoch 21/250\n",
            "521/521 [==============================] - 0s 58us/step - loss: 1.4324 - acc: 0.3589\n",
            "Epoch 22/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4131 - acc: 0.3781\n",
            "Epoch 23/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4321 - acc: 0.3666\n",
            "Epoch 24/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4311 - acc: 0.3397\n",
            "Epoch 25/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4459 - acc: 0.3436\n",
            "Epoch 26/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4408 - acc: 0.3474\n",
            "Epoch 27/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4260 - acc: 0.3551\n",
            "Epoch 28/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4108 - acc: 0.3589\n",
            "Epoch 29/250\n",
            "521/521 [==============================] - 0s 63us/step - loss: 1.4182 - acc: 0.3589\n",
            "Epoch 30/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4706 - acc: 0.3244\n",
            "Epoch 31/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4137 - acc: 0.3589\n",
            "Epoch 32/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4244 - acc: 0.3493\n",
            "Epoch 33/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4396 - acc: 0.3666\n",
            "Epoch 34/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4295 - acc: 0.3397\n",
            "Epoch 35/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4090 - acc: 0.3512\n",
            "Epoch 36/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4069 - acc: 0.3474\n",
            "Epoch 37/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4187 - acc: 0.3666\n",
            "Epoch 38/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4210 - acc: 0.3474\n",
            "Epoch 39/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4086 - acc: 0.3532\n",
            "Epoch 40/250\n",
            "521/521 [==============================] - 0s 58us/step - loss: 1.4060 - acc: 0.3340\n",
            "Epoch 41/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4220 - acc: 0.3647\n",
            "Epoch 42/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3870 - acc: 0.3608\n",
            "Epoch 43/250\n",
            "521/521 [==============================] - 0s 58us/step - loss: 1.3733 - acc: 0.3608\n",
            "Epoch 44/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4341 - acc: 0.3512\n",
            "Epoch 45/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4032 - acc: 0.3570\n",
            "Epoch 46/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4202 - acc: 0.3417\n",
            "Epoch 47/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3849 - acc: 0.3493\n",
            "Epoch 48/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4199 - acc: 0.3282\n",
            "Epoch 49/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4668 - acc: 0.3340\n",
            "Epoch 50/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4390 - acc: 0.3378\n",
            "Epoch 51/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4313 - acc: 0.3378\n",
            "Epoch 52/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4186 - acc: 0.3455\n",
            "Epoch 53/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4022 - acc: 0.3397\n",
            "Epoch 54/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3925 - acc: 0.3532\n",
            "Epoch 55/250\n",
            "521/521 [==============================] - 0s 72us/step - loss: 1.3985 - acc: 0.3474\n",
            "Epoch 56/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4190 - acc: 0.3512\n",
            "Epoch 57/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3873 - acc: 0.3551\n",
            "Epoch 58/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4186 - acc: 0.3378\n",
            "Epoch 59/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4430 - acc: 0.3225\n",
            "Epoch 60/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4109 - acc: 0.3359\n",
            "Epoch 61/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4323 - acc: 0.3417\n",
            "Epoch 62/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3884 - acc: 0.3589\n",
            "Epoch 63/250\n",
            "521/521 [==============================] - 0s 59us/step - loss: 1.4129 - acc: 0.3628\n",
            "Epoch 64/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4395 - acc: 0.3436\n",
            "Epoch 65/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4128 - acc: 0.3321\n",
            "Epoch 66/250\n",
            "521/521 [==============================] - 0s 61us/step - loss: 1.4168 - acc: 0.3378\n",
            "Epoch 67/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.4319 - acc: 0.3493\n",
            "Epoch 68/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4087 - acc: 0.3589\n",
            "Epoch 69/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4119 - acc: 0.3474\n",
            "Epoch 70/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4176 - acc: 0.3474\n",
            "Epoch 71/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4205 - acc: 0.3493\n",
            "Epoch 72/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3973 - acc: 0.3417\n",
            "Epoch 73/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4008 - acc: 0.3493\n",
            "Epoch 74/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3947 - acc: 0.3589\n",
            "Epoch 75/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4193 - acc: 0.3417\n",
            "Epoch 76/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3888 - acc: 0.3647\n",
            "Epoch 77/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4041 - acc: 0.3301\n",
            "Epoch 78/250\n",
            "521/521 [==============================] - 0s 47us/step - loss: 1.4036 - acc: 0.3186\n",
            "Epoch 79/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4174 - acc: 0.3263\n",
            "Epoch 80/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.3964 - acc: 0.3397\n",
            "Epoch 81/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4240 - acc: 0.3225\n",
            "Epoch 82/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4305 - acc: 0.3282\n",
            "Epoch 83/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4450 - acc: 0.3167\n",
            "Epoch 84/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4002 - acc: 0.3781\n",
            "Epoch 85/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4186 - acc: 0.3301\n",
            "Epoch 86/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3921 - acc: 0.3474\n",
            "Epoch 87/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4157 - acc: 0.3359\n",
            "Epoch 88/250\n",
            "521/521 [==============================] - 0s 47us/step - loss: 1.4237 - acc: 0.3417\n",
            "Epoch 89/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3781 - acc: 0.3512\n",
            "Epoch 90/250\n",
            "521/521 [==============================] - 0s 69us/step - loss: 1.4445 - acc: 0.3455\n",
            "Epoch 91/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4121 - acc: 0.3493\n",
            "Epoch 92/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4384 - acc: 0.3167\n",
            "Epoch 93/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4034 - acc: 0.3589\n",
            "Epoch 94/250\n",
            "521/521 [==============================] - 0s 63us/step - loss: 1.4234 - acc: 0.3474\n",
            "Epoch 95/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4180 - acc: 0.3359\n",
            "Epoch 96/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3862 - acc: 0.3417\n",
            "Epoch 97/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4048 - acc: 0.3340\n",
            "Epoch 98/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3763 - acc: 0.3551\n",
            "Epoch 99/250\n",
            "521/521 [==============================] - 0s 46us/step - loss: 1.3901 - acc: 0.3436\n",
            "Epoch 100/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4113 - acc: 0.3570\n",
            "Epoch 101/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4004 - acc: 0.3512\n",
            "Epoch 102/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.3925 - acc: 0.3589\n",
            "Epoch 103/250\n",
            "521/521 [==============================] - 0s 61us/step - loss: 1.3827 - acc: 0.3493\n",
            "Epoch 104/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3954 - acc: 0.3417\n",
            "Epoch 105/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.3937 - acc: 0.3570\n",
            "Epoch 106/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4347 - acc: 0.3455\n",
            "Epoch 107/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3997 - acc: 0.3397\n",
            "Epoch 108/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3732 - acc: 0.3301\n",
            "Epoch 109/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4224 - acc: 0.3474\n",
            "Epoch 110/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4061 - acc: 0.3608\n",
            "Epoch 111/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4299 - acc: 0.3436\n",
            "Epoch 112/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3953 - acc: 0.3455\n",
            "Epoch 113/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4101 - acc: 0.3359\n",
            "Epoch 114/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4277 - acc: 0.3301\n",
            "Epoch 115/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4167 - acc: 0.3282\n",
            "Epoch 116/250\n",
            "521/521 [==============================] - 0s 60us/step - loss: 1.4137 - acc: 0.3359\n",
            "Epoch 117/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4035 - acc: 0.3628\n",
            "Epoch 118/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4122 - acc: 0.3608\n",
            "Epoch 119/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4074 - acc: 0.3378\n",
            "Epoch 120/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4187 - acc: 0.3532\n",
            "Epoch 121/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4082 - acc: 0.3570\n",
            "Epoch 122/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4093 - acc: 0.3205\n",
            "Epoch 123/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3941 - acc: 0.3455\n",
            "Epoch 124/250\n",
            "521/521 [==============================] - 0s 63us/step - loss: 1.4047 - acc: 0.3417\n",
            "Epoch 125/250\n",
            "521/521 [==============================] - 0s 65us/step - loss: 1.4082 - acc: 0.3321\n",
            "Epoch 126/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3832 - acc: 0.3417\n",
            "Epoch 127/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4012 - acc: 0.3417\n",
            "Epoch 128/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.4195 - acc: 0.3321\n",
            "Epoch 129/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.4302 - acc: 0.3417\n",
            "Epoch 130/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3986 - acc: 0.3493\n",
            "Epoch 131/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4153 - acc: 0.3397\n",
            "Epoch 132/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.3760 - acc: 0.3532\n",
            "Epoch 133/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4163 - acc: 0.3321\n",
            "Epoch 134/250\n",
            "521/521 [==============================] - 0s 59us/step - loss: 1.4167 - acc: 0.3129\n",
            "Epoch 135/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3981 - acc: 0.3570\n",
            "Epoch 136/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3971 - acc: 0.3455\n",
            "Epoch 137/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3571 - acc: 0.3628\n",
            "Epoch 138/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4122 - acc: 0.3417\n",
            "Epoch 139/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3883 - acc: 0.3436\n",
            "Epoch 140/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4166 - acc: 0.3474\n",
            "Epoch 141/250\n",
            "521/521 [==============================] - 0s 61us/step - loss: 1.4286 - acc: 0.3090\n",
            "Epoch 142/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4181 - acc: 0.3493\n",
            "Epoch 143/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3859 - acc: 0.3225\n",
            "Epoch 144/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3895 - acc: 0.3493\n",
            "Epoch 145/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3869 - acc: 0.3359\n",
            "Epoch 146/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4287 - acc: 0.3282\n",
            "Epoch 147/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.3760 - acc: 0.3589\n",
            "Epoch 148/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4023 - acc: 0.3397\n",
            "Epoch 149/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4451 - acc: 0.3263\n",
            "Epoch 150/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4118 - acc: 0.3359\n",
            "Epoch 151/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4124 - acc: 0.2975\n",
            "Epoch 152/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4001 - acc: 0.3186\n",
            "Epoch 153/250\n",
            "521/521 [==============================] - 0s 61us/step - loss: 1.4138 - acc: 0.3282\n",
            "Epoch 154/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.3948 - acc: 0.3359\n",
            "Epoch 155/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4019 - acc: 0.3359\n",
            "Epoch 156/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.3645 - acc: 0.3551\n",
            "Epoch 157/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3942 - acc: 0.3378\n",
            "Epoch 158/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3937 - acc: 0.3493\n",
            "Epoch 159/250\n",
            "521/521 [==============================] - 0s 61us/step - loss: 1.3879 - acc: 0.2956\n",
            "Epoch 160/250\n",
            "521/521 [==============================] - 0s 58us/step - loss: 1.3939 - acc: 0.3378\n",
            "Epoch 161/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4011 - acc: 0.2956\n",
            "Epoch 162/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4065 - acc: 0.2841\n",
            "Epoch 163/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4178 - acc: 0.3321\n",
            "Epoch 164/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3853 - acc: 0.3359\n",
            "Epoch 165/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4239 - acc: 0.3263\n",
            "Epoch 166/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4150 - acc: 0.3244\n",
            "Epoch 167/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.3978 - acc: 0.3512\n",
            "Epoch 168/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4110 - acc: 0.3474\n",
            "Epoch 169/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3887 - acc: 0.3417\n",
            "Epoch 170/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4046 - acc: 0.3551\n",
            "Epoch 171/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3766 - acc: 0.3493\n",
            "Epoch 172/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3938 - acc: 0.3551\n",
            "Epoch 173/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3806 - acc: 0.3455\n",
            "Epoch 174/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.3661 - acc: 0.3628\n",
            "Epoch 175/250\n",
            "521/521 [==============================] - 0s 60us/step - loss: 1.3880 - acc: 0.3378\n",
            "Epoch 176/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3975 - acc: 0.3282\n",
            "Epoch 177/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4149 - acc: 0.3436\n",
            "Epoch 178/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3921 - acc: 0.3608\n",
            "Epoch 179/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3515 - acc: 0.3608\n",
            "Epoch 180/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3980 - acc: 0.3186\n",
            "Epoch 181/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3946 - acc: 0.3167\n",
            "Epoch 182/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.3583 - acc: 0.3493\n",
            "Epoch 183/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.3978 - acc: 0.3244\n",
            "Epoch 184/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.4089 - acc: 0.3321\n",
            "Epoch 185/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3756 - acc: 0.3628\n",
            "Epoch 186/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.4044 - acc: 0.3378\n",
            "Epoch 187/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3847 - acc: 0.3685\n",
            "Epoch 188/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4032 - acc: 0.3417\n",
            "Epoch 189/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3980 - acc: 0.3378\n",
            "Epoch 190/250\n",
            "521/521 [==============================] - 0s 64us/step - loss: 1.4105 - acc: 0.3359\n",
            "Epoch 191/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3969 - acc: 0.3397\n",
            "Epoch 192/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3843 - acc: 0.3493\n",
            "Epoch 193/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3966 - acc: 0.3628\n",
            "Epoch 194/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3859 - acc: 0.3704\n",
            "Epoch 195/250\n",
            "521/521 [==============================] - 0s 69us/step - loss: 1.3911 - acc: 0.3436\n",
            "Epoch 196/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.4075 - acc: 0.3340\n",
            "Epoch 197/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3746 - acc: 0.3436\n",
            "Epoch 198/250\n",
            "521/521 [==============================] - 0s 47us/step - loss: 1.3900 - acc: 0.3436\n",
            "Epoch 199/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3786 - acc: 0.3378\n",
            "Epoch 200/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3938 - acc: 0.3378\n",
            "Epoch 201/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3892 - acc: 0.3493\n",
            "Epoch 202/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3998 - acc: 0.3397\n",
            "Epoch 203/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.3888 - acc: 0.3474\n",
            "Epoch 204/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4064 - acc: 0.3321\n",
            "Epoch 205/250\n",
            "521/521 [==============================] - 0s 58us/step - loss: 1.4175 - acc: 0.3551\n",
            "Epoch 206/250\n",
            "521/521 [==============================] - 0s 47us/step - loss: 1.4255 - acc: 0.3417\n",
            "Epoch 207/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.3691 - acc: 0.3436\n",
            "Epoch 208/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3702 - acc: 0.3340\n",
            "Epoch 209/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.3902 - acc: 0.3512\n",
            "Epoch 210/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4113 - acc: 0.3109\n",
            "Epoch 211/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.4154 - acc: 0.3493\n",
            "Epoch 212/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4060 - acc: 0.3397\n",
            "Epoch 213/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3813 - acc: 0.3608\n",
            "Epoch 214/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4041 - acc: 0.3417\n",
            "Epoch 215/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.4155 - acc: 0.3493\n",
            "Epoch 216/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.4109 - acc: 0.3321\n",
            "Epoch 217/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4055 - acc: 0.3397\n",
            "Epoch 218/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4138 - acc: 0.3301\n",
            "Epoch 219/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.3910 - acc: 0.3321\n",
            "Epoch 220/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4021 - acc: 0.3340\n",
            "Epoch 221/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3845 - acc: 0.3378\n",
            "Epoch 222/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3814 - acc: 0.3570\n",
            "Epoch 223/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3892 - acc: 0.3551\n",
            "Epoch 224/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4005 - acc: 0.3301\n",
            "Epoch 225/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3939 - acc: 0.3417\n",
            "Epoch 226/250\n",
            "521/521 [==============================] - 0s 47us/step - loss: 1.3845 - acc: 0.3455\n",
            "Epoch 227/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.4002 - acc: 0.3340\n",
            "Epoch 228/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3809 - acc: 0.3532\n",
            "Epoch 229/250\n",
            "521/521 [==============================] - 0s 56us/step - loss: 1.3747 - acc: 0.3436\n",
            "Epoch 230/250\n",
            "521/521 [==============================] - 0s 80us/step - loss: 1.3879 - acc: 0.3589\n",
            "Epoch 231/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3665 - acc: 0.3570\n",
            "Epoch 232/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3777 - acc: 0.3493\n",
            "Epoch 233/250\n",
            "521/521 [==============================] - 0s 51us/step - loss: 1.3682 - acc: 0.3532\n",
            "Epoch 234/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3868 - acc: 0.3455\n",
            "Epoch 235/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.4311 - acc: 0.3109\n",
            "Epoch 236/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3991 - acc: 0.3378\n",
            "Epoch 237/250\n",
            "521/521 [==============================] - 0s 55us/step - loss: 1.3803 - acc: 0.3762\n",
            "Epoch 238/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3985 - acc: 0.3512\n",
            "Epoch 239/250\n",
            "521/521 [==============================] - 0s 66us/step - loss: 1.4062 - acc: 0.3474\n",
            "Epoch 240/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3864 - acc: 0.3359\n",
            "Epoch 241/250\n",
            "521/521 [==============================] - 0s 54us/step - loss: 1.3912 - acc: 0.3340\n",
            "Epoch 242/250\n",
            "521/521 [==============================] - 0s 48us/step - loss: 1.3794 - acc: 0.3532\n",
            "Epoch 243/250\n",
            "521/521 [==============================] - 0s 57us/step - loss: 1.4137 - acc: 0.3321\n",
            "Epoch 244/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3994 - acc: 0.3417\n",
            "Epoch 245/250\n",
            "521/521 [==============================] - 0s 58us/step - loss: 1.3808 - acc: 0.3474\n",
            "Epoch 246/250\n",
            "521/521 [==============================] - 0s 52us/step - loss: 1.4009 - acc: 0.3359\n",
            "Epoch 247/250\n",
            "521/521 [==============================] - 0s 50us/step - loss: 1.3943 - acc: 0.3455\n",
            "Epoch 248/250\n",
            "521/521 [==============================] - 0s 49us/step - loss: 1.3942 - acc: 0.3512\n",
            "Epoch 249/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.4010 - acc: 0.3397\n",
            "Epoch 250/250\n",
            "521/521 [==============================] - 0s 53us/step - loss: 1.3853 - acc: 0.3436\n",
            "521/521 [==============================] - 0s 25us/step\n",
            "Test loss: 1.6048315571274272\n",
            "Test accuracy: 0.4184261038184395\n",
            "Test loss: 1.6048315571274272\n",
            "Test accuracy: 0.4184261038184395\n",
            "Seq Test accuracy score : 0.2629558541266795 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 18/22 [02:36<00:33,  8.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        43\n",
            "         1.0       0.00      0.00      0.00        66\n",
            "         2.0       0.26      1.00      0.42       137\n",
            "         3.0       0.00      0.00      0.00       191\n",
            "         4.0       0.00      0.00      0.00        84\n",
            "\n",
            "   micro avg       0.26      0.26      0.26       521\n",
            "   macro avg       0.05      0.20      0.08       521\n",
            "weighted avg       0.07      0.26      0.11       521\n",
            "\n",
            "[[  0   0  43   0   0]\n",
            " [  0   0  66   0   0]\n",
            " [  0   0 137   0   0]\n",
            " [  0   0 191   0   0]\n",
            " [  0   0  84   0   0]]\n",
            "[[ 88   0   0   0   0]\n",
            " [  2   0   2   0   2]\n",
            " [  0   0 166   0   0]\n",
            " [  0   0  23  91   0]\n",
            " [  0   0   7   0 140]]\n",
            "Epoch 1/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.4086 - acc: 0.3510\n",
            "Epoch 2/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.3370 - acc: 0.4408\n",
            "Epoch 3/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3431 - acc: 0.3776\n",
            "Epoch 4/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.3215 - acc: 0.4000\n",
            "Epoch 5/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.3172 - acc: 0.3796\n",
            "Epoch 6/250\n",
            "490/490 [==============================] - 0s 60us/step - loss: 1.3218 - acc: 0.3571\n",
            "Epoch 7/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3126 - acc: 0.3735\n",
            "Epoch 8/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3533 - acc: 0.3429\n",
            "Epoch 9/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.3142 - acc: 0.3633\n",
            "Epoch 10/250\n",
            "490/490 [==============================] - 0s 55us/step - loss: 1.3236 - acc: 0.3551\n",
            "Epoch 11/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3081 - acc: 0.3776\n",
            "Epoch 12/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.3050 - acc: 0.3694\n",
            "Epoch 13/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3205 - acc: 0.3367\n",
            "Epoch 14/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2945 - acc: 0.4061\n",
            "Epoch 15/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.3189 - acc: 0.3898\n",
            "Epoch 16/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2917 - acc: 0.3633\n",
            "Epoch 17/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3069 - acc: 0.3918\n",
            "Epoch 18/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3341 - acc: 0.3735\n",
            "Epoch 19/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.3165 - acc: 0.3714\n",
            "Epoch 20/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.3195 - acc: 0.3755\n",
            "Epoch 21/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3171 - acc: 0.3612\n",
            "Epoch 22/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.3237 - acc: 0.3796\n",
            "Epoch 23/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3173 - acc: 0.3571\n",
            "Epoch 24/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3104 - acc: 0.3796\n",
            "Epoch 25/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2991 - acc: 0.3980\n",
            "Epoch 26/250\n",
            "490/490 [==============================] - 0s 41us/step - loss: 1.3187 - acc: 0.3735\n",
            "Epoch 27/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2909 - acc: 0.3939\n",
            "Epoch 28/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.3151 - acc: 0.3633\n",
            "Epoch 29/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3075 - acc: 0.3816\n",
            "Epoch 30/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.3189 - acc: 0.3592\n",
            "Epoch 31/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3149 - acc: 0.3755\n",
            "Epoch 32/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2957 - acc: 0.3918\n",
            "Epoch 33/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2930 - acc: 0.4143\n",
            "Epoch 34/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.3162 - acc: 0.3653\n",
            "Epoch 35/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3078 - acc: 0.3857\n",
            "Epoch 36/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.3084 - acc: 0.3776\n",
            "Epoch 37/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3034 - acc: 0.4000\n",
            "Epoch 38/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.3200 - acc: 0.3898\n",
            "Epoch 39/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3132 - acc: 0.3653\n",
            "Epoch 40/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3051 - acc: 0.3612\n",
            "Epoch 41/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.3132 - acc: 0.3837\n",
            "Epoch 42/250\n",
            "490/490 [==============================] - 0s 54us/step - loss: 1.2893 - acc: 0.4020\n",
            "Epoch 43/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.3026 - acc: 0.3694\n",
            "Epoch 44/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2915 - acc: 0.3816\n",
            "Epoch 45/250\n",
            "490/490 [==============================] - 0s 41us/step - loss: 1.3112 - acc: 0.3837\n",
            "Epoch 46/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2963 - acc: 0.3755\n",
            "Epoch 47/250\n",
            "490/490 [==============================] - 0s 69us/step - loss: 1.2885 - acc: 0.3796\n",
            "Epoch 48/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3033 - acc: 0.3898\n",
            "Epoch 49/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2921 - acc: 0.4041\n",
            "Epoch 50/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3163 - acc: 0.3959\n",
            "Epoch 51/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2856 - acc: 0.3816\n",
            "Epoch 52/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2970 - acc: 0.3755\n",
            "Epoch 53/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2989 - acc: 0.3878\n",
            "Epoch 54/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.3157 - acc: 0.3714\n",
            "Epoch 55/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.3073 - acc: 0.3837\n",
            "Epoch 56/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2934 - acc: 0.3857\n",
            "Epoch 57/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2987 - acc: 0.3878\n",
            "Epoch 58/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3092 - acc: 0.3490\n",
            "Epoch 59/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2947 - acc: 0.3898\n",
            "Epoch 60/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2848 - acc: 0.3939\n",
            "Epoch 61/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2926 - acc: 0.3959\n",
            "Epoch 62/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2884 - acc: 0.4061\n",
            "Epoch 63/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3169 - acc: 0.3837\n",
            "Epoch 64/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3108 - acc: 0.3653\n",
            "Epoch 65/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3130 - acc: 0.3592\n",
            "Epoch 66/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2897 - acc: 0.3816\n",
            "Epoch 67/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.3039 - acc: 0.3816\n",
            "Epoch 68/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.3116 - acc: 0.3735\n",
            "Epoch 69/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.2985 - acc: 0.3735\n",
            "Epoch 70/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.3113 - acc: 0.3776\n",
            "Epoch 71/250\n",
            "490/490 [==============================] - 0s 63us/step - loss: 1.3072 - acc: 0.3673\n",
            "Epoch 72/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2906 - acc: 0.3612\n",
            "Epoch 73/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2956 - acc: 0.4020\n",
            "Epoch 74/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2863 - acc: 0.3939\n",
            "Epoch 75/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3011 - acc: 0.3694\n",
            "Epoch 76/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2926 - acc: 0.3959\n",
            "Epoch 77/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.3227 - acc: 0.3612\n",
            "Epoch 78/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2964 - acc: 0.4020\n",
            "Epoch 79/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3066 - acc: 0.3714\n",
            "Epoch 80/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2934 - acc: 0.3673\n",
            "Epoch 81/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2955 - acc: 0.3714\n",
            "Epoch 82/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2898 - acc: 0.3837\n",
            "Epoch 83/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3013 - acc: 0.3714\n",
            "Epoch 84/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.3052 - acc: 0.3939\n",
            "Epoch 85/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3058 - acc: 0.3592\n",
            "Epoch 86/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3056 - acc: 0.3918\n",
            "Epoch 87/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3038 - acc: 0.3898\n",
            "Epoch 88/250\n",
            "490/490 [==============================] - 0s 61us/step - loss: 1.2979 - acc: 0.3776\n",
            "Epoch 89/250\n",
            "490/490 [==============================] - 0s 54us/step - loss: 1.2872 - acc: 0.3796\n",
            "Epoch 90/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2945 - acc: 0.3714\n",
            "Epoch 91/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2737 - acc: 0.3918\n",
            "Epoch 92/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3087 - acc: 0.3653\n",
            "Epoch 93/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2885 - acc: 0.3755\n",
            "Epoch 94/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2957 - acc: 0.3857\n",
            "Epoch 95/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2756 - acc: 0.3816\n",
            "Epoch 96/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2816 - acc: 0.3980\n",
            "Epoch 97/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2953 - acc: 0.3837\n",
            "Epoch 98/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2941 - acc: 0.3633\n",
            "Epoch 99/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2737 - acc: 0.4143\n",
            "Epoch 100/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2970 - acc: 0.3898\n",
            "Epoch 101/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2911 - acc: 0.3714\n",
            "Epoch 102/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2844 - acc: 0.4102\n",
            "Epoch 103/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2945 - acc: 0.3898\n",
            "Epoch 104/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.3028 - acc: 0.3755\n",
            "Epoch 105/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2978 - acc: 0.3796\n",
            "Epoch 106/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.3008 - acc: 0.3939\n",
            "Epoch 107/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.3002 - acc: 0.3776\n",
            "Epoch 108/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2918 - acc: 0.3898\n",
            "Epoch 109/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2745 - acc: 0.4143\n",
            "Epoch 110/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2890 - acc: 0.3755\n",
            "Epoch 111/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2865 - acc: 0.3980\n",
            "Epoch 112/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2902 - acc: 0.3755\n",
            "Epoch 113/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2888 - acc: 0.3878\n",
            "Epoch 114/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2851 - acc: 0.3837\n",
            "Epoch 115/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2909 - acc: 0.4020\n",
            "Epoch 116/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2872 - acc: 0.3796\n",
            "Epoch 117/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2888 - acc: 0.3714\n",
            "Epoch 118/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2878 - acc: 0.3653\n",
            "Epoch 119/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.3036 - acc: 0.3878\n",
            "Epoch 120/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2895 - acc: 0.3633\n",
            "Epoch 121/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.2789 - acc: 0.3653\n",
            "Epoch 122/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2860 - acc: 0.3959\n",
            "Epoch 123/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2872 - acc: 0.3980\n",
            "Epoch 124/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2885 - acc: 0.3959\n",
            "Epoch 125/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2874 - acc: 0.3694\n",
            "Epoch 126/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2873 - acc: 0.4020\n",
            "Epoch 127/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2832 - acc: 0.3898\n",
            "Epoch 128/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2955 - acc: 0.3898\n",
            "Epoch 129/250\n",
            "490/490 [==============================] - 0s 63us/step - loss: 1.3055 - acc: 0.3735\n",
            "Epoch 130/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2810 - acc: 0.3796\n",
            "Epoch 131/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.3134 - acc: 0.3857\n",
            "Epoch 132/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2905 - acc: 0.3735\n",
            "Epoch 133/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2886 - acc: 0.3776\n",
            "Epoch 134/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3052 - acc: 0.3531\n",
            "Epoch 135/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2812 - acc: 0.4000\n",
            "Epoch 136/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2914 - acc: 0.3857\n",
            "Epoch 137/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.3128 - acc: 0.3367\n",
            "Epoch 138/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2931 - acc: 0.3694\n",
            "Epoch 139/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2809 - acc: 0.3816\n",
            "Epoch 140/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.3035 - acc: 0.3633\n",
            "Epoch 141/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2902 - acc: 0.3918\n",
            "Epoch 142/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2661 - acc: 0.4061\n",
            "Epoch 143/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2886 - acc: 0.3633\n",
            "Epoch 144/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2881 - acc: 0.4204\n",
            "Epoch 145/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2766 - acc: 0.3878\n",
            "Epoch 146/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2992 - acc: 0.3816\n",
            "Epoch 147/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2923 - acc: 0.3755\n",
            "Epoch 148/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2789 - acc: 0.3796\n",
            "Epoch 149/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2979 - acc: 0.3633\n",
            "Epoch 150/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2788 - acc: 0.4082\n",
            "Epoch 151/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.3002 - acc: 0.3633\n",
            "Epoch 152/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2663 - acc: 0.3918\n",
            "Epoch 153/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2899 - acc: 0.3776\n",
            "Epoch 154/250\n",
            "490/490 [==============================] - 0s 54us/step - loss: 1.2990 - acc: 0.3755\n",
            "Epoch 155/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2866 - acc: 0.3939\n",
            "Epoch 156/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.3007 - acc: 0.3918\n",
            "Epoch 157/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2850 - acc: 0.3878\n",
            "Epoch 158/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.3000 - acc: 0.3816\n",
            "Epoch 159/250\n",
            "490/490 [==============================] - 0s 55us/step - loss: 1.2944 - acc: 0.3816\n",
            "Epoch 160/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2697 - acc: 0.3959\n",
            "Epoch 161/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2880 - acc: 0.3776\n",
            "Epoch 162/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2794 - acc: 0.3735\n",
            "Epoch 163/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2831 - acc: 0.3714\n",
            "Epoch 164/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2928 - acc: 0.3959\n",
            "Epoch 165/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2844 - acc: 0.3755\n",
            "Epoch 166/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2991 - acc: 0.3714\n",
            "Epoch 167/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2743 - acc: 0.3959\n",
            "Epoch 168/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.2947 - acc: 0.3694\n",
            "Epoch 169/250\n",
            "490/490 [==============================] - 0s 61us/step - loss: 1.2751 - acc: 0.3939\n",
            "Epoch 170/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2832 - acc: 0.4082\n",
            "Epoch 171/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2751 - acc: 0.3796\n",
            "Epoch 172/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2667 - acc: 0.3857\n",
            "Epoch 173/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2904 - acc: 0.3735\n",
            "Epoch 174/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2679 - acc: 0.4082\n",
            "Epoch 175/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2733 - acc: 0.3633\n",
            "Epoch 176/250\n",
            "490/490 [==============================] - 0s 54us/step - loss: 1.2854 - acc: 0.3816\n",
            "Epoch 177/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2626 - acc: 0.3980\n",
            "Epoch 178/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2907 - acc: 0.3755\n",
            "Epoch 179/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2888 - acc: 0.3878\n",
            "Epoch 180/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2942 - acc: 0.3776\n",
            "Epoch 181/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2842 - acc: 0.3898\n",
            "Epoch 182/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2992 - acc: 0.4061\n",
            "Epoch 183/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2930 - acc: 0.3776\n",
            "Epoch 184/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2669 - acc: 0.3837\n",
            "Epoch 185/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2642 - acc: 0.3898\n",
            "Epoch 186/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2718 - acc: 0.4082\n",
            "Epoch 187/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2844 - acc: 0.3755\n",
            "Epoch 188/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2675 - acc: 0.4041\n",
            "Epoch 189/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2957 - acc: 0.3653\n",
            "Epoch 190/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2719 - acc: 0.3939\n",
            "Epoch 191/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2846 - acc: 0.3939\n",
            "Epoch 192/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2818 - acc: 0.3959\n",
            "Epoch 193/250\n",
            "490/490 [==============================] - 0s 57us/step - loss: 1.2757 - acc: 0.3939\n",
            "Epoch 194/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2934 - acc: 0.3980\n",
            "Epoch 195/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2663 - acc: 0.4041\n",
            "Epoch 196/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2957 - acc: 0.3571\n",
            "Epoch 197/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2913 - acc: 0.3816\n",
            "Epoch 198/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2914 - acc: 0.3796\n",
            "Epoch 199/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2896 - acc: 0.3653\n",
            "Epoch 200/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2719 - acc: 0.3857\n",
            "Epoch 201/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2836 - acc: 0.3959\n",
            "Epoch 202/250\n",
            "490/490 [==============================] - 0s 56us/step - loss: 1.2765 - acc: 0.3898\n",
            "Epoch 203/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2920 - acc: 0.3653\n",
            "Epoch 204/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2721 - acc: 0.3959\n",
            "Epoch 205/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2732 - acc: 0.3898\n",
            "Epoch 206/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2881 - acc: 0.3878\n",
            "Epoch 207/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2841 - acc: 0.3796\n",
            "Epoch 208/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2739 - acc: 0.3837\n",
            "Epoch 209/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2821 - acc: 0.3694\n",
            "Epoch 210/250\n",
            "490/490 [==============================] - 0s 70us/step - loss: 1.2945 - acc: 0.3837\n",
            "Epoch 211/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2788 - acc: 0.4000\n",
            "Epoch 212/250\n",
            "490/490 [==============================] - 0s 40us/step - loss: 1.2790 - acc: 0.3735\n",
            "Epoch 213/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2985 - acc: 0.3714\n",
            "Epoch 214/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2961 - acc: 0.3837\n",
            "Epoch 215/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2759 - acc: 0.3878\n",
            "Epoch 216/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2961 - acc: 0.3755\n",
            "Epoch 217/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2903 - acc: 0.3714\n",
            "Epoch 218/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2910 - acc: 0.3776\n",
            "Epoch 219/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2848 - acc: 0.3898\n",
            "Epoch 220/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2811 - acc: 0.3939\n",
            "Epoch 221/250\n",
            "490/490 [==============================] - 0s 57us/step - loss: 1.2941 - acc: 0.3755\n",
            "Epoch 222/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2825 - acc: 0.3592\n",
            "Epoch 223/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2830 - acc: 0.3857\n",
            "Epoch 224/250\n",
            "490/490 [==============================] - 0s 43us/step - loss: 1.2809 - acc: 0.3816\n",
            "Epoch 225/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.3062 - acc: 0.3714\n",
            "Epoch 226/250\n",
            "490/490 [==============================] - 0s 51us/step - loss: 1.2662 - acc: 0.4000\n",
            "Epoch 227/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2658 - acc: 0.3816\n",
            "Epoch 228/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2953 - acc: 0.3714\n",
            "Epoch 229/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2952 - acc: 0.3714\n",
            "Epoch 230/250\n",
            "490/490 [==============================] - 0s 52us/step - loss: 1.2756 - acc: 0.3673\n",
            "Epoch 231/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2773 - acc: 0.3796\n",
            "Epoch 232/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2816 - acc: 0.3857\n",
            "Epoch 233/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.2712 - acc: 0.4000\n",
            "Epoch 234/250\n",
            "490/490 [==============================] - 0s 49us/step - loss: 1.2663 - acc: 0.4184\n",
            "Epoch 235/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2774 - acc: 0.3755\n",
            "Epoch 236/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2970 - acc: 0.3939\n",
            "Epoch 237/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.2770 - acc: 0.3857\n",
            "Epoch 238/250\n",
            "490/490 [==============================] - 0s 46us/step - loss: 1.2913 - acc: 0.3837\n",
            "Epoch 239/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2925 - acc: 0.3653\n",
            "Epoch 240/250\n",
            "490/490 [==============================] - 0s 53us/step - loss: 1.2811 - acc: 0.3898\n",
            "Epoch 241/250\n",
            "490/490 [==============================] - 0s 45us/step - loss: 1.2881 - acc: 0.3673\n",
            "Epoch 242/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2933 - acc: 0.3857\n",
            "Epoch 243/250\n",
            "490/490 [==============================] - 0s 44us/step - loss: 1.2737 - acc: 0.3939\n",
            "Epoch 244/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2706 - acc: 0.3816\n",
            "Epoch 245/250\n",
            "490/490 [==============================] - 0s 42us/step - loss: 1.2816 - acc: 0.3776\n",
            "Epoch 246/250\n",
            "490/490 [==============================] - 0s 48us/step - loss: 1.2661 - acc: 0.4061\n",
            "Epoch 247/250\n",
            "490/490 [==============================] - 0s 47us/step - loss: 1.2724 - acc: 0.3898\n",
            "Epoch 248/250\n",
            "490/490 [==============================] - 0s 56us/step - loss: 1.2856 - acc: 0.3612\n",
            "Epoch 249/250\n",
            "490/490 [==============================] - 0s 50us/step - loss: 1.2662 - acc: 0.3939\n",
            "Epoch 250/250\n",
            "490/490 [==============================] - 0s 74us/step - loss: 1.2655 - acc: 0.3959\n",
            "490/490 [==============================] - 0s 28us/step\n",
            "Test loss: 1.2735535047492204\n",
            "Test accuracy: 0.6163265259898438\n",
            "Test loss: 1.2735535047492204\n",
            "Test accuracy: 0.6163265259898438\n",
            "Seq Test accuracy score : 0.2571428571428571 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▋ | 19/22 [02:43<00:24,  8.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        23\n",
            "         1.0       0.00      0.00      0.00        30\n",
            "         2.0       0.26      1.00      0.41       126\n",
            "         3.0       0.00      0.00      0.00       143\n",
            "         4.0       0.00      0.00      0.00       168\n",
            "\n",
            "   micro avg       0.26      0.26      0.26       490\n",
            "   macro avg       0.05      0.20      0.08       490\n",
            "weighted avg       0.07      0.26      0.11       490\n",
            "\n",
            "[[  0   0  23   0   0]\n",
            " [  0   0  30   0   0]\n",
            " [  0   0 126   0   0]\n",
            " [  0   0 143   0   0]\n",
            " [  0   0 168   0   0]]\n",
            "[[ 32   0   0   0]\n",
            " [  0 132   0   0]\n",
            " [  0   8 143   0]\n",
            " [  0  59   0 116]]\n",
            "Epoch 1/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.2190 - acc: 0.5185\n",
            "Epoch 2/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.1703 - acc: 0.5166\n",
            "Epoch 3/250\n",
            "513/513 [==============================] - 0s 46us/step - loss: 1.1428 - acc: 0.5400\n",
            "Epoch 4/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.1054 - acc: 0.5088\n",
            "Epoch 5/250\n",
            "513/513 [==============================] - 0s 48us/step - loss: 1.0924 - acc: 0.4834\n",
            "Epoch 6/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.0778 - acc: 0.4444\n",
            "Epoch 7/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0823 - acc: 0.4113\n",
            "Epoch 8/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0681 - acc: 0.4795\n",
            "Epoch 9/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0641 - acc: 0.5517\n",
            "Epoch 10/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0521 - acc: 0.5965\n",
            "Epoch 11/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0541 - acc: 0.5945\n",
            "Epoch 12/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 1.0315 - acc: 0.5887\n",
            "Epoch 13/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0352 - acc: 0.6023\n",
            "Epoch 14/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0211 - acc: 0.6004\n",
            "Epoch 15/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0335 - acc: 0.6043\n",
            "Epoch 16/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.0200 - acc: 0.6043\n",
            "Epoch 17/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 1.0281 - acc: 0.5965\n",
            "Epoch 18/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0134 - acc: 0.5926\n",
            "Epoch 19/250\n",
            "513/513 [==============================] - 0s 98us/step - loss: 1.0323 - acc: 0.5965\n",
            "Epoch 20/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 1.0119 - acc: 0.6082\n",
            "Epoch 21/250\n",
            "513/513 [==============================] - 0s 77us/step - loss: 1.0138 - acc: 0.5965\n",
            "Epoch 22/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.0126 - acc: 0.5945\n",
            "Epoch 23/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0075 - acc: 0.5906\n",
            "Epoch 24/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0250 - acc: 0.5906\n",
            "Epoch 25/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0112 - acc: 0.5965\n",
            "Epoch 26/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0198 - acc: 0.6023\n",
            "Epoch 27/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0135 - acc: 0.5965\n",
            "Epoch 28/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0043 - acc: 0.5984\n",
            "Epoch 29/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0145 - acc: 0.6082\n",
            "Epoch 30/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0197 - acc: 0.6062\n",
            "Epoch 31/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0030 - acc: 0.6121\n",
            "Epoch 32/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0132 - acc: 0.6121\n",
            "Epoch 33/250\n",
            "513/513 [==============================] - 0s 48us/step - loss: 1.0128 - acc: 0.6082\n",
            "Epoch 34/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.0034 - acc: 0.6004\n",
            "Epoch 35/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 0.9936 - acc: 0.6121\n",
            "Epoch 36/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0049 - acc: 0.6023\n",
            "Epoch 37/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0105 - acc: 0.6101\n",
            "Epoch 38/250\n",
            "513/513 [==============================] - 0s 63us/step - loss: 1.0161 - acc: 0.6101\n",
            "Epoch 39/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0186 - acc: 0.6082\n",
            "Epoch 40/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0098 - acc: 0.6101\n",
            "Epoch 41/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0105 - acc: 0.6101\n",
            "Epoch 42/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.0027 - acc: 0.6082\n",
            "Epoch 43/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0160 - acc: 0.6121\n",
            "Epoch 44/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0039 - acc: 0.6121\n",
            "Epoch 45/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0296 - acc: 0.6121\n",
            "Epoch 46/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9941 - acc: 0.6082\n",
            "Epoch 47/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0153 - acc: 0.6121\n",
            "Epoch 48/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0191 - acc: 0.6101\n",
            "Epoch 49/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9916 - acc: 0.6121\n",
            "Epoch 50/250\n",
            "513/513 [==============================] - 0s 63us/step - loss: 1.0077 - acc: 0.6082\n",
            "Epoch 51/250\n",
            "513/513 [==============================] - 0s 47us/step - loss: 0.9995 - acc: 0.6082\n",
            "Epoch 52/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9852 - acc: 0.6101\n",
            "Epoch 53/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0148 - acc: 0.6121\n",
            "Epoch 54/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 0.9865 - acc: 0.6101\n",
            "Epoch 55/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9942 - acc: 0.6101\n",
            "Epoch 56/250\n",
            "513/513 [==============================] - 0s 68us/step - loss: 0.9715 - acc: 0.6121\n",
            "Epoch 57/250\n",
            "513/513 [==============================] - 0s 48us/step - loss: 0.9952 - acc: 0.6101\n",
            "Epoch 58/250\n",
            "513/513 [==============================] - 0s 48us/step - loss: 0.9990 - acc: 0.6101\n",
            "Epoch 59/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 0.9921 - acc: 0.6043\n",
            "Epoch 60/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9950 - acc: 0.6062\n",
            "Epoch 61/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0063 - acc: 0.5965\n",
            "Epoch 62/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0223 - acc: 0.6004\n",
            "Epoch 63/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0270 - acc: 0.5965\n",
            "Epoch 64/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9914 - acc: 0.6082\n",
            "Epoch 65/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9930 - acc: 0.6101\n",
            "Epoch 66/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0119 - acc: 0.6082\n",
            "Epoch 67/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9928 - acc: 0.6082\n",
            "Epoch 68/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0038 - acc: 0.6101\n",
            "Epoch 69/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9859 - acc: 0.6121\n",
            "Epoch 70/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 0.9796 - acc: 0.6121\n",
            "Epoch 71/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9904 - acc: 0.6121\n",
            "Epoch 72/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0013 - acc: 0.6121\n",
            "Epoch 73/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9836 - acc: 0.6101\n",
            "Epoch 74/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 1.0145 - acc: 0.6082\n",
            "Epoch 75/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 0.9787 - acc: 0.6101\n",
            "Epoch 76/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0016 - acc: 0.6121\n",
            "Epoch 77/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9918 - acc: 0.6121\n",
            "Epoch 78/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0169 - acc: 0.6082\n",
            "Epoch 79/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9906 - acc: 0.6121\n",
            "Epoch 80/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0028 - acc: 0.6082\n",
            "Epoch 81/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9951 - acc: 0.6121\n",
            "Epoch 82/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0016 - acc: 0.6121\n",
            "Epoch 83/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 0.9986 - acc: 0.6062\n",
            "Epoch 84/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9894 - acc: 0.6121\n",
            "Epoch 85/250\n",
            "513/513 [==============================] - 0s 64us/step - loss: 0.9925 - acc: 0.6121\n",
            "Epoch 86/250\n",
            "513/513 [==============================] - 0s 63us/step - loss: 1.0062 - acc: 0.6101\n",
            "Epoch 87/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0023 - acc: 0.6082\n",
            "Epoch 88/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0128 - acc: 0.6101\n",
            "Epoch 89/250\n",
            "513/513 [==============================] - 0s 66us/step - loss: 0.9874 - acc: 0.6121\n",
            "Epoch 90/250\n",
            "513/513 [==============================] - 0s 96us/step - loss: 0.9938 - acc: 0.6101\n",
            "Epoch 91/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0056 - acc: 0.6082\n",
            "Epoch 92/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9915 - acc: 0.6101\n",
            "Epoch 93/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9930 - acc: 0.6101\n",
            "Epoch 94/250\n",
            "513/513 [==============================] - 0s 64us/step - loss: 0.9961 - acc: 0.6082\n",
            "Epoch 95/250\n",
            "513/513 [==============================] - 0s 76us/step - loss: 0.9860 - acc: 0.6101\n",
            "Epoch 96/250\n",
            "513/513 [==============================] - 0s 88us/step - loss: 0.9697 - acc: 0.6101\n",
            "Epoch 97/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9855 - acc: 0.6101\n",
            "Epoch 98/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0174 - acc: 0.6101\n",
            "Epoch 99/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9921 - acc: 0.6121\n",
            "Epoch 100/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9870 - acc: 0.6101\n",
            "Epoch 101/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9827 - acc: 0.6121\n",
            "Epoch 102/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 0.9843 - acc: 0.6121\n",
            "Epoch 103/250\n",
            "513/513 [==============================] - 0s 79us/step - loss: 1.0083 - acc: 0.6082\n",
            "Epoch 104/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0027 - acc: 0.6101\n",
            "Epoch 105/250\n",
            "513/513 [==============================] - 0s 65us/step - loss: 1.0097 - acc: 0.6101\n",
            "Epoch 106/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0090 - acc: 0.6062\n",
            "Epoch 107/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0132 - acc: 0.6121\n",
            "Epoch 108/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 0.9905 - acc: 0.6121\n",
            "Epoch 109/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0020 - acc: 0.6121\n",
            "Epoch 110/250\n",
            "513/513 [==============================] - 0s 70us/step - loss: 1.0207 - acc: 0.6082\n",
            "Epoch 111/250\n",
            "513/513 [==============================] - 0s 65us/step - loss: 1.0035 - acc: 0.6101\n",
            "Epoch 112/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 0.9961 - acc: 0.6121\n",
            "Epoch 113/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 0.9940 - acc: 0.6101\n",
            "Epoch 114/250\n",
            "513/513 [==============================] - 0s 62us/step - loss: 0.9868 - acc: 0.6140\n",
            "Epoch 115/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0046 - acc: 0.6101\n",
            "Epoch 116/250\n",
            "513/513 [==============================] - 0s 63us/step - loss: 1.0315 - acc: 0.6121\n",
            "Epoch 117/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9939 - acc: 0.6082\n",
            "Epoch 118/250\n",
            "513/513 [==============================] - 0s 67us/step - loss: 0.9942 - acc: 0.6121\n",
            "Epoch 119/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 1.0084 - acc: 0.6082\n",
            "Epoch 120/250\n",
            "513/513 [==============================] - 0s 79us/step - loss: 0.9837 - acc: 0.6101\n",
            "Epoch 121/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 0.9979 - acc: 0.6082\n",
            "Epoch 122/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 1.0015 - acc: 0.6121\n",
            "Epoch 123/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0077 - acc: 0.6101\n",
            "Epoch 124/250\n",
            "513/513 [==============================] - 0s 66us/step - loss: 0.9851 - acc: 0.6121\n",
            "Epoch 125/250\n",
            "513/513 [==============================] - 0s 62us/step - loss: 1.0123 - acc: 0.6101\n",
            "Epoch 126/250\n",
            "513/513 [==============================] - 0s 67us/step - loss: 1.0046 - acc: 0.6101\n",
            "Epoch 127/250\n",
            "513/513 [==============================] - 0s 67us/step - loss: 1.0073 - acc: 0.6062\n",
            "Epoch 128/250\n",
            "513/513 [==============================] - 0s 75us/step - loss: 1.0025 - acc: 0.5984\n",
            "Epoch 129/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9991 - acc: 0.6004\n",
            "Epoch 130/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0117 - acc: 0.6023\n",
            "Epoch 131/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 1.0085 - acc: 0.6101\n",
            "Epoch 132/250\n",
            "513/513 [==============================] - 0s 64us/step - loss: 0.9996 - acc: 0.6140\n",
            "Epoch 133/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9973 - acc: 0.6101\n",
            "Epoch 134/250\n",
            "513/513 [==============================] - 0s 66us/step - loss: 0.9996 - acc: 0.6121\n",
            "Epoch 135/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 0.9977 - acc: 0.6101\n",
            "Epoch 136/250\n",
            "513/513 [==============================] - 0s 79us/step - loss: 0.9934 - acc: 0.6121\n",
            "Epoch 137/250\n",
            "513/513 [==============================] - 0s 80us/step - loss: 0.9906 - acc: 0.6101\n",
            "Epoch 138/250\n",
            "513/513 [==============================] - 0s 68us/step - loss: 1.0114 - acc: 0.6082\n",
            "Epoch 139/250\n",
            "513/513 [==============================] - 0s 73us/step - loss: 0.9951 - acc: 0.6121\n",
            "Epoch 140/250\n",
            "513/513 [==============================] - 0s 96us/step - loss: 1.0049 - acc: 0.6121\n",
            "Epoch 141/250\n",
            "513/513 [==============================] - 0s 72us/step - loss: 1.0074 - acc: 0.6062\n",
            "Epoch 142/250\n",
            "513/513 [==============================] - 0s 61us/step - loss: 0.9827 - acc: 0.6121\n",
            "Epoch 143/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 1.0004 - acc: 0.6082\n",
            "Epoch 144/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9998 - acc: 0.6140\n",
            "Epoch 145/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0090 - acc: 0.6121\n",
            "Epoch 146/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0069 - acc: 0.6082\n",
            "Epoch 147/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9951 - acc: 0.6101\n",
            "Epoch 148/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0027 - acc: 0.6121\n",
            "Epoch 149/250\n",
            "513/513 [==============================] - 0s 63us/step - loss: 1.0016 - acc: 0.6101\n",
            "Epoch 150/250\n",
            "513/513 [==============================] - 0s 78us/step - loss: 0.9928 - acc: 0.6101\n",
            "Epoch 151/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0076 - acc: 0.6121\n",
            "Epoch 152/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0188 - acc: 0.6101\n",
            "Epoch 153/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9750 - acc: 0.6101\n",
            "Epoch 154/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0014 - acc: 0.6121\n",
            "Epoch 155/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0079 - acc: 0.6101\n",
            "Epoch 156/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9909 - acc: 0.6101\n",
            "Epoch 157/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9953 - acc: 0.6121\n",
            "Epoch 158/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0080 - acc: 0.6121\n",
            "Epoch 159/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9883 - acc: 0.6101\n",
            "Epoch 160/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9849 - acc: 0.6121\n",
            "Epoch 161/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 0.9813 - acc: 0.6121\n",
            "Epoch 162/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 0.9999 - acc: 0.6082\n",
            "Epoch 163/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0114 - acc: 0.6043\n",
            "Epoch 164/250\n",
            "513/513 [==============================] - 0s 47us/step - loss: 0.9953 - acc: 0.6062\n",
            "Epoch 165/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0146 - acc: 0.6023\n",
            "Epoch 166/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 1.0139 - acc: 0.6023\n",
            "Epoch 167/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9957 - acc: 0.6101\n",
            "Epoch 168/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0025 - acc: 0.6082\n",
            "Epoch 169/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 1.0018 - acc: 0.6082\n",
            "Epoch 170/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9953 - acc: 0.6121\n",
            "Epoch 171/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9914 - acc: 0.6121\n",
            "Epoch 172/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0094 - acc: 0.6121\n",
            "Epoch 173/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 0.9885 - acc: 0.6121\n",
            "Epoch 174/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9961 - acc: 0.6101\n",
            "Epoch 175/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9848 - acc: 0.6121\n",
            "Epoch 176/250\n",
            "513/513 [==============================] - 0s 65us/step - loss: 0.9920 - acc: 0.6101\n",
            "Epoch 177/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0031 - acc: 0.6101\n",
            "Epoch 178/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9901 - acc: 0.6121\n",
            "Epoch 179/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9905 - acc: 0.6121\n",
            "Epoch 180/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9933 - acc: 0.6121\n",
            "Epoch 181/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9971 - acc: 0.6121\n",
            "Epoch 182/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9865 - acc: 0.6121\n",
            "Epoch 183/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 0.9887 - acc: 0.6121\n",
            "Epoch 184/250\n",
            "513/513 [==============================] - 0s 68us/step - loss: 1.0045 - acc: 0.6121\n",
            "Epoch 185/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9964 - acc: 0.6121\n",
            "Epoch 186/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9777 - acc: 0.6101\n",
            "Epoch 187/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9987 - acc: 0.6082\n",
            "Epoch 188/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0021 - acc: 0.6121\n",
            "Epoch 189/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9906 - acc: 0.6121\n",
            "Epoch 190/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 1.0097 - acc: 0.6101\n",
            "Epoch 191/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9915 - acc: 0.6121\n",
            "Epoch 192/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9817 - acc: 0.6121\n",
            "Epoch 193/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0018 - acc: 0.6121\n",
            "Epoch 194/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0004 - acc: 0.6121\n",
            "Epoch 195/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0201 - acc: 0.6121\n",
            "Epoch 196/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9907 - acc: 0.6101\n",
            "Epoch 197/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9890 - acc: 0.6121\n",
            "Epoch 198/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9950 - acc: 0.6121\n",
            "Epoch 199/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9998 - acc: 0.6101\n",
            "Epoch 200/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 1.0044 - acc: 0.6101\n",
            "Epoch 201/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9978 - acc: 0.6121\n",
            "Epoch 202/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9922 - acc: 0.6121\n",
            "Epoch 203/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9764 - acc: 0.6082\n",
            "Epoch 204/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9785 - acc: 0.6121\n",
            "Epoch 205/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0137 - acc: 0.6101\n",
            "Epoch 206/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9808 - acc: 0.6082\n",
            "Epoch 207/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 1.0200 - acc: 0.6082\n",
            "Epoch 208/250\n",
            "513/513 [==============================] - 0s 48us/step - loss: 1.0074 - acc: 0.6121\n",
            "Epoch 209/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9995 - acc: 0.6082\n",
            "Epoch 210/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9747 - acc: 0.6121\n",
            "Epoch 211/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9980 - acc: 0.6101\n",
            "Epoch 212/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9833 - acc: 0.6082\n",
            "Epoch 213/250\n",
            "513/513 [==============================] - 0s 57us/step - loss: 0.9777 - acc: 0.6101\n",
            "Epoch 214/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 0.9949 - acc: 0.6121\n",
            "Epoch 215/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9933 - acc: 0.6121\n",
            "Epoch 216/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 0.9994 - acc: 0.6121\n",
            "Epoch 217/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 1.0018 - acc: 0.6121\n",
            "Epoch 218/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9801 - acc: 0.6121\n",
            "Epoch 219/250\n",
            "513/513 [==============================] - 0s 80us/step - loss: 0.9847 - acc: 0.6101\n",
            "Epoch 220/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0017 - acc: 0.6062\n",
            "Epoch 221/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9940 - acc: 0.6043\n",
            "Epoch 222/250\n",
            "513/513 [==============================] - 0s 54us/step - loss: 1.0070 - acc: 0.6062\n",
            "Epoch 223/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9893 - acc: 0.6082\n",
            "Epoch 224/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 0.9876 - acc: 0.6121\n",
            "Epoch 225/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0039 - acc: 0.6082\n",
            "Epoch 226/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9998 - acc: 0.6082\n",
            "Epoch 227/250\n",
            "513/513 [==============================] - 0s 60us/step - loss: 1.0016 - acc: 0.6062\n",
            "Epoch 228/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9963 - acc: 0.6043\n",
            "Epoch 229/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9983 - acc: 0.6062\n",
            "Epoch 230/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9908 - acc: 0.6121\n",
            "Epoch 231/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 1.0153 - acc: 0.6121\n",
            "Epoch 232/250\n",
            "513/513 [==============================] - 0s 48us/step - loss: 0.9926 - acc: 0.6121\n",
            "Epoch 233/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 1.0003 - acc: 0.6121\n",
            "Epoch 234/250\n",
            "513/513 [==============================] - 0s 50us/step - loss: 0.9832 - acc: 0.6121\n",
            "Epoch 235/250\n",
            "513/513 [==============================] - 0s 66us/step - loss: 1.0054 - acc: 0.6101\n",
            "Epoch 236/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 1.0063 - acc: 0.6082\n",
            "Epoch 237/250\n",
            "513/513 [==============================] - 0s 49us/step - loss: 0.9808 - acc: 0.6121\n",
            "Epoch 238/250\n",
            "513/513 [==============================] - 0s 58us/step - loss: 0.9834 - acc: 0.6140\n",
            "Epoch 239/250\n",
            "513/513 [==============================] - 0s 46us/step - loss: 0.9986 - acc: 0.6062\n",
            "Epoch 240/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9869 - acc: 0.6121\n",
            "Epoch 241/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9957 - acc: 0.6062\n",
            "Epoch 242/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9819 - acc: 0.6101\n",
            "Epoch 243/250\n",
            "513/513 [==============================] - 0s 62us/step - loss: 0.9864 - acc: 0.6101\n",
            "Epoch 244/250\n",
            "513/513 [==============================] - 0s 56us/step - loss: 0.9933 - acc: 0.6082\n",
            "Epoch 245/250\n",
            "513/513 [==============================] - 0s 59us/step - loss: 1.0052 - acc: 0.6101\n",
            "Epoch 246/250\n",
            "513/513 [==============================] - 0s 55us/step - loss: 0.9887 - acc: 0.6062\n",
            "Epoch 247/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 1.0082 - acc: 0.6121\n",
            "Epoch 248/250\n",
            "513/513 [==============================] - 0s 52us/step - loss: 0.9874 - acc: 0.6121\n",
            "Epoch 249/250\n",
            "513/513 [==============================] - 0s 51us/step - loss: 0.9937 - acc: 0.6101\n",
            "Epoch 250/250\n",
            "513/513 [==============================] - 0s 53us/step - loss: 0.9986 - acc: 0.6121\n",
            "513/513 [==============================] - 0s 26us/step\n",
            "Test loss: 1.6112291818706148\n",
            "Test accuracy: 0.5419103313840156\n",
            "Test loss: 1.6112291818706148\n",
            "Test accuracy: 0.5419103313840156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 20/22 [02:52<00:16,  8.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.34307992202729043 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        41\n",
            "         1.0       0.00      0.00      0.00        34\n",
            "         2.0       0.34      1.00      0.51       176\n",
            "         3.0       0.00      0.00      0.00       166\n",
            "         4.0       0.00      0.00      0.00        96\n",
            "\n",
            "   micro avg       0.34      0.34      0.34       513\n",
            "   macro avg       0.07      0.20      0.10       513\n",
            "weighted avg       0.12      0.34      0.18       513\n",
            "\n",
            "[[  0   0  41   0   0]\n",
            " [  0   0  34   0   0]\n",
            " [  0   0 176   0   0]\n",
            " [  0   0 166   0   0]\n",
            " [  0   0  96   0   0]]\n",
            "[[ 29   0   0   0]\n",
            " [  0 276   0   0]\n",
            " [  0  14  23   0]\n",
            " [  0  24   0 147]]\n",
            "Epoch 1/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.9833 - acc: 0.7669\n",
            "Epoch 2/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.9659 - acc: 0.7669\n",
            "Epoch 3/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.9536 - acc: 0.7627\n",
            "Epoch 4/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.9113 - acc: 0.7585\n",
            "Epoch 5/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.8769 - acc: 0.7585\n",
            "Epoch 6/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.8656 - acc: 0.7564\n",
            "Epoch 7/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.8606 - acc: 0.7479\n",
            "Epoch 8/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.8288 - acc: 0.7521\n",
            "Epoch 9/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.8358 - acc: 0.7521\n",
            "Epoch 10/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.8439 - acc: 0.7479\n",
            "Epoch 11/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.8303 - acc: 0.7458\n",
            "Epoch 12/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.8480 - acc: 0.7458\n",
            "Epoch 13/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.8321 - acc: 0.7479\n",
            "Epoch 14/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.8283 - acc: 0.7436\n",
            "Epoch 15/250\n",
            "472/472 [==============================] - 0s 44us/step - loss: 0.8278 - acc: 0.7500\n",
            "Epoch 16/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.8258 - acc: 0.7415\n",
            "Epoch 17/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.8269 - acc: 0.7415\n",
            "Epoch 18/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.8281 - acc: 0.7500\n",
            "Epoch 19/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.8264 - acc: 0.7606\n",
            "Epoch 20/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7944 - acc: 0.7479\n",
            "Epoch 21/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.8006 - acc: 0.7585\n",
            "Epoch 22/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7975 - acc: 0.7564\n",
            "Epoch 23/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.8117 - acc: 0.7585\n",
            "Epoch 24/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.8221 - acc: 0.7606\n",
            "Epoch 25/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.8171 - acc: 0.7500\n",
            "Epoch 26/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.8081 - acc: 0.7585\n",
            "Epoch 27/250\n",
            "472/472 [==============================] - 0s 43us/step - loss: 0.8071 - acc: 0.7564\n",
            "Epoch 28/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7954 - acc: 0.7585\n",
            "Epoch 29/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.7909 - acc: 0.7564\n",
            "Epoch 30/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.8195 - acc: 0.7606\n",
            "Epoch 31/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.8103 - acc: 0.7479\n",
            "Epoch 32/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7894 - acc: 0.7669\n",
            "Epoch 33/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7949 - acc: 0.7606\n",
            "Epoch 34/250\n",
            "472/472 [==============================] - 0s 71us/step - loss: 0.8049 - acc: 0.7542\n",
            "Epoch 35/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.8040 - acc: 0.7542\n",
            "Epoch 36/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7961 - acc: 0.7542\n",
            "Epoch 37/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7981 - acc: 0.7564\n",
            "Epoch 38/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7974 - acc: 0.7542\n",
            "Epoch 39/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7892 - acc: 0.7521\n",
            "Epoch 40/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.8029 - acc: 0.7585\n",
            "Epoch 41/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.8030 - acc: 0.7521\n",
            "Epoch 42/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.8082 - acc: 0.7500\n",
            "Epoch 43/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7899 - acc: 0.7585\n",
            "Epoch 44/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7875 - acc: 0.7521\n",
            "Epoch 45/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7840 - acc: 0.7606\n",
            "Epoch 46/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.7974 - acc: 0.7500\n",
            "Epoch 47/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7960 - acc: 0.7606\n",
            "Epoch 48/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7871 - acc: 0.7564\n",
            "Epoch 49/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7963 - acc: 0.7606\n",
            "Epoch 50/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7996 - acc: 0.7564\n",
            "Epoch 51/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7895 - acc: 0.7542\n",
            "Epoch 52/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7860 - acc: 0.7627\n",
            "Epoch 53/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7733 - acc: 0.7585\n",
            "Epoch 54/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7841 - acc: 0.7500\n",
            "Epoch 55/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7909 - acc: 0.7564\n",
            "Epoch 56/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7840 - acc: 0.7585\n",
            "Epoch 57/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7746 - acc: 0.7606\n",
            "Epoch 58/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7955 - acc: 0.7542\n",
            "Epoch 59/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7862 - acc: 0.7606\n",
            "Epoch 60/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7784 - acc: 0.7627\n",
            "Epoch 61/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7873 - acc: 0.7542\n",
            "Epoch 62/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.7729 - acc: 0.7585\n",
            "Epoch 63/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7868 - acc: 0.7585\n",
            "Epoch 64/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7894 - acc: 0.7521\n",
            "Epoch 65/250\n",
            "472/472 [==============================] - 0s 44us/step - loss: 0.7666 - acc: 0.7564\n",
            "Epoch 66/250\n",
            "472/472 [==============================] - 0s 43us/step - loss: 0.7722 - acc: 0.7606\n",
            "Epoch 67/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7757 - acc: 0.7627\n",
            "Epoch 68/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7746 - acc: 0.7500\n",
            "Epoch 69/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7843 - acc: 0.7585\n",
            "Epoch 70/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7931 - acc: 0.7627\n",
            "Epoch 71/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7850 - acc: 0.7564\n",
            "Epoch 72/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7788 - acc: 0.7585\n",
            "Epoch 73/250\n",
            "472/472 [==============================] - 0s 70us/step - loss: 0.7773 - acc: 0.7542\n",
            "Epoch 74/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7898 - acc: 0.7564\n",
            "Epoch 75/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7882 - acc: 0.7627\n",
            "Epoch 76/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7767 - acc: 0.7585\n",
            "Epoch 77/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7854 - acc: 0.7627\n",
            "Epoch 78/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.8050 - acc: 0.7542\n",
            "Epoch 79/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7880 - acc: 0.7542\n",
            "Epoch 80/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7721 - acc: 0.7542\n",
            "Epoch 81/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7654 - acc: 0.7648\n",
            "Epoch 82/250\n",
            "472/472 [==============================] - 0s 43us/step - loss: 0.7727 - acc: 0.7648\n",
            "Epoch 83/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7737 - acc: 0.7521\n",
            "Epoch 84/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7634 - acc: 0.7585\n",
            "Epoch 85/250\n",
            "472/472 [==============================] - 0s 44us/step - loss: 0.7974 - acc: 0.7585\n",
            "Epoch 86/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7831 - acc: 0.7606\n",
            "Epoch 87/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7728 - acc: 0.7585\n",
            "Epoch 88/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7820 - acc: 0.7542\n",
            "Epoch 89/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.7800 - acc: 0.7542\n",
            "Epoch 90/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7754 - acc: 0.7521\n",
            "Epoch 91/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7769 - acc: 0.7606\n",
            "Epoch 92/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7755 - acc: 0.7542\n",
            "Epoch 93/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7802 - acc: 0.7521\n",
            "Epoch 94/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7792 - acc: 0.7500\n",
            "Epoch 95/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7886 - acc: 0.7627\n",
            "Epoch 96/250\n",
            "472/472 [==============================] - 0s 42us/step - loss: 0.7805 - acc: 0.7585\n",
            "Epoch 97/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7768 - acc: 0.7585\n",
            "Epoch 98/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7698 - acc: 0.7627\n",
            "Epoch 99/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7923 - acc: 0.7606\n",
            "Epoch 100/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7823 - acc: 0.7648\n",
            "Epoch 101/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7626 - acc: 0.7627\n",
            "Epoch 102/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7777 - acc: 0.7648\n",
            "Epoch 103/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7830 - acc: 0.7669\n",
            "Epoch 104/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7730 - acc: 0.7606\n",
            "Epoch 105/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7814 - acc: 0.7648\n",
            "Epoch 106/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7670 - acc: 0.7648\n",
            "Epoch 107/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7699 - acc: 0.7606\n",
            "Epoch 108/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7818 - acc: 0.7648\n",
            "Epoch 109/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7754 - acc: 0.7648\n",
            "Epoch 110/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7908 - acc: 0.7648\n",
            "Epoch 111/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7666 - acc: 0.7648\n",
            "Epoch 112/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7623 - acc: 0.7648\n",
            "Epoch 113/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.7733 - acc: 0.7627\n",
            "Epoch 114/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7676 - acc: 0.7627\n",
            "Epoch 115/250\n",
            "472/472 [==============================] - 0s 43us/step - loss: 0.7818 - acc: 0.7606\n",
            "Epoch 116/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7695 - acc: 0.7627\n",
            "Epoch 117/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7800 - acc: 0.7669\n",
            "Epoch 118/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7828 - acc: 0.7648\n",
            "Epoch 119/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7544 - acc: 0.7606\n",
            "Epoch 120/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7703 - acc: 0.7712\n",
            "Epoch 121/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7716 - acc: 0.7606\n",
            "Epoch 122/250\n",
            "472/472 [==============================] - 0s 61us/step - loss: 0.7720 - acc: 0.7627\n",
            "Epoch 123/250\n",
            "472/472 [==============================] - 0s 63us/step - loss: 0.7576 - acc: 0.7669\n",
            "Epoch 124/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7626 - acc: 0.7669\n",
            "Epoch 125/250\n",
            "472/472 [==============================] - 0s 44us/step - loss: 0.7624 - acc: 0.7648\n",
            "Epoch 126/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7648 - acc: 0.7691\n",
            "Epoch 127/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7664 - acc: 0.7669\n",
            "Epoch 128/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7717 - acc: 0.7648\n",
            "Epoch 129/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7753 - acc: 0.7669\n",
            "Epoch 130/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7737 - acc: 0.7669\n",
            "Epoch 131/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7727 - acc: 0.7669\n",
            "Epoch 132/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7556 - acc: 0.7669\n",
            "Epoch 133/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7533 - acc: 0.7669\n",
            "Epoch 134/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.7765 - acc: 0.7669\n",
            "Epoch 135/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7731 - acc: 0.7627\n",
            "Epoch 136/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7591 - acc: 0.7669\n",
            "Epoch 137/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.7716 - acc: 0.7648\n",
            "Epoch 138/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7762 - acc: 0.7669\n",
            "Epoch 139/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7928 - acc: 0.7669\n",
            "Epoch 140/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7789 - acc: 0.7669\n",
            "Epoch 141/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7856 - acc: 0.7648\n",
            "Epoch 142/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7646 - acc: 0.7669\n",
            "Epoch 143/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7601 - acc: 0.7669\n",
            "Epoch 144/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.7733 - acc: 0.7669\n",
            "Epoch 145/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7652 - acc: 0.7669\n",
            "Epoch 146/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7895 - acc: 0.7669\n",
            "Epoch 147/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7832 - acc: 0.7669\n",
            "Epoch 148/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.7710 - acc: 0.7669\n",
            "Epoch 149/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7591 - acc: 0.7669\n",
            "Epoch 150/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7626 - acc: 0.7648\n",
            "Epoch 151/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7809 - acc: 0.7669\n",
            "Epoch 152/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7605 - acc: 0.7669\n",
            "Epoch 153/250\n",
            "472/472 [==============================] - 0s 71us/step - loss: 0.7556 - acc: 0.7691\n",
            "Epoch 154/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7540 - acc: 0.7669\n",
            "Epoch 155/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7724 - acc: 0.7606\n",
            "Epoch 156/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7598 - acc: 0.7648\n",
            "Epoch 157/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7814 - acc: 0.7669\n",
            "Epoch 158/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7740 - acc: 0.7648\n",
            "Epoch 159/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7687 - acc: 0.7669\n",
            "Epoch 160/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7729 - acc: 0.7627\n",
            "Epoch 161/250\n",
            "472/472 [==============================] - 0s 43us/step - loss: 0.7800 - acc: 0.7669\n",
            "Epoch 162/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7899 - acc: 0.7669\n",
            "Epoch 163/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7632 - acc: 0.7627\n",
            "Epoch 164/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7678 - acc: 0.7669\n",
            "Epoch 165/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7624 - acc: 0.7669\n",
            "Epoch 166/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7820 - acc: 0.7669\n",
            "Epoch 167/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7619 - acc: 0.7669\n",
            "Epoch 168/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7751 - acc: 0.7648\n",
            "Epoch 169/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7767 - acc: 0.7648\n",
            "Epoch 170/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7817 - acc: 0.7648\n",
            "Epoch 171/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7650 - acc: 0.7648\n",
            "Epoch 172/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7683 - acc: 0.7648\n",
            "Epoch 173/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7698 - acc: 0.7606\n",
            "Epoch 174/250\n",
            "472/472 [==============================] - 0s 43us/step - loss: 0.7609 - acc: 0.7669\n",
            "Epoch 175/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7739 - acc: 0.7669\n",
            "Epoch 176/250\n",
            "472/472 [==============================] - 0s 66us/step - loss: 0.7627 - acc: 0.7669\n",
            "Epoch 177/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7622 - acc: 0.7691\n",
            "Epoch 178/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7605 - acc: 0.7669\n",
            "Epoch 179/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7575 - acc: 0.7669\n",
            "Epoch 180/250\n",
            "472/472 [==============================] - 0s 62us/step - loss: 0.7522 - acc: 0.7669\n",
            "Epoch 181/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7695 - acc: 0.7669\n",
            "Epoch 182/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7808 - acc: 0.7627\n",
            "Epoch 183/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7717 - acc: 0.7669\n",
            "Epoch 184/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7712 - acc: 0.7669\n",
            "Epoch 185/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7766 - acc: 0.7648\n",
            "Epoch 186/250\n",
            "472/472 [==============================] - 0s 42us/step - loss: 0.7673 - acc: 0.7669\n",
            "Epoch 187/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7744 - acc: 0.7669\n",
            "Epoch 188/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7807 - acc: 0.7669\n",
            "Epoch 189/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7689 - acc: 0.7648\n",
            "Epoch 190/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7592 - acc: 0.7648\n",
            "Epoch 191/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7699 - acc: 0.7648\n",
            "Epoch 192/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7591 - acc: 0.7669\n",
            "Epoch 193/250\n",
            "472/472 [==============================] - 0s 78us/step - loss: 0.7838 - acc: 0.7648\n",
            "Epoch 194/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.7689 - acc: 0.7669\n",
            "Epoch 195/250\n",
            "472/472 [==============================] - 0s 56us/step - loss: 0.7707 - acc: 0.7669\n",
            "Epoch 196/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7817 - acc: 0.7669\n",
            "Epoch 197/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.7696 - acc: 0.7606\n",
            "Epoch 198/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7645 - acc: 0.7648\n",
            "Epoch 199/250\n",
            "472/472 [==============================] - 0s 49us/step - loss: 0.7807 - acc: 0.7648\n",
            "Epoch 200/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7794 - acc: 0.7669\n",
            "Epoch 201/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7771 - acc: 0.7585\n",
            "Epoch 202/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7660 - acc: 0.7669\n",
            "Epoch 203/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7693 - acc: 0.7648\n",
            "Epoch 204/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7771 - acc: 0.7648\n",
            "Epoch 205/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7585 - acc: 0.7691\n",
            "Epoch 206/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7665 - acc: 0.7669\n",
            "Epoch 207/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7732 - acc: 0.7669\n",
            "Epoch 208/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7516 - acc: 0.7648\n",
            "Epoch 209/250\n",
            "472/472 [==============================] - 0s 54us/step - loss: 0.7826 - acc: 0.7648\n",
            "Epoch 210/250\n",
            "472/472 [==============================] - 0s 57us/step - loss: 0.7771 - acc: 0.7669\n",
            "Epoch 211/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7770 - acc: 0.7669\n",
            "Epoch 212/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7653 - acc: 0.7669\n",
            "Epoch 213/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7841 - acc: 0.7648\n",
            "Epoch 214/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7637 - acc: 0.7648\n",
            "Epoch 215/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7706 - acc: 0.7691\n",
            "Epoch 216/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7483 - acc: 0.7627\n",
            "Epoch 217/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7764 - acc: 0.7627\n",
            "Epoch 218/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7637 - acc: 0.7648\n",
            "Epoch 219/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7810 - acc: 0.7669\n",
            "Epoch 220/250\n",
            "472/472 [==============================] - 0s 53us/step - loss: 0.7581 - acc: 0.7627\n",
            "Epoch 221/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7616 - acc: 0.7669\n",
            "Epoch 222/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7816 - acc: 0.7691\n",
            "Epoch 223/250\n",
            "472/472 [==============================] - 0s 51us/step - loss: 0.7740 - acc: 0.7669\n",
            "Epoch 224/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7692 - acc: 0.7669\n",
            "Epoch 225/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7557 - acc: 0.7669\n",
            "Epoch 226/250\n",
            "472/472 [==============================] - 0s 58us/step - loss: 0.7840 - acc: 0.7627\n",
            "Epoch 227/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7584 - acc: 0.7669\n",
            "Epoch 228/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7669 - acc: 0.7669\n",
            "Epoch 229/250\n",
            "472/472 [==============================] - 0s 44us/step - loss: 0.7672 - acc: 0.7669\n",
            "Epoch 230/250\n",
            "472/472 [==============================] - 0s 42us/step - loss: 0.7664 - acc: 0.7669\n",
            "Epoch 231/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.7735 - acc: 0.7648\n",
            "Epoch 232/250\n",
            "472/472 [==============================] - 0s 65us/step - loss: 0.7730 - acc: 0.7648\n",
            "Epoch 233/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7719 - acc: 0.7648\n",
            "Epoch 234/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7650 - acc: 0.7627\n",
            "Epoch 235/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7653 - acc: 0.7627\n",
            "Epoch 236/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7789 - acc: 0.7648\n",
            "Epoch 237/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7657 - acc: 0.7669\n",
            "Epoch 238/250\n",
            "472/472 [==============================] - 0s 45us/step - loss: 0.7565 - acc: 0.7648\n",
            "Epoch 239/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7542 - acc: 0.7648\n",
            "Epoch 240/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7596 - acc: 0.7627\n",
            "Epoch 241/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7722 - acc: 0.7691\n",
            "Epoch 242/250\n",
            "472/472 [==============================] - 0s 55us/step - loss: 0.7722 - acc: 0.7648\n",
            "Epoch 243/250\n",
            "472/472 [==============================] - 0s 59us/step - loss: 0.7601 - acc: 0.7669\n",
            "Epoch 244/250\n",
            "472/472 [==============================] - 0s 60us/step - loss: 0.7730 - acc: 0.7669\n",
            "Epoch 245/250\n",
            "472/472 [==============================] - 0s 46us/step - loss: 0.7792 - acc: 0.7627\n",
            "Epoch 246/250\n",
            "472/472 [==============================] - 0s 47us/step - loss: 0.7745 - acc: 0.7669\n",
            "Epoch 247/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7628 - acc: 0.7648\n",
            "Epoch 248/250\n",
            "472/472 [==============================] - 0s 50us/step - loss: 0.7751 - acc: 0.7648\n",
            "Epoch 249/250\n",
            "472/472 [==============================] - 0s 52us/step - loss: 0.7618 - acc: 0.7669\n",
            "Epoch 250/250\n",
            "472/472 [==============================] - 0s 48us/step - loss: 0.7718 - acc: 0.7648\n",
            "472/472 [==============================] - 0s 20us/step\n",
            "Test loss: 1.9635052620354345\n",
            "Test accuracy: 0.44279661117973973\n",
            "Test loss: 1.9635052620354345\n",
            "Test accuracy: 0.44279661117973973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 95%|█████████▌| 21/22 [03:00<00:08,  8.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Seq Test accuracy score : 0.2733050847457627 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        82\n",
            "         1.0       0.00      0.00      0.00        71\n",
            "         2.0       0.27      1.00      0.43       129\n",
            "         3.0       0.00      0.00      0.00        91\n",
            "         4.0       0.00      0.00      0.00        99\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       472\n",
            "   macro avg       0.05      0.20      0.09       472\n",
            "weighted avg       0.07      0.27      0.12       472\n",
            "\n",
            "[[  0   0  82   0   0]\n",
            " [  0   0  71   0   0]\n",
            " [  0   0 129   0   0]\n",
            " [  0   0  91   0   0]\n",
            " [  0   0  99   0   0]]\n",
            "[[  6   0   0   0   2]\n",
            " [  0   0   2   0   0]\n",
            " [  0   0 351   0   0]\n",
            " [  0   0   4  63   0]\n",
            " [  0   0   5   0  39]]\n",
            "Epoch 1/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 2.2005 - acc: 0.2397\n",
            "Epoch 2/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 2.1529 - acc: 0.2418\n",
            "Epoch 3/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 2.1512 - acc: 0.2375\n",
            "Epoch 4/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 2.1162 - acc: 0.2397\n",
            "Epoch 5/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 2.1140 - acc: 0.2331\n",
            "Epoch 6/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 2.0890 - acc: 0.2397\n",
            "Epoch 7/250\n",
            "459/459 [==============================] - 0s 43us/step - loss: 2.0588 - acc: 0.2418\n",
            "Epoch 8/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 2.0481 - acc: 0.2418\n",
            "Epoch 9/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 2.0379 - acc: 0.2462\n",
            "Epoch 10/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 2.0199 - acc: 0.2397\n",
            "Epoch 11/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 2.0067 - acc: 0.2375\n",
            "Epoch 12/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.9943 - acc: 0.2397\n",
            "Epoch 13/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.9983 - acc: 0.2397\n",
            "Epoch 14/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.9715 - acc: 0.2288\n",
            "Epoch 15/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.9501 - acc: 0.2397\n",
            "Epoch 16/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.9568 - acc: 0.2397\n",
            "Epoch 17/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.9473 - acc: 0.2309\n",
            "Epoch 18/250\n",
            "459/459 [==============================] - 0s 42us/step - loss: 1.9102 - acc: 0.2484\n",
            "Epoch 19/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.9092 - acc: 0.2375\n",
            "Epoch 20/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.9112 - acc: 0.2397\n",
            "Epoch 21/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.8927 - acc: 0.2418\n",
            "Epoch 22/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.8995 - acc: 0.2331\n",
            "Epoch 23/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.8725 - acc: 0.2353\n",
            "Epoch 24/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.8714 - acc: 0.2331\n",
            "Epoch 25/250\n",
            "459/459 [==============================] - 0s 60us/step - loss: 1.8590 - acc: 0.2418\n",
            "Epoch 26/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.8604 - acc: 0.2353\n",
            "Epoch 27/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.8204 - acc: 0.2418\n",
            "Epoch 28/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.8299 - acc: 0.2353\n",
            "Epoch 29/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.8101 - acc: 0.2418\n",
            "Epoch 30/250\n",
            "459/459 [==============================] - 0s 65us/step - loss: 1.8086 - acc: 0.2353\n",
            "Epoch 31/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.7873 - acc: 0.2397\n",
            "Epoch 32/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.7984 - acc: 0.2397\n",
            "Epoch 33/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.7698 - acc: 0.2440\n",
            "Epoch 34/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.7658 - acc: 0.2331\n",
            "Epoch 35/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.7533 - acc: 0.2484\n",
            "Epoch 36/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.7341 - acc: 0.2309\n",
            "Epoch 37/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.7120 - acc: 0.2484\n",
            "Epoch 38/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.7287 - acc: 0.2418\n",
            "Epoch 39/250\n",
            "459/459 [==============================] - 0s 60us/step - loss: 1.7157 - acc: 0.2375\n",
            "Epoch 40/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.7063 - acc: 0.2353\n",
            "Epoch 41/250\n",
            "459/459 [==============================] - 0s 57us/step - loss: 1.7030 - acc: 0.2309\n",
            "Epoch 42/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.6965 - acc: 0.2484\n",
            "Epoch 43/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.6924 - acc: 0.2309\n",
            "Epoch 44/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.6683 - acc: 0.2505\n",
            "Epoch 45/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.6668 - acc: 0.2440\n",
            "Epoch 46/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.6579 - acc: 0.2309\n",
            "Epoch 47/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.6560 - acc: 0.2375\n",
            "Epoch 48/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.6151 - acc: 0.2527\n",
            "Epoch 49/250\n",
            "459/459 [==============================] - 0s 61us/step - loss: 1.6454 - acc: 0.2222\n",
            "Epoch 50/250\n",
            "459/459 [==============================] - 0s 57us/step - loss: 1.6433 - acc: 0.2505\n",
            "Epoch 51/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.6230 - acc: 0.2397\n",
            "Epoch 52/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.6476 - acc: 0.2244\n",
            "Epoch 53/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.6063 - acc: 0.2353\n",
            "Epoch 54/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.5993 - acc: 0.2462\n",
            "Epoch 55/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.6148 - acc: 0.2462\n",
            "Epoch 56/250\n",
            "459/459 [==============================] - 0s 76us/step - loss: 1.5796 - acc: 0.2353\n",
            "Epoch 57/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.5954 - acc: 0.2397\n",
            "Epoch 58/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.5838 - acc: 0.2397\n",
            "Epoch 59/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.5758 - acc: 0.2375\n",
            "Epoch 60/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.5594 - acc: 0.2331\n",
            "Epoch 61/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.6052 - acc: 0.2375\n",
            "Epoch 62/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.5661 - acc: 0.2288\n",
            "Epoch 63/250\n",
            "459/459 [==============================] - 0s 57us/step - loss: 1.5747 - acc: 0.2222\n",
            "Epoch 64/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.5735 - acc: 0.2309\n",
            "Epoch 65/250\n",
            "459/459 [==============================] - 0s 57us/step - loss: 1.5133 - acc: 0.2505\n",
            "Epoch 66/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.5584 - acc: 0.2353\n",
            "Epoch 67/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.5409 - acc: 0.2266\n",
            "Epoch 68/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.5419 - acc: 0.2200\n",
            "Epoch 69/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.5059 - acc: 0.2353\n",
            "Epoch 70/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.5566 - acc: 0.2375\n",
            "Epoch 71/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.5482 - acc: 0.2266\n",
            "Epoch 72/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.5537 - acc: 0.2288\n",
            "Epoch 73/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.5311 - acc: 0.2484\n",
            "Epoch 74/250\n",
            "459/459 [==============================] - 0s 57us/step - loss: 1.5259 - acc: 0.2527\n",
            "Epoch 75/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.5132 - acc: 0.2484\n",
            "Epoch 76/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.5160 - acc: 0.2331\n",
            "Epoch 77/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4981 - acc: 0.2353\n",
            "Epoch 78/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.5018 - acc: 0.2462\n",
            "Epoch 79/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.5262 - acc: 0.2288\n",
            "Epoch 80/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.5211 - acc: 0.2288\n",
            "Epoch 81/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.5099 - acc: 0.2353\n",
            "Epoch 82/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4912 - acc: 0.2462\n",
            "Epoch 83/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.5141 - acc: 0.2505\n",
            "Epoch 84/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.5076 - acc: 0.2571\n",
            "Epoch 85/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4943 - acc: 0.2331\n",
            "Epoch 86/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4914 - acc: 0.2702\n",
            "Epoch 87/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4799 - acc: 0.2593\n",
            "Epoch 88/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.4748 - acc: 0.2614\n",
            "Epoch 89/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.5016 - acc: 0.2745\n",
            "Epoch 90/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4698 - acc: 0.3028\n",
            "Epoch 91/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4883 - acc: 0.2745\n",
            "Epoch 92/250\n",
            "459/459 [==============================] - 0s 58us/step - loss: 1.4555 - acc: 0.2723\n",
            "Epoch 93/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.4778 - acc: 0.2593\n",
            "Epoch 94/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4884 - acc: 0.2462\n",
            "Epoch 95/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4483 - acc: 0.2898\n",
            "Epoch 96/250\n",
            "459/459 [==============================] - 0s 84us/step - loss: 1.4844 - acc: 0.2397\n",
            "Epoch 97/250\n",
            "459/459 [==============================] - 0s 60us/step - loss: 1.4832 - acc: 0.2527\n",
            "Epoch 98/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4520 - acc: 0.2549\n",
            "Epoch 99/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4728 - acc: 0.2919\n",
            "Epoch 100/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4987 - acc: 0.2658\n",
            "Epoch 101/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4513 - acc: 0.2898\n",
            "Epoch 102/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4592 - acc: 0.2593\n",
            "Epoch 103/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.4827 - acc: 0.2832\n",
            "Epoch 104/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4706 - acc: 0.2702\n",
            "Epoch 105/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4453 - acc: 0.2680\n",
            "Epoch 106/250\n",
            "459/459 [==============================] - 0s 58us/step - loss: 1.4777 - acc: 0.2658\n",
            "Epoch 107/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4530 - acc: 0.2898\n",
            "Epoch 108/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4563 - acc: 0.3050\n",
            "Epoch 109/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4566 - acc: 0.2854\n",
            "Epoch 110/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4697 - acc: 0.2527\n",
            "Epoch 111/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4496 - acc: 0.2789\n",
            "Epoch 112/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.4588 - acc: 0.2658\n",
            "Epoch 113/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4483 - acc: 0.2571\n",
            "Epoch 114/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.4452 - acc: 0.2789\n",
            "Epoch 115/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4578 - acc: 0.2789\n",
            "Epoch 116/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4744 - acc: 0.2810\n",
            "Epoch 117/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4501 - acc: 0.2963\n",
            "Epoch 118/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4546 - acc: 0.2571\n",
            "Epoch 119/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4669 - acc: 0.2789\n",
            "Epoch 120/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4574 - acc: 0.2832\n",
            "Epoch 121/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4484 - acc: 0.2832\n",
            "Epoch 122/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4589 - acc: 0.2702\n",
            "Epoch 123/250\n",
            "459/459 [==============================] - 0s 62us/step - loss: 1.4560 - acc: 0.2789\n",
            "Epoch 124/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4776 - acc: 0.2810\n",
            "Epoch 125/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4563 - acc: 0.2767\n",
            "Epoch 126/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4290 - acc: 0.3137\n",
            "Epoch 127/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4433 - acc: 0.2810\n",
            "Epoch 128/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4577 - acc: 0.2898\n",
            "Epoch 129/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4465 - acc: 0.2854\n",
            "Epoch 130/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.4665 - acc: 0.2832\n",
            "Epoch 131/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4432 - acc: 0.2810\n",
            "Epoch 132/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4321 - acc: 0.2919\n",
            "Epoch 133/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4449 - acc: 0.2789\n",
            "Epoch 134/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4450 - acc: 0.2832\n",
            "Epoch 135/250\n",
            "459/459 [==============================] - 0s 68us/step - loss: 1.4535 - acc: 0.2505\n",
            "Epoch 136/250\n",
            "459/459 [==============================] - 0s 62us/step - loss: 1.4533 - acc: 0.2767\n",
            "Epoch 137/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4131 - acc: 0.3028\n",
            "Epoch 138/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4463 - acc: 0.2832\n",
            "Epoch 139/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4373 - acc: 0.2702\n",
            "Epoch 140/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4441 - acc: 0.2680\n",
            "Epoch 141/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4352 - acc: 0.3072\n",
            "Epoch 142/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.4431 - acc: 0.2745\n",
            "Epoch 143/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4342 - acc: 0.2876\n",
            "Epoch 144/250\n",
            "459/459 [==============================] - 0s 61us/step - loss: 1.4231 - acc: 0.3007\n",
            "Epoch 145/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4638 - acc: 0.2571\n",
            "Epoch 146/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4356 - acc: 0.2876\n",
            "Epoch 147/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4392 - acc: 0.3050\n",
            "Epoch 148/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4277 - acc: 0.2723\n",
            "Epoch 149/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4543 - acc: 0.2462\n",
            "Epoch 150/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4471 - acc: 0.2898\n",
            "Epoch 151/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4432 - acc: 0.2832\n",
            "Epoch 152/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4276 - acc: 0.2789\n",
            "Epoch 153/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4344 - acc: 0.2941\n",
            "Epoch 154/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4177 - acc: 0.3072\n",
            "Epoch 155/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4151 - acc: 0.2854\n",
            "Epoch 156/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4535 - acc: 0.2745\n",
            "Epoch 157/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4600 - acc: 0.2789\n",
            "Epoch 158/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.4301 - acc: 0.3028\n",
            "Epoch 159/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.4397 - acc: 0.2767\n",
            "Epoch 160/250\n",
            "459/459 [==============================] - 0s 70us/step - loss: 1.4071 - acc: 0.2832\n",
            "Epoch 161/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4228 - acc: 0.2789\n",
            "Epoch 162/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4306 - acc: 0.2941\n",
            "Epoch 163/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4328 - acc: 0.2854\n",
            "Epoch 164/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4125 - acc: 0.2963\n",
            "Epoch 165/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4271 - acc: 0.2723\n",
            "Epoch 166/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4229 - acc: 0.2810\n",
            "Epoch 167/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4304 - acc: 0.2680\n",
            "Epoch 168/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.4402 - acc: 0.2963\n",
            "Epoch 169/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4348 - acc: 0.2789\n",
            "Epoch 170/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4351 - acc: 0.2636\n",
            "Epoch 171/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.4324 - acc: 0.2723\n",
            "Epoch 172/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4332 - acc: 0.2636\n",
            "Epoch 173/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.4321 - acc: 0.2810\n",
            "Epoch 174/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4290 - acc: 0.2985\n",
            "Epoch 175/250\n",
            "459/459 [==============================] - 0s 69us/step - loss: 1.4259 - acc: 0.2658\n",
            "Epoch 176/250\n",
            "459/459 [==============================] - 0s 64us/step - loss: 1.4333 - acc: 0.2723\n",
            "Epoch 177/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4278 - acc: 0.2723\n",
            "Epoch 178/250\n",
            "459/459 [==============================] - 0s 51us/step - loss: 1.4332 - acc: 0.2658\n",
            "Epoch 179/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4158 - acc: 0.2985\n",
            "Epoch 180/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4496 - acc: 0.2505\n",
            "Epoch 181/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4460 - acc: 0.2745\n",
            "Epoch 182/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4518 - acc: 0.2571\n",
            "Epoch 183/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.4010 - acc: 0.2985\n",
            "Epoch 184/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.4288 - acc: 0.2898\n",
            "Epoch 185/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4148 - acc: 0.2963\n",
            "Epoch 186/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4235 - acc: 0.2789\n",
            "Epoch 187/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.4382 - acc: 0.2636\n",
            "Epoch 188/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4235 - acc: 0.2919\n",
            "Epoch 189/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4377 - acc: 0.2680\n",
            "Epoch 190/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4255 - acc: 0.2832\n",
            "Epoch 191/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.4483 - acc: 0.2397\n",
            "Epoch 192/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4314 - acc: 0.2571\n",
            "Epoch 193/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.4325 - acc: 0.2593\n",
            "Epoch 194/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4152 - acc: 0.2963\n",
            "Epoch 195/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4345 - acc: 0.2876\n",
            "Epoch 196/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.4428 - acc: 0.2593\n",
            "Epoch 197/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4269 - acc: 0.2723\n",
            "Epoch 198/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4327 - acc: 0.2702\n",
            "Epoch 199/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4283 - acc: 0.2789\n",
            "Epoch 200/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4174 - acc: 0.2876\n",
            "Epoch 201/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4261 - acc: 0.2702\n",
            "Epoch 202/250\n",
            "459/459 [==============================] - 0s 61us/step - loss: 1.4173 - acc: 0.2767\n",
            "Epoch 203/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4454 - acc: 0.2767\n",
            "Epoch 204/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4413 - acc: 0.2614\n",
            "Epoch 205/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4421 - acc: 0.2898\n",
            "Epoch 206/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4244 - acc: 0.2614\n",
            "Epoch 207/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4136 - acc: 0.2876\n",
            "Epoch 208/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4097 - acc: 0.2854\n",
            "Epoch 209/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4410 - acc: 0.2767\n",
            "Epoch 210/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4238 - acc: 0.2636\n",
            "Epoch 211/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4264 - acc: 0.2832\n",
            "Epoch 212/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4370 - acc: 0.2832\n",
            "Epoch 213/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4175 - acc: 0.2702\n",
            "Epoch 214/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4105 - acc: 0.2876\n",
            "Epoch 215/250\n",
            "459/459 [==============================] - 0s 65us/step - loss: 1.4339 - acc: 0.2505\n",
            "Epoch 216/250\n",
            "459/459 [==============================] - 0s 68us/step - loss: 1.4128 - acc: 0.2963\n",
            "Epoch 217/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4123 - acc: 0.3072\n",
            "Epoch 218/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4521 - acc: 0.2810\n",
            "Epoch 219/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4242 - acc: 0.2789\n",
            "Epoch 220/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4191 - acc: 0.2985\n",
            "Epoch 221/250\n",
            "459/459 [==============================] - 0s 46us/step - loss: 1.4350 - acc: 0.2549\n",
            "Epoch 222/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4198 - acc: 0.2702\n",
            "Epoch 223/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4186 - acc: 0.3072\n",
            "Epoch 224/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4220 - acc: 0.2898\n",
            "Epoch 225/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4141 - acc: 0.2898\n",
            "Epoch 226/250\n",
            "459/459 [==============================] - 0s 57us/step - loss: 1.4333 - acc: 0.2702\n",
            "Epoch 227/250\n",
            "459/459 [==============================] - 0s 56us/step - loss: 1.3986 - acc: 0.3203\n",
            "Epoch 228/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4208 - acc: 0.2810\n",
            "Epoch 229/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.4109 - acc: 0.2963\n",
            "Epoch 230/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4300 - acc: 0.2810\n",
            "Epoch 231/250\n",
            "459/459 [==============================] - 0s 47us/step - loss: 1.4211 - acc: 0.2985\n",
            "Epoch 232/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4245 - acc: 0.2789\n",
            "Epoch 233/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4113 - acc: 0.2745\n",
            "Epoch 234/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4180 - acc: 0.2680\n",
            "Epoch 235/250\n",
            "459/459 [==============================] - 0s 59us/step - loss: 1.4386 - acc: 0.2549\n",
            "Epoch 236/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4473 - acc: 0.2636\n",
            "Epoch 237/250\n",
            "459/459 [==============================] - 0s 50us/step - loss: 1.4378 - acc: 0.2723\n",
            "Epoch 238/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4203 - acc: 0.2702\n",
            "Epoch 239/250\n",
            "459/459 [==============================] - 0s 48us/step - loss: 1.4363 - acc: 0.2658\n",
            "Epoch 240/250\n",
            "459/459 [==============================] - 0s 45us/step - loss: 1.4015 - acc: 0.3137\n",
            "Epoch 241/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4158 - acc: 0.2854\n",
            "Epoch 242/250\n",
            "459/459 [==============================] - 0s 54us/step - loss: 1.4289 - acc: 0.2462\n",
            "Epoch 243/250\n",
            "459/459 [==============================] - 0s 58us/step - loss: 1.4330 - acc: 0.2876\n",
            "Epoch 244/250\n",
            "459/459 [==============================] - 0s 55us/step - loss: 1.4140 - acc: 0.2832\n",
            "Epoch 245/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4090 - acc: 0.2941\n",
            "Epoch 246/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4233 - acc: 0.2767\n",
            "Epoch 247/250\n",
            "459/459 [==============================] - 0s 43us/step - loss: 1.4186 - acc: 0.2789\n",
            "Epoch 248/250\n",
            "459/459 [==============================] - 0s 49us/step - loss: 1.4343 - acc: 0.2745\n",
            "Epoch 249/250\n",
            "459/459 [==============================] - 0s 52us/step - loss: 1.4200 - acc: 0.2680\n",
            "Epoch 250/250\n",
            "459/459 [==============================] - 0s 53us/step - loss: 1.4144 - acc: 0.2636\n",
            "459/459 [==============================] - 0s 23us/step\n",
            "Test loss: 1.6441311947920223\n",
            "Test accuracy: 0.09586056657865935\n",
            "Test loss: 1.6441311947920223\n",
            "Test accuracy: 0.09586056657865935\n",
            "Seq Test accuracy score : 0.030501089324618737 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        84\n",
            "         1.0       0.00      0.00      0.00       111\n",
            "         2.0       0.00      0.00      0.00       168\n",
            "         3.0       0.00      0.00      0.00        82\n",
            "         4.0       0.03      1.00      0.06        14\n",
            "\n",
            "   micro avg       0.03      0.03      0.03       459\n",
            "   macro avg       0.01      0.20      0.01       459\n",
            "weighted avg       0.00      0.03      0.00       459\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 22/22 [03:08<00:00,  8.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0  84]\n",
            " [  0   0   0   0 111]\n",
            " [  0   0   0   0 168]\n",
            " [  0   0   0   0  82]\n",
            " [  0   0   0   0  14]]\n",
            "[[ 93   1   1   0   0]\n",
            " [ 15   0   2   0  12]\n",
            " [  2   0 106   5  13]\n",
            " [  0   0   1  93   0]\n",
            " [  1   0   0   0 114]]\n",
            "Seq Test f1 score : 0.6906357679074638 \n",
            "Seq Test accuracy score : 0.759576567686373 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.82      0.75      3300\n",
            "           1       0.48      0.28      0.36      1687\n",
            "           2       0.78      0.86      0.82      8928\n",
            "           3       0.85      0.68      0.76      4104\n",
            "           4       0.78      0.77      0.77      4275\n",
            "\n",
            "   micro avg       0.76      0.76      0.76     22294\n",
            "   macro avg       0.72      0.68      0.69     22294\n",
            "weighted avg       0.76      0.76      0.75     22294\n",
            "\n",
            "Confusion matrix, without normalization\n",
            "[[2718  238   90   45  209]\n",
            " [ 438  479  424   16  330]\n",
            " [ 393  169 7671  384  311]\n",
            " [  34   15 1178 2787   90]\n",
            " [ 338   98  526   34 3279]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Normalized confusion matrix\n",
            "[[0.82 0.07 0.03 0.01 0.06]\n",
            " [0.26 0.28 0.25 0.01 0.2 ]\n",
            " [0.04 0.02 0.86 0.04 0.03]\n",
            " [0.01 0.   0.29 0.68 0.02]\n",
            " [0.08 0.02 0.12 0.01 0.77]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAGACAYAAAAwIRxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGACAYAAADMNDeHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDxor9hcRjmvSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N98veRbjpA_u",
        "colab_type": "code",
        "outputId": "f7070853-6505-422f-f824-13bbfa3b1f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "Fe[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.1 , -0.26, -0.49,  0.03,  0.28, -2.35,  0.14,  0.48,  1.27,\n",
              "       -0.13,  0.54, -0.48,  0.06, -0.53, -0.55,  0.41,  0.08,  0.7 ,\n",
              "        0.43, -1.64,  1.64, -1.61,  1.58, -1.64,  1.69, -1.71,  1.68,\n",
              "       -1.76,  1.82, -1.74,  1.68, -1.63,  1.64, -1.64,  1.67, -1.71,\n",
              "        1.64, -1.58,  1.47, -1.29,  1.13, -1.02,  0.77, -0.33, -0.07,\n",
              "        0.34, -0.5 , -0.75, -0.04,  0.72,  0.41,  0.31, -2.59, -0.06,\n",
              "        0.57, -0.17, -0.18,  1.54, -0.07,  0.01,  1.45, -0.16, -1.57,\n",
              "       -0.67,  1.06, -0.2 ,  1.8 ,  0.14,  0.23,  0.26, -0.61,  1.79,\n",
              "       -0.1 ,  0.22,  0.25, -0.31,  1.75, -0.37,  0.13,  0.51, -0.27,\n",
              "        1.26, -0.37,  0.13,  0.52, -0.27,  1.26, -0.48, -1.06, -0.61,\n",
              "       -0.34,  0.03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLQKPUq1Xf2o",
        "colab_type": "text"
      },
      "source": [
        "# cnn_crf_model_20_folds.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l7_Oty3pDea",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "%cd/content/drive/My Drive\n",
        "base_path = \"pure_data\"\n",
        "\n",
        "files = sorted(glob(os.path.join(base_path, \"*.npz\")))\n",
        "\n",
        "ids = list(set([x.split(\"/\")[-1][:5] for x in files]))\n",
        "list_f1 = []\n",
        "list_acc = []\n",
        "preds = []\n",
        "gt = []\n",
        "for id in ids:\n",
        "    test_ids = {id}\n",
        "    train_ids = set([x.split(\"/\")[-1][:5] for x in files]) - test_ids\n",
        "\n",
        "    train_val, test = [x for x in files if x.split(\"/\")[-1][:5] in train_ids],\\\n",
        "                      [x for x in files if x.split(\"/\")[-1][:5] in test_ids]\n",
        "\n",
        "    train, val = train_test_split(train_val, test_size=0.1, random_state=1337)\n",
        "\n",
        "    train_dict = {k: np.load(k) for k in train}\n",
        "    test_dict = {k: np.load(k) for k in test}\n",
        "    val_dict = {k: np.load(k) for k in val}\n",
        "\n",
        "    model = get_model_cnn_crf(lr=0.0001)\n",
        "\n",
        "    file_path = \"cnn_crf_model_20_folds.h5\"\n",
        "    # model.load_weights(file_path)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20, verbose=1)\n",
        "    redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=2)\n",
        "    callbacks_list = [checkpoint, redonplat]  # early\n",
        "\n",
        "    model.fit_generator(gen(train_dict, aug=False), validation_data=gen(val_dict), epochs=40, verbose=2,\n",
        "                        steps_per_epoch=1000, validation_steps=300, callbacks=callbacks_list)\n",
        "    model.load_weights(file_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for record in tqdm(test_dict):\n",
        "        all_rows = test_dict[record]['x']\n",
        "        record_y_gt = []\n",
        "        record_y_pred = []\n",
        "        for batch_hyp in chunker(range(all_rows.shape[0])):\n",
        "\n",
        "\n",
        "            X = all_rows[min(batch_hyp):max(batch_hyp)+1, ...]\n",
        "            Y = test_dict[record]['y'][min(batch_hyp):max(batch_hyp)+1]\n",
        "\n",
        "            X = np.expand_dims(X, 0)\n",
        "\n",
        "            X = rescale_array(X)\n",
        "\n",
        "            Y_pred = model.predict(X)\n",
        "            Y_pred = Y_pred.argmax(axis=-1).ravel().tolist()\n",
        "\n",
        "            gt += Y.ravel().tolist()\n",
        "            preds += Y_pred\n",
        "\n",
        "            record_y_gt += Y.ravel().tolist()\n",
        "            record_y_pred += Y_pred\n",
        "\n",
        "\n",
        "f1 = f1_score(gt, preds, average=\"macro\")\n",
        "\n",
        "acc = accuracy_score(gt, preds)\n",
        "print(\"acc %s, f1 %s\"%(acc, f1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}